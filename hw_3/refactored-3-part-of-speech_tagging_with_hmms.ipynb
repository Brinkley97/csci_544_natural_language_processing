{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3-Part-of-Speech Tagging with HMMs + Decoding Techniques (Greedy and Viterbi)\n",
    "\n",
    "- Detravious Jamari Brinkley\n",
    "- CSCI-544: Applied Natural Language Processing\n",
    "- python version: 3\n",
    "\n",
    "---\n",
    "\n",
    "1. Part-of-Speech (POS) Tagging [a type of sequence labelling task where of a given word, assign the part of speech]\n",
    "2. HMMs (Hidden Markov Model) [a generative-based model that's used for POS Tagging]\n",
    "    1. Generative-based [provides the probabilities for all possible combinations of values of variables in the set using the joint distribution]\n",
    "    2. With POS Tagging: Given a sequence of observations (sentences), the task is to infer the most likely sequence of hidden states (POS Tags) that could have generated the observed data.\n",
    "3. **Decoding Techniques:**\n",
    "    1. Greedy [find the optimal (OPT) solution at each step]\n",
    "    2. Viterbi [make use of dynammic programming to find the OPT solution with backtracking while searching the entire search space]\n",
    "4. **Notes of the data and given files:**\n",
    "    - Dataset: Wall Street Journal section of the Penn Treebank\n",
    "    - Folder named `data` with the following files:\n",
    "        1. `train`, sentences *with* human-annotated POS Tags\n",
    "        2. `dev`, sentences *with* human-annotated POS Tags\n",
    "        3. `test`, sentences *without* POS Tags, thus predict the POS Tags\n",
    "    - Format: Blank like at the end of each sentence. Each line contains 3 items separated by the `\\t`, the tab symbol. These three items are\n",
    "        1. Index of the word in the sentence\n",
    "        2. Word type\n",
    "        3. POS Tag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Update Data\n",
    "- [x] Find a way to separate sentences when loading the df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str, file_name: str, is_test_file: bool, config_index: bool = True):\n",
    "    \n",
    "    if config_index == True:\n",
    "        if is_test_file != True:\n",
    "            file =  file_path + file_name\n",
    "            open_df = pd.read_table(file, sep = \"\\t\", names=['Index', 'Word', 'POS Tag'], skip_blank_lines=False)\n",
    "        else:\n",
    "            file =  file_path + file_name\n",
    "            open_df = pd.read_table(file, sep = \"\\t\", names=['Index', 'Word'], skip_blank_lines=False)\n",
    "        \n",
    "    return open_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_df_rows_with_dummy(df: pd.DataFrame, new_columns_name: list) -> pd.DataFrame:  \n",
    "    \"\"\"Update the rows of the dataframe if blank space, fill with dummy\"\"\"  \n",
    "\n",
    "    dummy_row = pd.DataFrame([['0.0', ' ', 'dummy']], columns=df.columns)\n",
    "    df = pd.concat([dummy_row, df], ignore_index=True)\n",
    "    df.columns = new_columns_name\n",
    "    df.fillna(\"dummy\", inplace=True)\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data('data/', 'train', False)\n",
    "dev_df = load_data('data/', 'dev', False)\n",
    "test_df = load_data('data/', 'test', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_columns_name = ['Index', 'Word', 'POS Tag']\n",
    "\n",
    "updated_train_df = update_df_rows_with_dummy(train_df, train_dev_columns_name)\n",
    "updated_dev_df = update_df_rows_with_dummy(dev_df, train_dev_columns_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dummy', 'NNP', ',', 'CD', 'NNS', 'JJ', 'MD', 'VB', 'DT', 'NN',\n",
       "       'IN', '.', 'VBZ', 'VBG', 'CC', 'VBD', 'VBN', 'RB', 'TO', 'PRP',\n",
       "       'RBR', 'WDT', 'VBP', 'RP', 'PRP$', 'JJS', 'POS', '``', 'EX', \"''\",\n",
       "       'WP', ':', 'JJR', 'WRB', '$', 'NNPS', 'WP$', '-LRB-', '-RRB-',\n",
       "       'PDT', 'RBS', 'FW', 'UH', 'SYM', 'LS', '#'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pos_tags = updated_train_df['POS Tag'].unique()\n",
    "all_pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline of Tasks\n",
    "\n",
    "1. Vocabulary Creation\n",
    "2. Model Learning\n",
    "3. Greedy Decoding with HMM\n",
    "4. Viterbi Decoding with HMM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Vocabulary Creation\n",
    "\n",
    "- **Problem:** Creating vocabulary to handle unkown words.\n",
    "    - **Solution:** Replace rare words wtih whose occurrences are less than a threshold (ie: 3) with a special token `< unk >`\n",
    "\n",
    "---\n",
    "\n",
    "1. [x] Create a vocabulary using the training data in the file train\n",
    "2. [x] Output the vocabulary into a txt file named `vocab.txt`\n",
    "    - [x] See PDF on how to properly format vocabulary file\n",
    "3. [x] Questions\n",
    "    1. [x] What is the selected threshold for unknown words replacement? 3\n",
    "    2. [x] What is the total size of your vocabulary? 13751\n",
    "    3. [x] What is the total occurrences of the special token `< unk >` after replacement? 29443"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>,</td>\n",
       "      <td>46476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>39533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dummy</td>\n",
       "      <td>38234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.</td>\n",
       "      <td>37452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>22104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43188</th>\n",
       "      <td>Birthday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43189</th>\n",
       "      <td>Happy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43190</th>\n",
       "      <td>Bertie</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43191</th>\n",
       "      <td>crouched</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43192</th>\n",
       "      <td>Huricane</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43193 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word  count\n",
       "0             ,  46476\n",
       "1           the  39533\n",
       "2         dummy  38234\n",
       "3             .  37452\n",
       "4            of  22104\n",
       "...         ...    ...\n",
       "43188  Birthday      1\n",
       "43189     Happy      1\n",
       "43190    Bertie      1\n",
       "43191  crouched      1\n",
       "43192  Huricane      1\n",
       "\n",
       "[43193 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_false_series = updated_train_df['Word'].value_counts()\n",
    "vocab_df = pd.DataFrame(true_false_series)\n",
    "vocab_df.reset_index(inplace = True)\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_threshold_df(df: pd.DataFrame, word_col_name: str, count_col_name: str, threhold: int, special_token: str, save_df: bool, save_path_with_name: str):\n",
    "    \"\"\"For every word in df, replace with special_token if below threshold\n",
    "    \n",
    "    \"\"\"\n",
    "    true_false_series = df[count_col_name] > 3\n",
    "    \n",
    "    updated_vocab_df = df.loc[true_false_series == True]\n",
    "    updated_false_vocab_df = df.loc[true_false_series == False]\n",
    "    updated_false_vocab_df[word_col_name] = special_token\n",
    "    \n",
    "    N_updated_false_vocab_df = len(updated_false_vocab_df)\n",
    "    \n",
    "    new_row = pd.DataFrame([[special_token, N_updated_false_vocab_df]], columns=updated_vocab_df.columns)\n",
    "    final_df = pd.concat([new_row, updated_vocab_df], ignore_index=True)\n",
    "    N_vocab = range(0, len(updated_vocab_df)+1)\n",
    "    \n",
    "    final_df[\"index\"] = N_vocab\n",
    "    \n",
    "    final_df = final_df.reindex(columns=[word_col_name, \"index\", count_col_name])\n",
    "    if save_df == True:\n",
    "        final_df.to_csv(save_path_with_name, header=None, index=None, sep='\\t')\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_56106/3585010688.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  updated_false_vocab_df[word_col_name] = special_token\n"
     ]
    }
   ],
   "source": [
    "word_col_name = \"Word\"\n",
    "count_col_name = \"count\"\n",
    "special_token = \"< unk >\"\n",
    "save_df = False\n",
    "save_file_path_and_name = \"submit/vocab.txt\"\n",
    "updated_vocab_df = create_vocab_threshold_df(vocab_df, word_col_name, count_col_name, 3, special_token, save_df, save_file_path_and_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>index</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt; unk &gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>29443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>1</td>\n",
       "      <td>46476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>2</td>\n",
       "      <td>39533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dummy</td>\n",
       "      <td>3</td>\n",
       "      <td>38234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.</td>\n",
       "      <td>4</td>\n",
       "      <td>37452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13746</th>\n",
       "      <td>trafficking</td>\n",
       "      <td>13746</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13747</th>\n",
       "      <td>7.62</td>\n",
       "      <td>13747</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13748</th>\n",
       "      <td>gut</td>\n",
       "      <td>13748</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13749</th>\n",
       "      <td>17.3</td>\n",
       "      <td>13749</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13750</th>\n",
       "      <td>seminar</td>\n",
       "      <td>13750</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13751 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Word  index  count\n",
       "0          < unk >      0  29443\n",
       "1                ,      1  46476\n",
       "2              the      2  39533\n",
       "3            dummy      3  38234\n",
       "4                .      4  37452\n",
       "...            ...    ...    ...\n",
       "13746  trafficking  13746      4\n",
       "13747         7.62  13747      4\n",
       "13748          gut  13748      4\n",
       "13749         17.3  13749      4\n",
       "13750      seminar  13750      4\n",
       "\n",
       "[13751 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_vocab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Learning\n",
    "\n",
    "- Learn an HMM from the training data\n",
    "- **HMM Parameters:**\n",
    "  <div style=\"text-align: center;\">\n",
    "\n",
    "    $\n",
    "    \\text{Transition Probability (} t \\text{)}: \\quad t(s' \\mid s) = \\frac{\\text{count}(s \\rightarrow s')}{\\text{count}(s)}\n",
    "    $\n",
    "\n",
    "    $\n",
    "    \\text{Emission Probability (} e \\text{)}: \\quad e(x \\mid s) = \\frac{\\text{count}(s \\rightarrow x)}{\\text{count}(s)}\n",
    "    $\n",
    "\n",
    "  </div>\n",
    "\n",
    "---\n",
    "\n",
    "1. [x] Learn a model using the training data in the file train\n",
    "2. [x] Output the learned model into a model file in json format, named `hmm.json`. The model file should contains two dictionaries for the emission and transition parameters, respectively.\n",
    "    1. [x] 1st dictionary: Named transition, contains items with pairs of (s, s′) as key and t(s′|s) as value. \n",
    "    2. [x] 2nd dictionary: Named emission, contains items with pairs of (s, x) as key and e(x|s) as value.\n",
    "3. Question\n",
    "    1. [x] How many transition and emission parameters in your HMM? transition = 1416. emission = 50287\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated_train_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(df, word_col_name, pos_tag_col_name, prev_pos_tag_col_name):\n",
    "    \"\"\"Count the transition and emission states, respectively\"\"\"\n",
    "    transition_states = defaultdict(int)\n",
    "    emission_state_word = defaultdict(int)\n",
    "    N_state = defaultdict(int)\n",
    "    \n",
    "    df[prev_pos_tag_col_name] = df[pos_tag_col_name].shift(1) # previous state for trnasition probabilities\n",
    "\n",
    "    # iterate through vocabulary\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "\n",
    "        emission_state_word[(row[pos_tag_col_name], row[word_col_name])] += 1\n",
    "        # transition count + 1\n",
    "        if pd.notnull(row[prev_pos_tag_col_name]):  # Check if it's not NaN\n",
    "            transition_states[(row[prev_pos_tag_col_name], row[pos_tag_col_name])] += 1\n",
    "\n",
    "        # increment tag when I see it\n",
    "        N_state[(row[pos_tag_col_name])] += 1\n",
    "\n",
    "    return transition_states, emission_state_word, N_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 950313/950313 [00:42<00:00, 22285.44it/s]\n"
     ]
    }
   ],
   "source": [
    "pos_tag_col_name = \"POS Tag\"\n",
    "prev_pos_tag_col_name = 'Previous_POS Tag'\n",
    "transitions, emissions, N_states = get_counts(updated_train_df, word_col_name, pos_tag_col_name, prev_pos_tag_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prob(transitions: dict, emissions: dict, N_states: dict, prob_type: str):   \n",
    "    \"\"\"Calculate the transistion and emissions probabilities, respectively\"\"\"\n",
    "\n",
    "    if prob_type == \"t\":\n",
    "        t_or_e = transitions\n",
    "    elif prob_type == \"e\":\n",
    "        t_or_e = emissions\n",
    "\n",
    "    store_probs = {}\n",
    "    for key, value in t_or_e.items():\n",
    "        \n",
    "        curr_state = key[0]       \n",
    "        store_probs[key] = value / N_states[curr_state]\n",
    "        \n",
    "    return store_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_probs = calculate_prob(transitions, emissions, N_states, 't')\n",
    "e_probs = calculate_prob(transitions, emissions, N_states, 'e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('dummy', 'NNP'), 0.19789104610393007),\n",
       " (('NNP', 'NNP'), 0.3782645420509543),\n",
       " (('NNP', ','), 0.13846908958086018),\n",
       " ((',', 'CD'), 0.021234939759036144),\n",
       " (('CD', 'NNS'), 0.15775891730703062),\n",
       " (('NNS', 'JJ'), 0.017196978862406887),\n",
       " (('JJ', ','), 0.029129343105320303)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(t_probs.items())[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('dummy', ' '), 2.6165681092678842e-05),\n",
       " (('NNP', 'Pierre'), 6.84868961738654e-05),\n",
       " (('NNP', 'Vinken'), 2.2828965391288468e-05),\n",
       " ((',', ','), 0.9999139414802065),\n",
       " (('CD', '61'), 0.0007168253240050465),\n",
       " (('NNS', 'years'), 0.019530237301024905),\n",
       " (('JJ', 'old'), 0.003613599348534202)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(e_probs.items())[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_with_name = \"submit/hmm.json\"\n",
    "\n",
    "combine_t_and_e_probs = {}\n",
    "combine_t_and_e_probs[\"transitions\"] = t_probs\n",
    "combine_t_and_e_probs[\"emissions\"] = e_probs\n",
    "\n",
    "t_e_probs_df = pd.DataFrame(combine_t_and_e_probs)\n",
    "# t_e_probs_df.to_json(save_path_with_name)\n",
    "\n",
    "# json_object = json.dumps(combine_t_and_e_probs)\n",
    "# with open(save_path_with_name, 'w') as json_file:\n",
    "#     json_file.write(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Greedy Decoding with HMM\n",
    "\n",
    "1. [x] Implement the greedy decoding algorithm\n",
    "2. [x] Evaluate it on the development data\n",
    "3. [x] Predicting the POS Tags of the sentences in the test data\n",
    "4. [x] Output the predictions in a file named `greedy.out`, in the same format of training data\n",
    "5. [x] Evaluate the results of the model on `eval.py` in the terminal with `python eval.py − p {predicted file} − g {gold-standard file}`\n",
    "6. [x] Question\n",
    "    1. [x] What is the accuracy on the dev data? 80.99% which is not great. Need more training data to improve accuracy. Also need to learn how to write correct and efficient code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated_dev_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(dev_df: pd.DataFrame, t_probs: dict, e_probs: dict, N_pos_tags: np.array):\n",
    "    \"\"\"Implement greedy decoding on the development file (words only) using the transition probability and emission probability. \n",
    "    Furthermore, don't use POS Tag of development file, thus only use POS Tag from training data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        Dev file\n",
    "\n",
    "    t_probs: `py dict`\n",
    "        Tranision probabilities for POS Tag given previous POS Tag\n",
    "\n",
    "    e_probs: `py dict`\n",
    "        Emission probabilities for Word given POS Tags\n",
    "\n",
    "    N_pos_tags: `np.array`\n",
    "        All POS Tags found in the training file\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    previous_pos_tag = \"dummy\"\n",
    "    all_words_with_pos_tag = []\n",
    "    \n",
    "    \n",
    "    for index, row in tqdm(dev_df.iterrows(), total=dev_df.shape[0]):\n",
    "        # print(\"index\", index, \"with word\", row['Word'])\n",
    "        if row['POS Tag'] != \"dummy\":\n",
    "\n",
    "            # Store per current word with all possible tags. Empty when at new word\n",
    "            store_scores = []\n",
    "            \n",
    "            for N_pos_tags_idx in range(len(N_pos_tags)):\n",
    "                current_pos_tag = N_pos_tags[N_pos_tags_idx]\n",
    "                # print(\"- Current POS Tag: \", current_pos_tag)\n",
    "    \n",
    "                \"\"\"Transition\n",
    "                Pr(t_find_pos_tag | t_given_pos_tag)\n",
    "                \"\"\"\n",
    "                t_find_pos_tag = current_pos_tag\n",
    "                t_given_pos_tag = previous_pos_tag\n",
    "                # print(f\"--- t({t_find_pos_tag} | {t_given_pos_tag})\")\n",
    "                \n",
    "                \"\"\"Emission\n",
    "                Pr(e_word | e_given_pos_tag)\n",
    "                \"\"\"\n",
    "                e_word = row['Word']\n",
    "                e_given_pos_tag = current_pos_tag\n",
    "                # print(f\"--- e({e_given_pos_tag} | {e_word})\") # order this way to match e_probs dictionary\n",
    "                \n",
    "                \"\"\"Transition * Emission\"\"\"\n",
    "                t_key = (t_find_pos_tag, t_given_pos_tag)\n",
    "                e_key = (e_given_pos_tag, e_word)\n",
    "                # print(t_key in t_probs, e_key in e_probs)\n",
    "    \n",
    "                # IF-ELSE bc not all pairs will be found. If pair is found, use score, otherwise (pair isn't found) set score to 0.0.\n",
    "                if t_key in t_probs and e_key in e_probs:\n",
    "                    t = t_probs[t_key]\n",
    "                    e = e_probs[e_key]\n",
    "                    score = t * e\n",
    "                    # print(f\"---  t({t_find_pos_tag} | {t_given_pos_tag}) * e({e_word} | {e_given_pos_tag}) = {score}\")\n",
    "                    \n",
    "                else:\n",
    "                    t = 0.000001\n",
    "                    e = 0.000001\n",
    "                    score = t * e\n",
    "                    # print(f\"--- t({t_find_pos_tag} | {t_given_pos_tag}) * e({e_word} | {e_given_pos_tag}) = {score}\")\n",
    "                            \n",
    "                store_scores.append(score)\n",
    "            # print(f\"Scores\", store_scores)\n",
    "        \n",
    "                # print(\"Word is: \", row['Word'], \"with POS Tag of\", current_pos_tag)\n",
    "            max_score_idx = np.argmax(np.array(store_scores)) # use argmax to get the index of max score\n",
    "            \n",
    "            current_pos_tag = N_pos_tags[max_score_idx] # use the index of the max score to find which POS Tag to update to\n",
    "            # print(\"Word is: \", row['Word'], \"with POS Tag of\", current_pos_tag)\n",
    "            # all_words_with_pos_tag[row['Word']] = current_pos_tag\n",
    "            all_words_with_pos_tag.append([row['Word'], current_pos_tag])\n",
    "            \n",
    "            previous_pos_tag = current_pos_tag\n",
    "            # print(all_words_with_pos_tag)\n",
    "    \n",
    "            # print(\"Updated POS Tag\", current_pos_tag)\n",
    "            # print()\n",
    "        else:\n",
    "            empty = \"\"\n",
    "            all_words_with_pos_tag.append([empty, empty])\n",
    "        \n",
    "    \n",
    "    return all_words_with_pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated_dev_df[updated_dev_df[\"POS Tag\"] == \"dummy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(e_probs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████| 137295/137295 [00:20<00:00, 6651.36it/s]\n"
     ]
    }
   ],
   "source": [
    "gd_output = greedy_decoding(updated_dev_df, t_probs, e_probs, all_pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'DT'],\n",
       " ['Arizona', 'NNP'],\n",
       " ['Corporations', 'NNS'],\n",
       " ['Commission', 'FW'],\n",
       " ['authorized', 'VBD'],\n",
       " ['an', 'DT'],\n",
       " ['11.5', 'CD'],\n",
       " ['%', 'NN'],\n",
       " ['rate', 'NN'],\n",
       " ['increase', 'VB'],\n",
       " ['at', 'IN'],\n",
       " ['Tucson', 'NNP'],\n",
       " ['Electric', 'NNP'],\n",
       " ['Power', 'NNP'],\n",
       " ['Co.', 'NNP'],\n",
       " [',', ','],\n",
       " ['substantially', 'RB'],\n",
       " ['lower', 'RBR'],\n",
       " ['than', 'IN'],\n",
       " ['recommended', 'VBN'],\n",
       " ['last', 'VB'],\n",
       " ['month', 'NN'],\n",
       " ['by', 'IN'],\n",
       " ['a', 'SYM'],\n",
       " ['commission', 'NN'],\n",
       " ['hearing', 'VBG'],\n",
       " ['officer', 'NN'],\n",
       " ['and', 'CC'],\n",
       " ['barely', 'RB'],\n",
       " ['half', 'NN'],\n",
       " ['the', 'DT'],\n",
       " ['rise', 'VB'],\n",
       " ['sought', 'VBD'],\n",
       " ['by', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['utility', 'NN'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['The', 'DT'],\n",
       " ['ruling', 'VBG'],\n",
       " ['follows', 'VBZ'],\n",
       " ['a', 'SYM'],\n",
       " ['host', 'NN'],\n",
       " ['of', 'IN'],\n",
       " ['problems', 'NNS'],\n",
       " ['at', 'IN'],\n",
       " ['Tucson', 'NNP'],\n",
       " ['Electric', 'NNP'],\n",
       " [',', ','],\n",
       " ['including', 'VBG'],\n",
       " ['major', 'JJ'],\n",
       " ['write-downs', 'NNS'],\n",
       " [',', ','],\n",
       " ['a', 'LS'],\n",
       " ['60', 'dummy'],\n",
       " ['%', 'NN'],\n",
       " ['slash', 'VB'],\n",
       " ['in', 'RB'],\n",
       " ['the', 'DT'],\n",
       " ['common', 'JJ'],\n",
       " ['stock', 'NN'],\n",
       " ['dividend', 'NN'],\n",
       " ['and', 'CC'],\n",
       " ['the', 'DT'],\n",
       " ['departure', 'NN'],\n",
       " ['of', 'IN'],\n",
       " ['former', 'JJ'],\n",
       " ['Chairman', 'NNP'],\n",
       " ['Einar', 'dummy'],\n",
       " ['Greve', 'dummy'],\n",
       " ['during', 'IN'],\n",
       " ['a', 'SYM'],\n",
       " ['company', 'NN'],\n",
       " ['investigation', 'NN'],\n",
       " ['of', 'IN'],\n",
       " ['his', 'PRP$'],\n",
       " ['stock', 'VB'],\n",
       " ['sales', 'NNS'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['The', 'DT'],\n",
       " ['Arizona', 'NNP'],\n",
       " ['regulatory', 'JJ'],\n",
       " ['ruling', 'VBG'],\n",
       " ['calls', 'VBZ'],\n",
       " ['for', 'IN'],\n",
       " ['$', 'dummy'],\n",
       " ['42', 'CD'],\n",
       " ['million', 'CD'],\n",
       " ['in', 'IN'],\n",
       " ['added', 'VBD'],\n",
       " ['revenue', 'NN'],\n",
       " ['yearly', 'JJ'],\n",
       " [',', ','],\n",
       " ['compared', 'VBN'],\n",
       " ['with', 'IN'],\n",
       " ['a', 'SYM'],\n",
       " ['$', 'dummy'],\n",
       " ['57', 'CD'],\n",
       " ['million', 'CD'],\n",
       " ['boost', 'VB'],\n",
       " ['proposed', 'VBN'],\n",
       " ['by', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['commission', 'NN'],\n",
       " ['hearing', 'VBG'],\n",
       " ['officer', 'NN'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['The', 'DT'],\n",
       " ['company', 'NN'],\n",
       " ['had', 'VBD'],\n",
       " ['sought', 'VBN'],\n",
       " ['increases', 'VBZ'],\n",
       " ['totaling', 'VBG'],\n",
       " ['$', 'dummy'],\n",
       " ['80.3', 'dummy'],\n",
       " ['million', 'CD'],\n",
       " [',', ','],\n",
       " ['or', 'CC'],\n",
       " ['22', 'CD'],\n",
       " ['%', 'NN'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['The', 'DT'],\n",
       " ['decision', 'NN'],\n",
       " ['was', 'VBD'],\n",
       " ['announced', 'VBD'],\n",
       " ['after', 'RB'],\n",
       " ['trading', 'VBG'],\n",
       " ['ended', 'VBD'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['Tucson', 'NNP'],\n",
       " ['Electric', 'NNP'],\n",
       " ['closed', 'VBD'],\n",
       " ['at', 'IN'],\n",
       " ['$', 'dummy'],\n",
       " ['20.875', 'dummy'],\n",
       " ['a', 'SYM'],\n",
       " ['share', 'NN'],\n",
       " [',', ','],\n",
       " ['down', 'RP'],\n",
       " ['25', 'dummy'],\n",
       " ['cents', 'NNS'],\n",
       " [',', ','],\n",
       " ['in', 'RP'],\n",
       " ['New', 'NNP'],\n",
       " ['York', 'NNP'],\n",
       " ['Stock', 'NNP'],\n",
       " ['Exchange', 'NNP'],\n",
       " ['composite', 'JJ'],\n",
       " ['trading', 'VBG'],\n",
       " ['.', 'dummy'],\n",
       " ['', ''],\n",
       " ['A', 'DT'],\n",
       " ['Tucson', 'NNP'],\n",
       " ['Electric', 'NNP'],\n",
       " ['spokesman', 'NN'],\n",
       " ['said', 'VBD'],\n",
       " ['the', 'DT'],\n",
       " ['utility', 'NN'],\n",
       " ['was', 'VBD'],\n",
       " ['disappointed', 'VBN'],\n",
       " ['by', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['commission', 'NN'],\n",
       " [\"'s\", 'POS'],\n",
       " ['decision', 'NN'],\n",
       " ['and', 'CC'],\n",
       " ['``', '``'],\n",
       " ['concerned', 'VBN'],\n",
       " ['about', 'RB'],\n",
       " ['the', 'DT'],\n",
       " ['financial', 'JJ'],\n",
       " ['integrity', 'NN'],\n",
       " ['of', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['company', 'NN'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['South', 'NNP'],\n",
       " ['Korean', 'JJ'],\n",
       " ['President', 'NNP'],\n",
       " ['Roh', 'NNP'],\n",
       " ['Tae', 'NNP'],\n",
       " ['Woo', 'NNP'],\n",
       " [',', ','],\n",
       " ['brushing', 'dummy'],\n",
       " ['aside', 'RP'],\n",
       " ['suggestions', 'NNS'],\n",
       " ['that', 'WDT'],\n",
       " ['the', 'DT'],\n",
       " ['won', 'VBD'],\n",
       " ['be', 'VB'],\n",
       " ['revalued', 'dummy'],\n",
       " ['again', 'RB'],\n",
       " [',', ','],\n",
       " ['said', 'VBD'],\n",
       " ['the', 'DT'],\n",
       " ['currency', 'NN'],\n",
       " [\"'s\", 'POS'],\n",
       " ['current', 'NN'],\n",
       " ['level', 'NN'],\n",
       " ['against', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['dollar', 'NN'],\n",
       " ['is', 'VBZ'],\n",
       " ['``', '``'],\n",
       " ['appropriate', 'JJ'],\n",
       " ['.', '.'],\n",
       " [\"''\", \"''\"],\n",
       " ['', ''],\n",
       " ['His', 'dummy'],\n",
       " ['comments', 'NNS'],\n",
       " [',', ','],\n",
       " ['made', 'VBN'],\n",
       " ['in', 'IN'],\n",
       " ['response', 'NN'],\n",
       " ['to', 'TO'],\n",
       " ['reporters', 'NNS'],\n",
       " [\"'\", 'POS'],\n",
       " ['questions', 'NNS'],\n",
       " ['at', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['National', 'NNP'],\n",
       " ['Press', 'NNP'],\n",
       " ['Club', 'NNP'],\n",
       " ['here', 'RB'],\n",
       " [',', ','],\n",
       " ['signaled', 'VBD'],\n",
       " ['that', 'WDT'],\n",
       " ['Seoul', 'NNP'],\n",
       " ['is', 'VBZ'],\n",
       " ['resisting', 'VBG'],\n",
       " ['U.S.', 'NNP'],\n",
       " ['pressure', 'NN'],\n",
       " ['for', 'IN'],\n",
       " ['a', 'SYM'],\n",
       " ['further', 'dummy'],\n",
       " ['rise', 'NN'],\n",
       " ['in', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['currency', 'NN'],\n",
       " [\"'s\", 'POS'],\n",
       " ['value', 'NN'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['The', 'DT'],\n",
       " ['U.S.', 'NNP'],\n",
       " ['wants', 'VBZ'],\n",
       " ['a', 'SYM'],\n",
       " ['higher', 'dummy'],\n",
       " ['won', 'VBD'],\n",
       " ['to', 'TO'],\n",
       " ['make', 'VB'],\n",
       " ['South', 'NNP'],\n",
       " ['Korea', 'NNP'],\n",
       " [\"'s\", 'POS'],\n",
       " ['exports', 'NNS'],\n",
       " ['more', 'JJR'],\n",
       " ['expensive', 'JJ'],\n",
       " ['and', 'CC'],\n",
       " ['help', 'VB'],\n",
       " ['trim', 'VB'],\n",
       " ['Seoul', 'NNP'],\n",
       " [\"'s\", 'POS'],\n",
       " ['trade', 'NN'],\n",
       " ['surplus', 'NN'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['Many', 'JJ'],\n",
       " ['South', 'JJ'],\n",
       " ['Korean', 'JJ'],\n",
       " ['business', 'NN'],\n",
       " ['people', 'NNS'],\n",
       " ['want', 'VBP'],\n",
       " ['a', 'DT'],\n",
       " ['devaluation', 'NN'],\n",
       " ['instead', 'RB'],\n",
       " [',', ','],\n",
       " ['arguing', 'VBG'],\n",
       " ['that', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['won', 'VBD'],\n",
       " [\"'s\", 'POS'],\n",
       " ['recent', 'JJ'],\n",
       " ['gains', 'NNS'],\n",
       " ['already', 'RB'],\n",
       " ['have', 'VBP'],\n",
       " ['weakened', 'VBN'],\n",
       " ['the', 'DT'],\n",
       " ['country', 'NN'],\n",
       " [\"'s\", 'POS'],\n",
       " ['export', 'NN'],\n",
       " ['performance', 'NN'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['Mr.', 'NNP'],\n",
       " ['Roh', 'NNP'],\n",
       " ['also', 'RB'],\n",
       " ['said', 'VBD'],\n",
       " ['South', 'NNP'],\n",
       " ['Korea', 'NNP'],\n",
       " ['is', 'VBZ'],\n",
       " ['taking', 'VBG'],\n",
       " ['steps', 'NNS'],\n",
       " ['that', 'WDT'],\n",
       " ['would', 'dummy'],\n",
       " ['free', 'JJ'],\n",
       " ['the', 'DT'],\n",
       " ['won', 'VBD'],\n",
       " ['to', 'TO'],\n",
       " ['respond', 'VB'],\n",
       " ['to', 'TO'],\n",
       " ['market', 'NN'],\n",
       " ['forces', 'NNS'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['Seoul', 'NNP'],\n",
       " ['has', 'VBZ'],\n",
       " ['pointed', 'VBN'],\n",
       " ['to', 'TO'],\n",
       " ['its', 'dummy'],\n",
       " ['lack', 'NN'],\n",
       " ['of', 'IN'],\n",
       " ['a', 'SYM'],\n",
       " ['foreign', 'dummy'],\n",
       " ['exchange', 'NN'],\n",
       " ['market', 'NN'],\n",
       " ['as', 'IN'],\n",
       " ['one', 'CD'],\n",
       " ['reason', 'NN'],\n",
       " ['the', 'DT'],\n",
       " ['won', 'VBD'],\n",
       " [\"'s\", 'POS'],\n",
       " ['value', 'NN'],\n",
       " ['remains', 'VBZ'],\n",
       " ['heavily', 'RB'],\n",
       " ['controlled', 'VBN'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['Mr.', 'NNP'],\n",
       " ['Roh', 'NNP'],\n",
       " ['said', 'VBD'],\n",
       " ['a', 'DT'],\n",
       " ['U.S.', 'NNP'],\n",
       " ['demand', 'VB'],\n",
       " ['for', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['removal', 'NN'],\n",
       " ['of', 'IN'],\n",
       " ['South', 'NNP'],\n",
       " ['Korean', 'JJ'],\n",
       " ['import', 'VBP'],\n",
       " ['quotas', 'NNS'],\n",
       " ['on', 'IN'],\n",
       " ['beef', 'NN'],\n",
       " ['will', 'MD'],\n",
       " ['be', 'VB'],\n",
       " ['resolved', 'VBN'],\n",
       " ['``', '``'],\n",
       " ['satisfactorily', 'RB'],\n",
       " [\"''\", \"''\"],\n",
       " ['but', 'RB'],\n",
       " ['gave', 'VBD'],\n",
       " ['no', 'RB'],\n",
       " ['hint', 'VBP'],\n",
       " ['when', 'WRB'],\n",
       " ['that', 'IN'],\n",
       " ['will', 'MD'],\n",
       " ['happen', 'VB'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['Speaking', 'VBG'],\n",
       " ['to', 'TO'],\n",
       " ['a', 'FW'],\n",
       " ['joint', 'JJ'],\n",
       " ['meeting', 'VBG'],\n",
       " ['of', 'IN'],\n",
       " ['Congress', 'NNP'],\n",
       " ['earlier', 'JJR'],\n",
       " [',', ','],\n",
       " ['he', 'PRP'],\n",
       " ['said', 'VBD'],\n",
       " ['South', 'NNP'],\n",
       " ['Korea', 'NNP'],\n",
       " ['ca', 'MD'],\n",
       " [\"n't\", 'RB'],\n",
       " ['move', 'VBP'],\n",
       " ['quickly', 'RB'],\n",
       " ['on', 'RP'],\n",
       " ['such', 'JJ'],\n",
       " ['agricultural', 'JJ'],\n",
       " ['trade', 'VB'],\n",
       " ['issues', 'NNS'],\n",
       " ['``', '``'],\n",
       " ['without', 'IN'],\n",
       " ['causing', 'VBG'],\n",
       " ['political', 'JJ'],\n",
       " ['and', 'CC'],\n",
       " ['social', 'JJ'],\n",
       " ['trauma', 'dummy'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['Great', 'NNP'],\n",
       " ['American', 'NNP'],\n",
       " ['Bank', 'NNP'],\n",
       " ['said', 'VBD'],\n",
       " ['its', 'PRP$'],\n",
       " ['board', 'NN'],\n",
       " ['approved', 'VBN'],\n",
       " ['the', 'DT'],\n",
       " ['formation', 'NN'],\n",
       " ['of', 'IN'],\n",
       " ['a', 'SYM'],\n",
       " ['holding', 'VBG'],\n",
       " ['company', 'NN'],\n",
       " ['enabling', 'VBG'],\n",
       " ['the', 'DT'],\n",
       " ['savings', 'NNS'],\n",
       " ['bank', 'NN'],\n",
       " ['to', 'TO'],\n",
       " ['pursue', 'VB'],\n",
       " ['nontraditional', 'dummy'],\n",
       " ['banking', 'NN'],\n",
       " ['activities', 'NNS'],\n",
       " ['under', 'IN'],\n",
       " ['a', 'SYM'],\n",
       " ['new', 'dummy'],\n",
       " ['federal', 'JJ'],\n",
       " ['law', 'NN'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['The', 'DT'],\n",
       " ['proposed', 'VBN'],\n",
       " ['holding', 'VBG'],\n",
       " ['company', 'NN'],\n",
       " [\"'s\", 'POS'],\n",
       " ['primary', 'NN'],\n",
       " ['purpose', 'NN'],\n",
       " ['would', 'MD'],\n",
       " ['be', 'VB'],\n",
       " ['to', 'TO'],\n",
       " ['allow', 'VB'],\n",
       " ['Great', 'NNP'],\n",
       " ['American', 'NNP'],\n",
       " ['to', 'TO'],\n",
       " ['continue', 'VB'],\n",
       " ['engaging', 'VBG'],\n",
       " ['in', 'IN'],\n",
       " ['real', 'JJ'],\n",
       " ['estate', 'NN'],\n",
       " ['development', 'NN'],\n",
       " ['activities', 'NNS'],\n",
       " [',', ','],\n",
       " ['it', 'PRP'],\n",
       " ['said', 'VBD'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['Those', 'DT'],\n",
       " ['activities', 'NNS'],\n",
       " ['generated', 'VBN'],\n",
       " ['$', 'dummy'],\n",
       " ['26.1', 'CD'],\n",
       " ['million', 'CD'],\n",
       " ['in', 'IN'],\n",
       " ['operating', 'VBG'],\n",
       " ['profit', 'NN'],\n",
       " ['last', 'JJ'],\n",
       " ['year', 'NN'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['But', 'NNP'],\n",
       " ['according', 'VBG'],\n",
       " ['to', 'TO'],\n",
       " ['Great', 'JJ'],\n",
       " ['American', 'JJ'],\n",
       " [',', ','],\n",
       " ['such', 'JJ'],\n",
       " ['profits', 'NNS'],\n",
       " ['do', 'VBP'],\n",
       " [\"n't\", 'RB'],\n",
       " ['count', 'VB'],\n",
       " ['toward', 'IN'],\n",
       " ['meeting', 'NN'],\n",
       " ['the', 'DT'],\n",
       " ['San', 'NNP'],\n",
       " ['Diego', 'NNP'],\n",
       " ['savings', 'NNS'],\n",
       " ['bank', 'NN'],\n",
       " [\"'s\", 'POS'],\n",
       " ['new', 'JJ'],\n",
       " ['capitalization', 'NN'],\n",
       " ['requirements', 'NNS'],\n",
       " ['under', 'IN'],\n",
       " ['1989', 'CD'],\n",
       " ['federal', 'JJ'],\n",
       " ['law', 'NN'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['The', 'DT'],\n",
       " ['new', 'JJ'],\n",
       " ['real', 'JJ'],\n",
       " ['estate', 'NN'],\n",
       " ['unit', 'NN'],\n",
       " ['would', 'MD'],\n",
       " ['have', 'VBP'],\n",
       " ['a', 'DT'],\n",
       " ['separate', 'VB'],\n",
       " ['capital', 'NN'],\n",
       " ['structure', 'NN'],\n",
       " ['to', 'TO'],\n",
       " ['comply', 'VB'],\n",
       " ['with', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['law', 'NN'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['The', 'DT'],\n",
       " ['proposed', 'VBN'],\n",
       " ['holding', 'VBG'],\n",
       " ['company', 'NN'],\n",
       " ['would', 'MD'],\n",
       " ['also', 'RB'],\n",
       " ['consolidate', 'VB'],\n",
       " ['Great', 'NNP'],\n",
       " ['American', 'NNP'],\n",
       " ['Bank', 'NNP'],\n",
       " ['in', 'IN'],\n",
       " ['San', 'NNP'],\n",
       " ['Diego', 'NNP'],\n",
       " ['and', 'CC'],\n",
       " ['its', 'PRP$'],\n",
       " ['Tucson', 'NNP'],\n",
       " [',', ','],\n",
       " ['Ariz.', 'NNP'],\n",
       " [',', ','],\n",
       " ['savings', 'NNS'],\n",
       " ['bank', 'NN'],\n",
       " ['into', 'IN'],\n",
       " ['a', 'SYM'],\n",
       " ['single', 'dummy'],\n",
       " [',', ','],\n",
       " ['federally', 'RB'],\n",
       " ['chartered', 'VBN'],\n",
       " ['institution', 'NN'],\n",
       " ['in', 'IN'],\n",
       " ['San', 'NNP'],\n",
       " ['Diego', 'NNP'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['The', 'DT'],\n",
       " ['consolidation', 'NN'],\n",
       " ['is', 'VBZ'],\n",
       " ['expected', 'VBN'],\n",
       " ['to', 'TO'],\n",
       " ['save', 'VB'],\n",
       " ['$', 'dummy'],\n",
       " ['1', 'CD'],\n",
       " ['million', 'CD'],\n",
       " ['a', 'SYM'],\n",
       " ['year', 'NN'],\n",
       " ['in', 'IN'],\n",
       " ['administrative', 'JJ'],\n",
       " ['costs', 'VBZ'],\n",
       " [',', ','],\n",
       " ['a', 'LS'],\n",
       " ['Great', 'dummy'],\n",
       " ['American', 'NNP'],\n",
       " ['spokesman', 'NN'],\n",
       " ['said', 'VBD'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['Dale', 'NNP'],\n",
       " ['Lang', 'NNP'],\n",
       " [',', ','],\n",
       " ['who', 'WP'],\n",
       " ['this', 'DT'],\n",
       " ['week', 'NN'],\n",
       " ['completed', 'VBN'],\n",
       " ['the', 'DT'],\n",
       " ['acquisition', 'NN'],\n",
       " ['of', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['publisher', 'NN'],\n",
       " ['of', 'IN'],\n",
       " ['Ms.', 'NNP'],\n",
       " ['and', 'CC'],\n",
       " ['Sassy', 'dummy'],\n",
       " [',', ','],\n",
       " ['is', 'VBZ'],\n",
       " ['candid', 'JJ'],\n",
       " ['about', 'RB'],\n",
       " ['the', 'DT'],\n",
       " ['challenge', 'VB'],\n",
       " ['he', 'PRP'],\n",
       " ['is', 'VBZ'],\n",
       " ['taking', 'VBG'],\n",
       " ['on', 'IN'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['Mr.', 'NNP'],\n",
       " ['Lang', 'NNP'],\n",
       " ['admits', 'VBZ'],\n",
       " ['that', 'WDT'],\n",
       " ['Ms.', 'NNP'],\n",
       " ['is', 'VBZ'],\n",
       " ['``', '``'],\n",
       " ['in', 'IN'],\n",
       " ['dire', 'FW'],\n",
       " ['straits', 'dummy'],\n",
       " [\"''\", \"''\"],\n",
       " ['and', 'JJ'],\n",
       " ['that', 'IN'],\n",
       " ['Sassy', 'dummy'],\n",
       " ['needs', 'NNS'],\n",
       " ['big', 'JJ'],\n",
       " ['promotional', 'JJ'],\n",
       " ['dollars', 'NNS'],\n",
       " ['to', 'TO'],\n",
       " ['keep', 'VB'],\n",
       " ['it', 'PRP'],\n",
       " ['alive', 'JJ'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['But', 'NNP'],\n",
       " ['the', 'DT'],\n",
       " ['57-year-old', 'JJ'],\n",
       " ['publisher', 'NN'],\n",
       " ['has', 'VBZ'],\n",
       " ['moved', 'VBN'],\n",
       " ['quickly', 'RB'],\n",
       " ['and', 'CC'],\n",
       " ['boldly', 'RB'],\n",
       " ['to', 'TO'],\n",
       " ['deal', 'NN'],\n",
       " ['with', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['magazines', 'NNS'],\n",
       " [\"'\", 'POS'],\n",
       " ['problems', 'NNS'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['Last', 'JJ'],\n",
       " ['Friday', 'NNP'],\n",
       " [',', ','],\n",
       " ['he', 'PRP'],\n",
       " ['told', 'VBD'],\n",
       " ['the', 'DT'],\n",
       " ['staff', 'VB'],\n",
       " ['of', 'IN'],\n",
       " ['Ms.', 'NNP'],\n",
       " ['that', 'WDT'],\n",
       " ['the', 'DT'],\n",
       " ['magazine', 'NN'],\n",
       " ['in', 'IN'],\n",
       " ['January', 'NNP'],\n",
       " ['would', 'MD'],\n",
       " ['begin', 'VB'],\n",
       " ['publishing', 'VBG'],\n",
       " ['without', 'IN'],\n",
       " ['advertising', 'NN'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['Mr.', 'NNP'],\n",
       " ['Lang', 'NNP'],\n",
       " ['will', 'MD'],\n",
       " ['do', 'VBP'],\n",
       " ['away', 'RB'],\n",
       " ['with', 'IN'],\n",
       " ['expensive', 'JJ'],\n",
       " ['circulation', 'NN'],\n",
       " ['drives', 'NNS'],\n",
       " [',', ','],\n",
       " ['not', 'RB'],\n",
       " ['to', 'TO'],\n",
       " ['mention', 'VB'],\n",
       " ['sales', 'NNS'],\n",
       " ['staff', 'NN'],\n",
       " [',', ','],\n",
       " ['and', 'CC'],\n",
       " ['attempt', 'NN'],\n",
       " ['to', 'TO'],\n",
       " ['publish', 'VB'],\n",
       " ['the', 'DT'],\n",
       " ['17-year-old', 'JJ'],\n",
       " ['magazine', 'NN'],\n",
       " ['supported', 'VBN'],\n",
       " ['by', 'IN'],\n",
       " ['circulation', 'NN'],\n",
       " ['revenue', 'NN'],\n",
       " ['alone', 'JJ'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['``', 'dummy'],\n",
       " ['Any', 'DT'],\n",
       " ['fool', 'VB'],\n",
       " ['can', 'MD'],\n",
       " ['publish', 'VB'],\n",
       " ['a', 'DT'],\n",
       " ['money-losing', 'JJ'],\n",
       " ['magazine', 'NN'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['I', 'PRP'],\n",
       " ['want', 'VBP'],\n",
       " ['to', 'TO'],\n",
       " ['publish', 'VB'],\n",
       " ['one', 'PRP'],\n",
       " ['that', 'WDT'],\n",
       " ['succeeds', 'VBZ'],\n",
       " [',', ','],\n",
       " [\"''\", \"''\"],\n",
       " ['said', 'VBD'],\n",
       " ['Mr.', 'NNP'],\n",
       " ['Lang', 'NNP'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['``', 'dummy'],\n",
       " ['For', 'IN'],\n",
       " ['Ms.', 'NNP'],\n",
       " [',', ','],\n",
       " ['it', 'PRP'],\n",
       " [\"'s\", 'VBZ'],\n",
       " ['time', 'NN'],\n",
       " ['to', 'TO'],\n",
       " ['publish', 'VB'],\n",
       " ['for', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['reader', 'NN'],\n",
       " [',', ','],\n",
       " ['not', 'RB'],\n",
       " ['the', 'DT'],\n",
       " ['advertiser', 'NN'],\n",
       " ['.', '.'],\n",
       " [\"''\", \"''\"],\n",
       " ['', ''],\n",
       " ['As', 'RB'],\n",
       " ['for', 'IN'],\n",
       " ['Sassy', 'dummy'],\n",
       " [',', ','],\n",
       " ['which', 'WDT'],\n",
       " ['competes', 'VBZ'],\n",
       " ['directly', 'RB'],\n",
       " ['with', 'IN'],\n",
       " ['News', 'NNP'],\n",
       " ['Corp.', 'NNP'],\n",
       " [\"'s\", 'POS'],\n",
       " ['Seventeen', 'CD'],\n",
       " ['magazine', 'NN'],\n",
       " [',', ','],\n",
       " ['Mr.', 'NNP'],\n",
       " ['Lang', 'NNP'],\n",
       " ['says', 'VBZ'],\n",
       " ['that', 'WDT'],\n",
       " ['in', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['next', 'IN'],\n",
       " ['two', 'CD'],\n",
       " ['years', 'NNS'],\n",
       " ['he', 'PRP'],\n",
       " ['will', 'MD'],\n",
       " ['spend', 'VBP'],\n",
       " ['$', 'dummy'],\n",
       " ['6', 'CD'],\n",
       " ['million', 'CD'],\n",
       " ['promoting', 'VBG'],\n",
       " ['and', 'CC'],\n",
       " ['improving', 'VBG'],\n",
       " ['the', 'DT'],\n",
       " ['magazine', 'NN'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['Though', 'NNP'],\n",
       " ['Sassy', 'dummy'],\n",
       " ['has', 'VBZ'],\n",
       " ['grown', 'VBN'],\n",
       " ['quickly', 'RB'],\n",
       " ['since', 'IN'],\n",
       " ['its', 'PRP$'],\n",
       " ['debut', 'VB'],\n",
       " ['in', 'RB'],\n",
       " ['March', 'NNP'],\n",
       " ['1988', 'CD'],\n",
       " [',', ','],\n",
       " ['it', 'PRP'],\n",
       " ['has', 'VBZ'],\n",
       " ['been', 'VBN'],\n",
       " ['the', 'DT'],\n",
       " ['target', 'VB'],\n",
       " ['of', 'IN'],\n",
       " ['conservative', 'JJ'],\n",
       " ['lobbyists', 'NNS'],\n",
       " ['and', 'CC'],\n",
       " ['skittish', 'JJ'],\n",
       " ['advertisers', 'NNS'],\n",
       " ['who', 'WP'],\n",
       " ['bristled', 'dummy'],\n",
       " ['at', 'IN'],\n",
       " ['its', 'PRP$'],\n",
       " ['frank', 'JJ'],\n",
       " ['editorial', 'JJ'],\n",
       " ['matter', 'VB'],\n",
       " ['on', 'RB'],\n",
       " ['teen-age', 'JJ'],\n",
       " ['problems', 'NNS'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['Mr.', 'NNP'],\n",
       " ['Lang', 'NNP'],\n",
       " ['said', 'VBD'],\n",
       " ['the', 'DT'],\n",
       " ['former', 'JJ'],\n",
       " ['Australian', 'JJ'],\n",
       " ['owners', 'NNS'],\n",
       " ['of', 'IN'],\n",
       " ['Sassy', 'dummy'],\n",
       " ['were', 'VBD'],\n",
       " ['``', '``'],\n",
       " ['blind-sided', 'JJ'],\n",
       " ['by', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['Moral', 'dummy'],\n",
       " ['Majority', 'NNP'],\n",
       " ['...', ':'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['Their', 'PRP$'],\n",
       " ['reaction', 'NN'],\n",
       " ['was', 'VBD'],\n",
       " ['to', 'TO'],\n",
       " ['do', 'VBP'],\n",
       " ['nothing', 'NN'],\n",
       " ['and', 'CC'],\n",
       " ['ride', 'NN'],\n",
       " ['it', 'PRP'],\n",
       " ['out', 'RP'],\n",
       " ['.', 'dummy'],\n",
       " [\"''\", \"''\"],\n",
       " ['', ''],\n",
       " ['He', 'PRP'],\n",
       " ['said', 'VBD'],\n",
       " ['Sassy', 'dummy'],\n",
       " ['will', 'NN'],\n",
       " ['keep', 'VB'],\n",
       " ['its', 'dummy'],\n",
       " ['irreverent', 'JJ'],\n",
       " ['tone', 'NN'],\n",
       " [',', ','],\n",
       " ['but', 'CC'],\n",
       " ['added', 'VBN'],\n",
       " [',', ','],\n",
       " ['``', 'dummy'],\n",
       " ['We', 'dummy'],\n",
       " ['will', 'NN'],\n",
       " ['keep', 'VB'],\n",
       " ['a', 'DT'],\n",
       " ['close', 'VB'],\n",
       " ['watch', 'VB'],\n",
       " ['on', 'RB'],\n",
       " ['the', 'DT'],\n",
       " ['editorial', 'NN'],\n",
       " ['content', 'JJ'],\n",
       " ['of', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['magazine', 'NN'],\n",
       " ['.', '.'],\n",
       " [\"''\", \"''\"],\n",
       " ['', ''],\n",
       " ['Sassy', 'dummy'],\n",
       " ['already', 'RB'],\n",
       " ['has', 'VBZ'],\n",
       " ['recovered', 'VBN'],\n",
       " [';', ':'],\n",
       " ['circulation', 'NN'],\n",
       " ['has', 'VBZ'],\n",
       " ['quickly', 'RB'],\n",
       " ['passed', 'VBN'],\n",
       " ['the', 'DT'],\n",
       " ['500,000', 'CD'],\n",
       " ['mark', 'VB'],\n",
       " ['and', 'CC'],\n",
       " ['advertising', 'NN'],\n",
       " ['pages', 'NNS'],\n",
       " ['have', 'VBP'],\n",
       " ['stabilized', 'VBN'],\n",
       " ['this', 'DT'],\n",
       " ['year', 'NN'],\n",
       " ['at', 'IN'],\n",
       " ['more', 'JJR'],\n",
       " ['than', 'IN'],\n",
       " ['300', 'CD'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['What', 'WP'],\n",
       " [\"'s\", 'POS'],\n",
       " ['more', 'JJ'],\n",
       " [',', ','],\n",
       " ['Mr.', 'NNP'],\n",
       " ['Lang', 'NNP'],\n",
       " ['says', 'VBZ'],\n",
       " ['he', 'PRP'],\n",
       " ['has', 'VBZ'],\n",
       " ['what', 'WP'],\n",
       " ['all', 'DT'],\n",
       " ['publishers', 'NNS'],\n",
       " ['wish', 'VBP'],\n",
       " ['for', 'IN'],\n",
       " [':', ':'],\n",
       " ['a', 'SYM'],\n",
       " ['bona', 'dummy'],\n",
       " ['fide', 'dummy'],\n",
       " ['niche', 'NN'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['``', 'dummy'],\n",
       " ['Seventeen', 'CD'],\n",
       " ['is', 'VBZ'],\n",
       " ['written', 'VBN'],\n",
       " ['more', 'RBR'],\n",
       " ['for', 'IN'],\n",
       " ['mothers', 'NNS'],\n",
       " [',', ','],\n",
       " ['not', 'RB'],\n",
       " ['their', 'PRP$'],\n",
       " ['daughters', 'NNS'],\n",
       " [',', ','],\n",
       " [\"''\", \"''\"],\n",
       " ['said', 'VBD'],\n",
       " ['Mr.', 'NNP'],\n",
       " ['Lang', 'NNP'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['``', 'dummy'],\n",
       " ['But', 'NNP'],\n",
       " ['Sassy', 'dummy'],\n",
       " ['has', 'VBZ'],\n",
       " ['a', 'SYM'],\n",
       " ['different', 'dummy'],\n",
       " ['spirit', 'NN'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['It', 'PRP'],\n",
       " ['gets', 'VBZ'],\n",
       " ['more', 'RBR'],\n",
       " ['mail', 'NN'],\n",
       " ['in', 'IN'],\n",
       " ['a', 'SYM'],\n",
       " ['month', 'NN'],\n",
       " ['than', 'IN'],\n",
       " ['McCall', 'NNP'],\n",
       " [\"'s\", 'POS'],\n",
       " ['got', 'VBN'],\n",
       " ['in', 'IN'],\n",
       " ['a', 'SYM'],\n",
       " ['year', 'NN'],\n",
       " [',', ','],\n",
       " ['and', 'CC'],\n",
       " ['it', 'PRP'],\n",
       " [\"'s\", 'VBZ'],\n",
       " ['not', 'RB'],\n",
       " ['from', 'IN'],\n",
       " ['mothers', 'NNS'],\n",
       " ['.', '.'],\n",
       " ['', ''],\n",
       " ['I', 'PRP'],\n",
       " ['feel', 'VBP'],\n",
       " ['about', 'RB'],\n",
       " ['Sassy', 'dummy'],\n",
       " ['like', 'IN'],\n",
       " ['I', 'PRP'],\n",
       " ['did', 'VBD'],\n",
       " ['about', 'RB'],\n",
       " ['Working', 'VBG'],\n",
       " ['Woman', 'NNP'],\n",
       " ['10', 'CD'],\n",
       " ['years', 'NNS'],\n",
       " ['ago', 'IN'],\n",
       " ['.', '.'],\n",
       " [\"''\", \"''\"],\n",
       " ['', ''],\n",
       " ['Mr.', 'NNP'],\n",
       " ['Lang', 'NNP'],\n",
       " ['took', 'VBD'],\n",
       " ['on', 'RB'],\n",
       " ['Ms.', 'NNP'],\n",
       " ['and', 'CC'],\n",
       " ['Sassy', 'dummy'],\n",
       " ['with', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['acquisition', 'NN'],\n",
       " ['of', 'IN'],\n",
       " ['Matilda', 'dummy'],\n",
       " ['Publications', 'NNPS'],\n",
       " ['Inc.', 'NNP'],\n",
       " ['by', 'IN'],\n",
       " ['his', 'PRP$'],\n",
       " ['newly', 'RB'],\n",
       " ['formed', 'VBN'],\n",
       " ...]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_output = gd_output[1:]\n",
    "# gd_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gd_output = gd_output[1:]\n",
    "\n",
    "with open('greedy.out', 'w') as op:\n",
    "    # # # # # # # \n",
    "    index = 1\n",
    "    for idx, word in enumerate(gd_output):\n",
    "        if word[0] == \"\":\n",
    "            index = 1\n",
    "            op.write(\"\\n\")\n",
    "        else:\n",
    "            op.write(f'{index}\\t{word[0]}\\t{word[1]}')\n",
    "            op.write(\"\\n\")\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gd_output_df = pd.DataFrame(list(gd_output.items()), columns=['Words', 'POS Tags'])\n",
    "# gd_output_df = gd_output_df.drop(0)\n",
    "# # gd_output_df\n",
    "\n",
    "# save_greedy_as = \"greedy.out\"\n",
    "# gd_output_df.to_csv(save_greedy_as, header=None, sep='\\t')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Viterbi Decoding with HMM\n",
    "\n",
    "1. [ ] Implement the viterbi decoding algorithm\n",
    "2. [ ] Evaluate it on the development data\n",
    "3. [ ] Predict the POS Tags of the sentences in the test data\n",
    "4. [ ] Output the predictions in a file named `viterbi.out`, in the same format of training data\n",
    "5. [ ] Question\n",
    "    1. [x] What is the accuracy on the dev data? 22.41% which is not great. Need more training data to improve accuracy. Also need to learn how to write correct and efficient code."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"Base case\n",
    "pi[1, all_pos_tags_in_dev_file] = t(all_pos_tags_in_dev_file | s) * e(words | all_pos_tags_in_dev_file)\n",
    "\n",
    "pi[1, all_pos_tags_in_dev_file] = t(t_find_pos_tag | t_given_pos_tag) * e(e_word | e_given_pos_tag)\n",
    "\n",
    "pi[1, dummy] = t(_ | dummy) * e(The | dummy) = s1\n",
    "pi[1, DT] = t(_ | DT) * e(The | DT) = s2\n",
    "pi[1, NNP] = t(_| NNP) * e(The | NNP) = s3\n",
    "\n",
    "...reset score to 0 bc we update POS Tag, dummy -> s1\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decoding(dev_df: pd.DataFrame, t_probs: dict, e_probs: dict, N_pos_tags: np.array):\n",
    "    \"\"\"Implement greedy decoding on the development file (words only) using the transition probability and emission probability. \n",
    "    \n",
    "    \n",
    "    ??? Furthermore, don't use POS Tag of development file, thus only use POS Tag from training data.\n",
    "\n",
    "    Parameters\n",
    "    ----------        \n",
    "    dev_df: `pd.DataFrame`\n",
    "        Dev file\n",
    "\n",
    "    t_probs: `py dict`\n",
    "        Tranision probabilities for POS Tag given previous POS Tag\n",
    "\n",
    "    e_probs: `py dict`\n",
    "        Emission probabilities for Word given POS Tags\n",
    "\n",
    "    N_pos_tags: `np.array`\n",
    "        All POS Tags found in the training file\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Base Cases:\n",
    "    ----------\n",
    "    v_pi: `py dictionary`\n",
    "        Dictionary to store the length of each sentence (thus will have to reset to 0 every new sentence) and \n",
    "        all possible POS Tags (which will remain the same for each sentence). \n",
    "        \n",
    "    v_pi[len_of_each_sentence, all_possible_tags] = t(all_pos_tags_in_dev_file | s) * e(words | all_pos_tags_in_dev_file)\n",
    "    \n",
    "    v_pi[0, all_pos_tags_in_dev_file] = t(t_find_pos_tag | t_given_pos_tag) * e(e_word | e_given_pos_tag)\n",
    "    \n",
    "    v_pi[0, dummy] = t(dummy | dummy) * e(The | dummy) = s1\n",
    "    v_pi[0, DT] = t(DT | _) * e(The | DT) = s2\n",
    "    v_pi[0, NNP] = t(NNP | _) * e(The | NNP) = s3\n",
    "\n",
    "    v_pi_key: `py tuple`\n",
    "        Tuple to store all possible combinations of keys for v_pi. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    previous_pos_tag = \"dummy\"\n",
    "    v_pi = {}\n",
    "    all_words_with_pos_tag = {}\n",
    "\n",
    "    # start_algo_idx = 0\n",
    "\n",
    "    \"\"\"Base cases\n",
    "    [ ] See all (words tags) to be 0\n",
    "    [ ] Fix on the first word\n",
    "    [ ] Consider using a 2D list of [word x tag]\n",
    "    \"\"\"\n",
    "    for index, row in tqdm(dev_df.iterrows(), total=dev_df.shape[0]):\n",
    "        # print(\"index\", index, \"with word\", row['Word'], \"and POS tag from dev is '\", row['POS Tag'], \"'2nd index\", row[\"Index\"])\n",
    "        if row[\"Index\"] == \"dummy\":\n",
    "            break\n",
    "        \n",
    "        current_pos_tag = row['POS Tag']\n",
    "        v_pi_key = (0, current_pos_tag)\n",
    "\n",
    "        \n",
    "        \"\"\"Transition\"\"\"\n",
    "        t_find_pos_tag = current_pos_tag\n",
    "        t_given_pos_tag = previous_pos_tag\n",
    "        # print(f\"--- t({t_find_pos_tag} | {t_given_pos_tag})\")\n",
    "\n",
    "        \"\"\"Emission\"\"\"\n",
    "        e_word = row['Word']\n",
    "        e_given_pos_tag = current_pos_tag\n",
    "        # print(f\"--- e({e_given_pos_tag} | {e_word})\") # order this way to match e_probs dictionary\n",
    "\n",
    "        \"\"\"Transition * Emission\"\"\"\n",
    "        t_key = (t_find_pos_tag, t_given_pos_tag)\n",
    "        e_key = (e_given_pos_tag, e_word)\n",
    "        # print(t_key in t_probs, e_key in e_probs)\n",
    "        \n",
    "        # IF-ELSE bc not all pairs will be found. If pair is found, use score, otherwise (pair isn't found) set score to 0.0.\n",
    "        if t_key in t_probs and e_key in e_probs:\n",
    "            if v_pi_key not in v_pi:\n",
    "                t = t_probs[t_key]\n",
    "                e = e_probs[e_key]\n",
    "                score = t * e\n",
    "                # print(f\"--- FOUND 1x: pi{v_pi_key} = t({t_find_pos_tag} | {t_given_pos_tag}) * e({e_word} | {e_given_pos_tag}) = {score}\")\n",
    "                v_pi[v_pi_key] = score\n",
    "            else: # if key in v_pi dict found again, add scores to strenghten key\n",
    "                # print(f\"--- FOUND AGAIN: pi{v_pi_key} = t({t_find_pos_tag} | {t_given_pos_tag}) * e({e_word} | {e_given_pos_tag}) = {score}\")\n",
    "                v_pi[v_pi_key] += score\n",
    "            \n",
    "        else:\n",
    "            t = 0.001\n",
    "            e = 0.001\n",
    "            score = t * e\n",
    "            # print(f\"--- NOT FOUND: pi{v_pi_key} = t({t_find_pos_tag} | {t_given_pos_tag}) * e({e_word} | {e_given_pos_tag}) = {score}\")\n",
    "            v_pi[v_pi_key] = score\n",
    "                        \n",
    "        # print()\n",
    "        all_words_with_pos_tag[row['Word']] = current_pos_tag\n",
    "    # print(\"Fillings of 0th index for v_pi --- \", v_pi)\n",
    "\n",
    "    # Fill v_pi with all possible options\n",
    "    for i, row in dev_df.iterrows():\n",
    "        for pos_tag in N_pos_tags:\n",
    "            v_pi_key = (i, pos_tag)\n",
    "            v_pi[v_pi_key] = 0.0\n",
    "        \n",
    "    # print(\"Fillings of remaining pairs for v_pi --- \", v_pi)\n",
    "\n",
    "               \n",
    "    \"\"\"Algo\"\"\"\n",
    "    s_1 = np.argmax(np.array(list(v_pi.values())))\n",
    "    updated_s1 = np.array(list(v_pi.keys()))[s_1]\n",
    "    max_previous_pos_tag = updated_s1[1]\n",
    "\n",
    "    # print()\n",
    "    # print(f\"Base case: {v_pi}\")\n",
    "    # print(f\"s_1 (from the base case) index is {s_1} and the key at this index is {max_previous_pos_tag}\")\n",
    "    # print()\n",
    "\n",
    "    track_pi_idx = 1 # TODO: Figure out when to reset. I think go until end of/ len of previous sentence\n",
    "    \n",
    "    for index, row in tqdm(dev_df.iterrows(), total=dev_df.shape[0]):\n",
    "        print(\"index\", index, \"with word\", row['Word'], \"and POS tag from dev\", row['POS Tag'], row[\"Index\"])\n",
    "        \n",
    "        idx_j = track_pi_idx - 1\n",
    "        # print(\"j - 1 = \", idx_j)\n",
    "        previous_v_pi_key = (idx_j, max_previous_pos_tag)\n",
    "        \n",
    "        if index > 38:\n",
    "            if row[\"Index\"] == \"dummy\":\n",
    "                break\n",
    "            # pass\n",
    "            if row[\"Index\"] >= 1.0:\n",
    "                current_pos_tag = row['POS Tag']\n",
    "                # v_pi_key = (1, current_pos_tag)\n",
    "                \n",
    "                v_pi_key = (track_pi_idx, current_pos_tag)\n",
    "                \n",
    "                # print(f\"Key of v_i is {v_pi_key} bc we  j is {track_pi_idx} and j - 1 is {idx_j}\")\n",
    "                \n",
    "\n",
    "                \"\"\"DP Algo\n",
    "                pi[number_of_words_in_sentence * all_possible_tags] = t(all_pos_tags_in_dev_file | s) * e(words | all_pos_tags_in_dev_file)\n",
    "                \n",
    "                pi[1, all_pos_tags_in_dev_file] = t(t_find_pos_tag | t_given_pos_tag) * e(e_word | e_given_pos_tag)\n",
    "\n",
    "                pi[1, DT] = max(pi[1, s1] * t(DT | s1) * e(The | DT) = s1\n",
    "                (1, 'DT') = max(pi[1, s1] * t(DT | CD) * e(The | DT) \n",
    "\n",
    "                \n",
    "                pi[2, NNP] = max(pi[2, s2] * t(NNP | s2) * e(Arizona | NNP) = s2\n",
    "                pi[3, NNP] = max(pi[3, s3] * t(NNP | s3) * e(Corps... | NNP) = s3\n",
    "                pi[4, NNP] = max(pi[4, s4] * t(NNP | s4) * e(Commission | NNP) = s4\n",
    "            \n",
    "                \"\"\"\n",
    "        \n",
    "                \"\"\"Transition\"\"\"\n",
    "                t_find_pos_tag = current_pos_tag\n",
    "                t_given_pos_tag = max_previous_pos_tag\n",
    "                # print(f\"--- t({t_find_pos_tag} | {t_given_pos_tag})\")\n",
    "        \n",
    "                \"\"\"Emission\"\"\"\n",
    "                e_word = row['Word']\n",
    "                e_given_pos_tag = current_pos_tag\n",
    "                # print(f\"--- e({e_given_pos_tag} | {e_word})\") # order this way to match e_probs dictionary\n",
    "        \n",
    "                \"\"\"Transition * Emission\"\"\"\n",
    "                t_key = (t_find_pos_tag, t_given_pos_tag)\n",
    "                e_key = (e_given_pos_tag, e_word)\n",
    "                # print(t_key in t_probs, e_key in e_probs)\n",
    "                # print(\"---previous_v_pi_key:\", previous_v_pi_key, \"with max_previous_pos_tag as\", max_previous_pos_tag)\n",
    "                \n",
    "                # IF-ELSE bc not all pairs will be found. If pair is found, use score, otherwise (pair isn't found) set score to 0.0.\n",
    "                if t_key in t_probs and e_key in e_probs and previous_v_pi_key in v_pi:\n",
    "                    t = t_probs[t_key]\n",
    "                    e = e_probs[e_key]\n",
    "                    score = v_pi[previous_v_pi_key] * t * e\n",
    "                    # v_pi[v_pi_key] = v_pi[previous_v_pi_key] * t * e\n",
    "                    print(f\"--- pi{previous_v_pi_key} is FOUND in v_pi. Now, let's update v_pi at pi{v_pi_key}\")\n",
    "                    print(f\"--- pi{v_pi_key} = pi({previous_v_pi_key}) * t({t_find_pos_tag} | {t_given_pos_tag}) * e({e_word} | {e_given_pos_tag}) = {score}\")\n",
    "                    # previous_v_pi_key = v_pi_key\n",
    "             \n",
    "                else:\n",
    "                    t = 0.001\n",
    "                    e = 0.001\n",
    "                    # v_pi[v_pi_key] = 0.0\n",
    "                    score = v_pi[previous_v_pi_key] * t * e\n",
    "                    print(f\"--- pi{previous_v_pi_key} is NOT FOUND in v_pi. Now, let's update v_pi at pi{v_pi_key}\")\n",
    "                    print(f\"--- pi{v_pi_key} = pi({previous_v_pi_key}) * t({t_find_pos_tag} | {t_given_pos_tag}) * e({e_word} | {e_given_pos_tag}) = {score}\")\n",
    "                    # previous_v_pi_key = v_pi_key\n",
    "                                \n",
    "                track_pi_idx += 1\n",
    "                v_pi[previous_v_pi_key] = score\n",
    "                \n",
    "                \n",
    "        \n",
    "                # print(\"UPDATE\")\n",
    "                s_i = np.argmax(np.array(list(v_pi.values())))\n",
    "                updated_s_i = np.array(list(v_pi.keys()))[s_i]\n",
    "                # print(f\"max value is at index: {s_i} in {v_pi} has key of {updated_s_i}\")\n",
    "                max_previous_pos_tag = updated_s_i[1]\n",
    "                all_words_with_pos_tag[row['Word']] = current_pos_tag\n",
    "                print(\"max_previous_pos_tag is\", max_previous_pos_tag)\n",
    "                print()\n",
    "            # print(f\"is {previous_v_pi_key} in v_pi\")\n",
    "    \n",
    "    # print(v_pi)\n",
    "\n",
    "    return all_words_with_pos_tag\n",
    "    # return v_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated_dev_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                     | 38/50 [00:00<00:00, 13040.71it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 1686.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0 with word   and POS tag from dev dummy 0.0\n",
      "index 1 with word The and POS tag from dev DT 1.0\n",
      "index 2 with word Arizona and POS tag from dev NNP 2.0\n",
      "index 3 with word Corporations and POS tag from dev NNP 3.0\n",
      "index 4 with word Commission and POS tag from dev NNP 4.0\n",
      "index 5 with word authorized and POS tag from dev VBD 5.0\n",
      "index 6 with word an and POS tag from dev DT 6.0\n",
      "index 7 with word 11.5 and POS tag from dev CD 7.0\n",
      "index 8 with word % and POS tag from dev NN 8.0\n",
      "index 9 with word rate and POS tag from dev NN 9.0\n",
      "index 10 with word increase and POS tag from dev NN 10.0\n",
      "index 11 with word at and POS tag from dev IN 11.0\n",
      "index 12 with word Tucson and POS tag from dev NNP 12.0\n",
      "index 13 with word Electric and POS tag from dev NNP 13.0\n",
      "index 14 with word Power and POS tag from dev NNP 14.0\n",
      "index 15 with word Co. and POS tag from dev NNP 15.0\n",
      "index 16 with word , and POS tag from dev , 16.0\n",
      "index 17 with word substantially and POS tag from dev RB 17.0\n",
      "index 18 with word lower and POS tag from dev JJR 18.0\n",
      "index 19 with word than and POS tag from dev IN 19.0\n",
      "index 20 with word recommended and POS tag from dev VBN 20.0\n",
      "index 21 with word last and POS tag from dev JJ 21.0\n",
      "index 22 with word month and POS tag from dev NN 22.0\n",
      "index 23 with word by and POS tag from dev IN 23.0\n",
      "index 24 with word a and POS tag from dev DT 24.0\n",
      "index 25 with word commission and POS tag from dev NN 25.0\n",
      "index 26 with word hearing and POS tag from dev NN 26.0\n",
      "index 27 with word officer and POS tag from dev NN 27.0\n",
      "index 28 with word and and POS tag from dev CC 28.0\n",
      "index 29 with word barely and POS tag from dev RB 29.0\n",
      "index 30 with word half and POS tag from dev PDT 30.0\n",
      "index 31 with word the and POS tag from dev DT 31.0\n",
      "index 32 with word rise and POS tag from dev NN 32.0\n",
      "index 33 with word sought and POS tag from dev VBN 33.0\n",
      "index 34 with word by and POS tag from dev IN 34.0\n",
      "index 35 with word the and POS tag from dev DT 35.0\n",
      "index 36 with word utility and POS tag from dev NN 36.0\n",
      "index 37 with word . and POS tag from dev . 37.0\n",
      "index 38 with word dummy and POS tag from dev dummy dummy\n",
      "index 39 with word The and POS tag from dev DT 1.0\n",
      "--- pi(0, 'dummy') is FOUND in v_pi. Now, let's update v_pi at pi(1, 'DT')\n",
      "--- pi(1, 'DT') = pi((0, 'dummy')) * t(DT | dummy) * e(The | DT) = 0.0\n",
      "max_previous_pos_tag is dummy\n",
      "\n",
      "index 40 with word ruling and POS tag from dev NN 2.0\n",
      "--- pi(1, 'dummy') is FOUND in v_pi. Now, let's update v_pi at pi(2, 'NN')\n",
      "--- pi(2, 'NN') = pi((1, 'dummy')) * t(NN | dummy) * e(ruling | NN) = 0.0\n",
      "max_previous_pos_tag is dummy\n",
      "\n",
      "index 41 with word follows and POS tag from dev VBZ 3.0\n",
      "--- pi(2, 'dummy') is FOUND in v_pi. Now, let's update v_pi at pi(3, 'VBZ')\n",
      "--- pi(3, 'VBZ') = pi((2, 'dummy')) * t(VBZ | dummy) * e(follows | VBZ) = 0.0\n",
      "max_previous_pos_tag is dummy\n",
      "\n",
      "index 42 with word a and POS tag from dev DT 4.0\n",
      "--- pi(3, 'dummy') is FOUND in v_pi. Now, let's update v_pi at pi(4, 'DT')\n",
      "--- pi(4, 'DT') = pi((3, 'dummy')) * t(DT | dummy) * e(a | DT) = 0.0\n",
      "max_previous_pos_tag is dummy\n",
      "\n",
      "index 43 with word host and POS tag from dev NN 5.0\n",
      "--- pi(4, 'dummy') is FOUND in v_pi. Now, let's update v_pi at pi(5, 'NN')\n",
      "--- pi(5, 'NN') = pi((4, 'dummy')) * t(NN | dummy) * e(host | NN) = 0.0\n",
      "max_previous_pos_tag is dummy\n",
      "\n",
      "index 44 with word of and POS tag from dev IN 6.0\n",
      "--- pi(5, 'dummy') is FOUND in v_pi. Now, let's update v_pi at pi(6, 'IN')\n",
      "--- pi(6, 'IN') = pi((5, 'dummy')) * t(IN | dummy) * e(of | IN) = 0.0\n",
      "max_previous_pos_tag is dummy\n",
      "\n",
      "index 45 with word problems and POS tag from dev NNS 7.0\n",
      "--- pi(6, 'dummy') is FOUND in v_pi. Now, let's update v_pi at pi(7, 'NNS')\n",
      "--- pi(7, 'NNS') = pi((6, 'dummy')) * t(NNS | dummy) * e(problems | NNS) = 0.0\n",
      "max_previous_pos_tag is dummy\n",
      "\n",
      "index 46 with word at and POS tag from dev IN 8.0\n",
      "--- pi(7, 'dummy') is FOUND in v_pi. Now, let's update v_pi at pi(8, 'IN')\n",
      "--- pi(8, 'IN') = pi((7, 'dummy')) * t(IN | dummy) * e(at | IN) = 0.0\n",
      "max_previous_pos_tag is dummy\n",
      "\n",
      "index 47 with word Tucson and POS tag from dev NNP 9.0\n",
      "--- pi(8, 'dummy') is FOUND in v_pi. Now, let's update v_pi at pi(9, 'NNP')\n",
      "--- pi(9, 'NNP') = pi((8, 'dummy')) * t(NNP | dummy) * e(Tucson | NNP) = 0.0\n",
      "max_previous_pos_tag is dummy\n",
      "\n",
      "index 48 with word Electric and POS tag from dev NNP 10.0\n",
      "--- pi(9, 'dummy') is FOUND in v_pi. Now, let's update v_pi at pi(10, 'NNP')\n",
      "--- pi(10, 'NNP') = pi((9, 'dummy')) * t(NNP | dummy) * e(Electric | NNP) = 0.0\n",
      "max_previous_pos_tag is dummy\n",
      "\n",
      "index 49 with word , and POS tag from dev , 11.0\n",
      "--- pi(10, 'dummy') is FOUND in v_pi. Now, let's update v_pi at pi(11, ',')\n",
      "--- pi(11, ',') = pi((10, 'dummy')) * t(, | dummy) * e(, | ,) = 0.0\n",
      "max_previous_pos_tag is dummy\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vd_output = viterbi_decoding(updated_dev_df[:50], t_probs, e_probs, all_pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vd_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vd_output_df = pd.DataFrame(list(vd_output.items()), columns=['Words', 'POS Tags'])\n",
    "# vd_output_df = vd_output_df.drop(0)\n",
    "# # vd_output_df\n",
    "\n",
    "# save_as = \"viterbi.out\"\n",
    "# vd_output_df.to_csv(save_as, header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
