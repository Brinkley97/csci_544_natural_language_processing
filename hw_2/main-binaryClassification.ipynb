{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2- Binary Classification \n",
    "- Detravious Jamari Brinkley\n",
    "- CSCI-544: Applied Natural Language Processing\n",
    "- python version: 3.11.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/brinkley97/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/brinkley97/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "1. Read data\n",
    "2. Keep reviews and ratings\n",
    "3. Create binary and ternary classes\n",
    "4. Clean data steps\n",
    "4. Clean data function\n",
    "5. Split data\n",
    "6. Load pretrained model and train my model\n",
    "    - Get similarity for the pretrained model\n",
    "    - Get similarity for my trained model\n",
    "9. Extract word embeddings\n",
    "    - Get word embeddings for pretrained model\n",
    "    - Get word embeddings for my model\n",
    "10. Simple models\n",
    "    - Get accuracy for perceptron on pretrained model\n",
    "    - Get accuracy for svm on pretrained model\n",
    "    - Get accuracy for perceptron on my model\n",
    "    - Get accuracy for svm on my model\n",
    "11. What do I conclude from comparing performances\n",
    "12. Feedforward Neural Network\n",
    "13. CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"../datasets/amazon_reviews_us_Office_Products_v1_00.tsv\"\n",
    "amazon_reviews_copy_df = pd.read_csv(dataset, sep='\\t', on_bad_lines='skip', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>4</td>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>4</td>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>4</td>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>5</td>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>5</td>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640254 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating                                        review_body\n",
       "0                 5                                     Great product.\n",
       "1                 5  What's to say about this commodity item except...\n",
       "2                 5    Haven't used yet, but I am sure I will like it.\n",
       "3                 1  Although this was labeled as &#34;new&#34; the...\n",
       "4                 4                    Gorgeous colors and easy to use\n",
       "...             ...                                                ...\n",
       "2640249           4  I can't live anymore whithout my Palm III. But...\n",
       "2640250           4  Although the Palm Pilot is thin and compact it...\n",
       "2640251           4  This book had a lot of great content without b...\n",
       "2640252           5  I am teaching a course in Excel and am using t...\n",
       "2640253           5  A very comprehensive layout of exactly how Vis...\n",
       "\n",
       "[2640254 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_ratings_df = amazon_reviews_copy_df.loc[0:, ['star_rating', 'review_body']]\n",
    "reviews_ratings_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_reviews(df: pd.DataFrame, review_col_name: str, number_of_reviews: int = 3):\n",
    "    \"\"\"Include reviews and ratings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    review_col_name: `str`\n",
    "        The specific_column to get the reviews and ratings of\n",
    "    \n",
    "    number_of_reviews: `int`\n",
    "        Number of samples to include\n",
    "\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    Nothing; instead, print the reviews with ratings\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    columns_to_include = [review_col_name, 'star_rating']\n",
    "\n",
    "    # Initialize an empty list to store dictionaries\n",
    "    list_of_dicts = []\n",
    "\n",
    "    # Iterate over the specified columns and retrieve the first three rows\n",
    "    for row in df[columns_to_include].head(3).to_dict(orient='records'):\n",
    "        list_of_dicts.append({'star_rating': row['star_rating'], review_col_name: row[review_col_name]})\n",
    "\n",
    "    for dictionary in list_of_dicts:\n",
    "        print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    " ## Create binary and ternary classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data_type(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Update the data type of the star ratings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with rating values\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the new sentiment appened\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    valid_ratings = ['1','2','3','4','5']\n",
    "    star_rating_series = df[col_name].copy()\n",
    "\n",
    "    # Convert type to strings\n",
    "    star_rating_series.astype('str')\n",
    "\n",
    "    # Check valid list and see which of our stars match\n",
    "    rows = star_rating_series.index\n",
    "    is_rating_in_valid_ratings = rows[star_rating_series.isin(valid_ratings)]\n",
    "\n",
    "    # Convert to list\n",
    "    is_rating_in_valid_ratings = is_rating_in_valid_ratings.to_list()\n",
    "\n",
    "    updated_df = df.iloc[is_rating_in_valid_ratings]\n",
    "    updated_df[col_name] = updated_df[col_name].astype(int)\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_34235/2907883124.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  updated_df[col_name] = updated_df[col_name].astype(int)\n"
     ]
    }
   ],
   "source": [
    "updated_reviews_ratings_df = update_data_type(reviews_ratings_df, 'star_rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>4</td>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>4</td>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>4</td>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>5</td>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>5</td>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640237 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "0                  5                                     Great product.\n",
       "1                  5  What's to say about this commodity item except...\n",
       "2                  5    Haven't used yet, but I am sure I will like it.\n",
       "3                  1  Although this was labeled as &#34;new&#34; the...\n",
       "4                  4                    Gorgeous colors and easy to use\n",
       "...              ...                                                ...\n",
       "2640249            4  I can't live anymore whithout my Palm III. But...\n",
       "2640250            4  Although the Palm Pilot is thin and compact it...\n",
       "2640251            4  This book had a lot of great content without b...\n",
       "2640252            5  I am teaching a course in Excel and am using t...\n",
       "2640253            5  A very comprehensive layout of exactly how Vis...\n",
       "\n",
       "[2640237 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_reviews_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>4</td>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>4</td>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>4</td>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>5</td>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>5</td>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640080 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "0                  5                                     Great product.\n",
       "1                  5  What's to say about this commodity item except...\n",
       "2                  5    Haven't used yet, but I am sure I will like it.\n",
       "3                  1  Although this was labeled as &#34;new&#34; the...\n",
       "4                  4                    Gorgeous colors and easy to use\n",
       "...              ...                                                ...\n",
       "2640249            4  I can't live anymore whithout my Palm III. But...\n",
       "2640250            4  Although the Palm Pilot is thin and compact it...\n",
       "2640251            4  This book had a lot of great content without b...\n",
       "2640252            5  I am teaching a course in Excel and am using t...\n",
       "2640253            5  A very comprehensive layout of exactly how Vis...\n",
       "\n",
       "[2640080 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_reviews_ratings_df = updated_reviews_ratings_df.dropna()\n",
    "updated_reviews_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         star_rating  review_body\n",
      "0              False        False\n",
      "1              False        False\n",
      "2              False        False\n",
      "3              False        False\n",
      "4              False        False\n",
      "...              ...          ...\n",
      "2640249        False        False\n",
      "2640250        False        False\n",
      "2640251        False        False\n",
      "2640252        False        False\n",
      "2640253        False        False\n",
      "\n",
      "[2640080 rows x 2 columns]\n",
      "There are no NaN values in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values\n",
    "nan_check = updated_reviews_ratings_df.isna()\n",
    "\n",
    "# Display the DataFrame with True where NaN values exist\n",
    "print(nan_check)\n",
    "\n",
    "# Check if any NaN value exists in the DataFrame\n",
    "if nan_check.any().any():\n",
    "    print(\"There are NaN values in the DataFrame.\")\n",
    "else:\n",
    "    print(\"There are no NaN values in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# reviews per rating star_rating\n",
      "5    1582704\n",
      "4     418348\n",
      "1     306967\n",
      "3     193680\n",
      "2     138381\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"# reviews per rating\", updated_reviews_ratings_df['star_rating'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_star_ratings(df: pd.DataFrame, col_name: str, star_value: int, number_of_reviews: int):\n",
    "    \"\"\"Build a subset balanced dataset with reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The dataframe to use\n",
    "    col_name: `str`\n",
    "        The name of the column to get reviews from\n",
    "    star_value: `int`\n",
    "        The star rating of the review\n",
    "    number_of_reviews: `int`\n",
    "        The number of sub reviews to include in sample\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    rating_df, sampled_rating_df: `tuple`\n",
    "        All reviews with that rating and the subset reviews with that rating\n",
    "    \"\"\"\n",
    "    \n",
    "    rating_df = df[df[col_name] == star_value]\n",
    "    sampled_rating_df = rating_df.sample(n=number_of_reviews)\n",
    "    return rating_df, sampled_rating_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_reviews = 50000\n",
    "# subset_reviews = 300\n",
    "\n",
    "one_star = 1\n",
    "rating_one, rating_one_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', one_star, subset_reviews)\n",
    "two_stars = 2\n",
    "rating_two, rating_two_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', two_stars, subset_reviews)\n",
    "three_stars = 3\n",
    "rating_three, rating_three_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', three_stars, subset_reviews)\n",
    "four_stars = 4\n",
    "rating_four, rating_four_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', four_stars, subset_reviews)\n",
    "five_stars = 5\n",
    "rating_five, rating_five_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', five_stars, subset_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews_df = pd.concat([rating_one_sampled, rating_two_sampled, rating_three_sampled, rating_four_sampled, rating_five_sampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2563244</th>\n",
       "      <td>1</td>\n",
       "      <td>The unit is a base and two handsets. One hands...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611092</th>\n",
       "      <td>1</td>\n",
       "      <td>I received this and did not open them until re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723857</th>\n",
       "      <td>1</td>\n",
       "      <td>I just find that the black ink of this package...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761019</th>\n",
       "      <td>1</td>\n",
       "      <td>rip off ran out in one week !!!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256629</th>\n",
       "      <td>1</td>\n",
       "      <td>Not pleased.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812315</th>\n",
       "      <td>5</td>\n",
       "      <td>Great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202491</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product but it's a shame that its discon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573678</th>\n",
       "      <td>5</td>\n",
       "      <td>This box should last a long time and costs no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466004</th>\n",
       "      <td>5</td>\n",
       "      <td>The Schmidt EF9000, is a great alternative to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343931</th>\n",
       "      <td>5</td>\n",
       "      <td>Item delivered on time, was as described...Exc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "2563244            1  The unit is a base and two handsets. One hands...\n",
       "611092             1  I received this and did not open them until re...\n",
       "1723857            1  I just find that the black ink of this package...\n",
       "761019             1               rip off ran out in one week !!!!!!!!\n",
       "256629             1                                       Not pleased.\n",
       "...              ...                                                ...\n",
       "812315             5                                              Great\n",
       "1202491            5  Great product but it's a shame that its discon...\n",
       "1573678            5  This box should last a long time and costs no ...\n",
       "1466004            5  The Schmidt EF9000, is a great alternative to ...\n",
       "343931             5  Item delivered on time, was as described...Exc...\n",
       "\n",
       "[250000 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_reviews_by_rating(df: pd.DataFrame, rating_col: str, threshold: int, sentiment_type: str):\n",
    "    \"\"\"Categorizes reviews by adding a rating\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    rating_col: `str`\n",
    "        Column with rating values\n",
    "    \n",
    "    threshold: `int`\n",
    "        Where to split the ratings such that categories can be formed\n",
    "\n",
    "    sentiment_type: `str`\n",
    "        One of three types of sentiment: positive, negative, or neural\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the new sentiment appened\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if sentiment_type == 'positive_review_class':\n",
    "        positive_review_threshold = df[rating_col].astype('int32') > threshold\n",
    "        df = df[positive_review_threshold]\n",
    "        df[sentiment_type] = 1\n",
    "\n",
    "    elif sentiment_type == 'negative_review_class':\n",
    "        negative_review_threshold = df[rating_col].astype('int32') < threshold\n",
    "        df = df[negative_review_threshold]\n",
    "        df[sentiment_type] = 2\n",
    "\n",
    "    elif sentiment_type == 'neutral_review_class':\n",
    "        neutral_review_threshold = df[rating_col].astype('int32') == threshold\n",
    "        df = df[neutral_review_threshold]\n",
    "        df[sentiment_type] = 3\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_34235/1400069123.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[sentiment_type] = 2\n",
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_34235/1400069123.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[sentiment_type] = 3\n",
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_34235/1400069123.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[sentiment_type] = 1\n"
     ]
    }
   ],
   "source": [
    "negative_review_class_df = separate_reviews_by_rating(sampled_reviews_df, 'star_rating', 3, 'negative_review_class')\n",
    "neutral_review_class_df = separate_reviews_by_rating(sampled_reviews_df, 'star_rating', 3, 'neutral_review_class')\n",
    "positive_review_class_df = separate_reviews_by_rating(sampled_reviews_df, 'star_rating', 3, 'positive_review_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>negative_review_class</th>\n",
       "      <th>neutral_review_class</th>\n",
       "      <th>positive_review_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2563244</th>\n",
       "      <td>1</td>\n",
       "      <td>The unit is a base and two handsets. One hands...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611092</th>\n",
       "      <td>1</td>\n",
       "      <td>I received this and did not open them until re...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723857</th>\n",
       "      <td>1</td>\n",
       "      <td>I just find that the black ink of this package...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761019</th>\n",
       "      <td>1</td>\n",
       "      <td>rip off ran out in one week !!!!!!!!</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256629</th>\n",
       "      <td>1</td>\n",
       "      <td>Not pleased.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812315</th>\n",
       "      <td>5</td>\n",
       "      <td>Great</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202491</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product but it's a shame that its discon...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573678</th>\n",
       "      <td>5</td>\n",
       "      <td>This box should last a long time and costs no ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466004</th>\n",
       "      <td>5</td>\n",
       "      <td>The Schmidt EF9000, is a great alternative to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343931</th>\n",
       "      <td>5</td>\n",
       "      <td>Item delivered on time, was as described...Exc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body  \\\n",
       "2563244            1  The unit is a base and two handsets. One hands...   \n",
       "611092             1  I received this and did not open them until re...   \n",
       "1723857            1  I just find that the black ink of this package...   \n",
       "761019             1               rip off ran out in one week !!!!!!!!   \n",
       "256629             1                                       Not pleased.   \n",
       "...              ...                                                ...   \n",
       "812315             5                                              Great   \n",
       "1202491            5  Great product but it's a shame that its discon...   \n",
       "1573678            5  This box should last a long time and costs no ...   \n",
       "1466004            5  The Schmidt EF9000, is a great alternative to ...   \n",
       "343931             5  Item delivered on time, was as described...Exc...   \n",
       "\n",
       "         negative_review_class  neutral_review_class  positive_review_class  \n",
       "2563244                    2.0                   NaN                    NaN  \n",
       "611092                     2.0                   NaN                    NaN  \n",
       "1723857                    2.0                   NaN                    NaN  \n",
       "761019                     2.0                   NaN                    NaN  \n",
       "256629                     2.0                   NaN                    NaN  \n",
       "...                        ...                   ...                    ...  \n",
       "812315                     NaN                   NaN                    1.0  \n",
       "1202491                    NaN                   NaN                    1.0  \n",
       "1573678                    NaN                   NaN                    1.0  \n",
       "1466004                    NaN                   NaN                    1.0  \n",
       "343931                     NaN                   NaN                    1.0  \n",
       "\n",
       "[250000 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_reviews_ratings_df = pd.concat([negative_review_class_df, neutral_review_class_df, positive_review_class_df])\n",
    "sampled_reviews_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews_df = sampled_reviews_ratings_df['negative_review_class'].dropna()\n",
    "neutral_reviews_df = sampled_reviews_ratings_df['neutral_review_class'].dropna()\n",
    "positive_reviews_df = sampled_reviews_ratings_df['positive_review_class'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews_ratings_df['binary_review_class'] = pd.concat([negative_reviews_df, positive_reviews_df])\n",
    "sampled_reviews_ratings_df['ternary_review_class'] = pd.concat([negative_reviews_df, neutral_reviews_df, positive_reviews_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2., nan,  1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_reviews_ratings_df['binary_review_class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Clean data steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_reviews_to_lower_case(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Convert all reviews to lower case\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the lower cased reviews\n",
    "    \"\"\"\n",
    "    \n",
    "    lower_case_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "    \n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "        # print(text_reviews_idx, type(text_review), text_review)\n",
    "\n",
    "        # NOT all reviews are strings, thus all can't be converted to lower cased\n",
    "        if type(text_review) != str:\n",
    "            print(True, text_review)\n",
    "            converted_str = str(text_review)\n",
    "            lower_case_reviews.append(text_review)\n",
    "         \n",
    "        else:\n",
    "            update_text_review = text_review.lower()\n",
    "            lower_case_reviews.append(update_text_review)\n",
    "\n",
    "    updated_df['lower_cased'] = lower_case_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_lower_cased = convert_reviews_to_lower_case(sampled_reviews_ratings_df, 'review_body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_lower_cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"reviews_lower_cased:\")\n",
    "# generate_sample_reviews(reviews_lower_cased, 'lower_cased', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove HTML and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_and_urls(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Remove HTML and URLs from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the html_and_urls removed\n",
    "    \"\"\"\n",
    "    \n",
    "    # url_pattern = re.compile(r'https?://\\S+|www\\. \\S+')\n",
    "\n",
    "    cleaned_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "\n",
    "        if isinstance(text_review, str):\n",
    "            # Check and remove HTML tags\n",
    "            has_html = bool(re.search('<.*?>', text_review))\n",
    "            if has_html == True:\n",
    "                # print(\"Review\", text_reviews_idx, \"has HTML -- \", text_review)\n",
    "                pass\n",
    "\n",
    "            no_html_review = re.sub('<.*?>', ' ', text_review)\n",
    "            # print(\"Review\", text_reviews_idx, \"without HTML -- \", no_html_review)\n",
    "        \n",
    "            # Check and remove URLs\n",
    "            has_url = bool(re.search(r'http\\S+', no_html_review))\n",
    "            if has_url == True:\n",
    "                # print(\"Review\", text_reviews_idx, \"has URL --\", no_html_review)\n",
    "                pass\n",
    "\n",
    "            no_html_url_review = re.sub(r'http\\S+', '', no_html_review)\n",
    "            # print(\"Review\", text_reviews_idx, \"without HTML, URL -- \", no_html_url_review)\n",
    "            # print()\n",
    "            cleaned_reviews.append(no_html_url_review)\n",
    "        else:\n",
    "            # print(text_reviews_idx, text_review)\n",
    "            cleaned_reviews.append(text_review)\n",
    "            \n",
    "\n",
    "    updated_df['without_html_urls'] = cleaned_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_html_urls_df = remove_html_and_urls(reviews_lower_cased, 'lower_cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_html_urls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_html_urls:\")\n",
    "# generate_sample_reviews(no_html_urls_df, 'without_html_urls', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_contractions = {\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"i'm\": \"I am\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"that'll\": \"that will\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'd\": \"what would\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when'll\": \"when will\",\n",
    "    \"when'd\": \"when would\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where'll\": \"where will\",\n",
    "    \"where'd\": \"where would\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why'll\": \"why will\",\n",
    "    \"why'd\": \"why would\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how'd\": \"how would\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_and_replace_contractions(review):\n",
    "    \"\"\"Find the contractions to replace from a specific review\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    review: `str`\n",
    "        A specific review\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    non_contraction_review: `str`\n",
    "        The updated specific review with contractions expanded\n",
    "    \n",
    "    \"\"\"\n",
    "    if isinstance(review, str):\n",
    "        get_words = review.split()\n",
    "\n",
    "        store_non_contraction_words = []\n",
    "\n",
    "        for word in get_words:\n",
    "            if word in store_contractions:\n",
    "                non_contraction_form = store_contractions[word]\n",
    "                # print(word, \"-->\", non_contraction_form)\n",
    "\n",
    "                store_non_contraction_words.append(non_contraction_form)\n",
    "\n",
    "            else:\n",
    "                # print(word)\n",
    "                store_non_contraction_words.append(word)\n",
    "\n",
    "        non_contraction_review = ' '.join(store_non_contraction_words)\n",
    "        return non_contraction_review\n",
    "    else:\n",
    "        return review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contractions(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Remove contractions from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the extra spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    without_contractions_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "\n",
    "        # print(\"Review\", text_reviews_idx, \"with possible contraction(s) -- \", text_review)\n",
    "\n",
    "        without_contraction = locate_and_replace_contractions(text_review)\n",
    "\n",
    "        # print(\"Review\", text_reviews_idx, \"without contraction -- \", without_contraction)\n",
    "        # print()\n",
    "\n",
    "        without_contractions_reviews.append(without_contraction)\n",
    "\n",
    "    updated_df['without_contractions'] = without_contractions_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_contractions_df = remove_contractions(no_html_urls_df, 'without_html_urls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_contractions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_contractions:\")\n",
    "# generate_sample_reviews(no_contractions_df, 'without_contractions', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove Non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphabetical_characters(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Remove Non-alphabetical characters from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the non-alphabetical characters removed\n",
    "    \"\"\"\n",
    "\n",
    "    alphabetical_char_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "    # print(text_reviews)\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "        \n",
    "        if isinstance(text_review, str):\n",
    "\n",
    "            # Check for non-alphabetical characters\n",
    "            has_non_alphabetical_char = bool(re.search(r'[^a-zA-Z]', text_review))\n",
    "            if has_non_alphabetical_char == True:\n",
    "                # print(\"Review\", text_reviews_idx, \"has HTML -- \", text_review)\n",
    "                pass\n",
    "            \n",
    "            # Remove non-alphabetical characters\n",
    "            with_alphabetical_char = re.sub(r'[^a-zA-Z\\s]', ' ', text_review)\n",
    "            # print(\"Review\", text_reviews_idx, \"has HTML -- \", with_alphabetical_char)\n",
    "            alphabetical_char_reviews.append(with_alphabetical_char)\n",
    "        else:\n",
    "            alphabetical_char_reviews.append(text_review)\n",
    "\n",
    "    updated_df['with_alpha_chars_only'] = alphabetical_char_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only_alpha_chars_df = remove_non_alphabetical_characters(no_contractions_df, 'without_contractions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only_alpha_chars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"with_alpha_chars_only:\")\n",
    "# generate_sample_reviews(only_alpha_chars_df, 'with_alpha_chars_only', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Remove extra spaces from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the extra spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    single_spaced_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "    # print(text_reviews)\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "\n",
    "        if isinstance(text_review, str):\n",
    "        # Check if there are any extra spaces\n",
    "            has_extra_space = bool(re.search(r' +', text_review))\n",
    "            if has_extra_space == True:\n",
    "                # print(\"Review\", text_reviews_idx, \"has extra space -- \", text_review)\n",
    "                pass\n",
    "            \n",
    "            # Remove extra spaces\n",
    "            single_spaced_review = re.sub(r' +', ' ', text_review)\n",
    "            # print(\"Review\", text_reviews_idx, \"without extra space -- \", single_spaced_review)\n",
    "            # print()\n",
    "            \n",
    "            single_spaced_reviews.append(single_spaced_review)\n",
    "        else:\n",
    "            single_spaced_reviews.append(text_review)\n",
    "\n",
    "    updated_df['without_extra_space'] = single_spaced_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_extra_space_df = remove_extra_spaces(only_alpha_chars_df, 'with_alpha_chars_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_extra_space_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_extra_space:\")\n",
    "# generate_sample_reviews(no_extra_space_df, 'without_extra_space', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stop_words(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Filter stop words out from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the extra spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    without_stop_words_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "\n",
    "        if isinstance(text_review, str):\n",
    "            text_review_words = word_tokenize(text_review) \n",
    "\n",
    "        \n",
    "\n",
    "            # print(\"Before stop word removal\", text_reviews_idx, \" -- \", text_review)\n",
    "\n",
    "            filtered_review = []\n",
    "\n",
    "            for text_review_words_idx in range(len(text_review_words)):\n",
    "                text_review_word = text_review_words[text_review_words_idx]\n",
    "                \n",
    "                # Check if review word is a stop word\n",
    "                if text_review_word in stop_words:\n",
    "                    # print(\"  Stop word -- \", text_review_word)\n",
    "                    pass\n",
    "                else:\n",
    "                    # print(text_review_word, \" -- is NOT a stop word in review\")\n",
    "                    filtered_review.append(text_review_word)\n",
    "\n",
    "            \n",
    "            filtered_review = \" \".join(filtered_review)\n",
    "            # print(\"After stop word removal\", text_reviews_idx, \" -- \", filtered_review)\n",
    "            # print()\n",
    "            \n",
    "            without_stop_words_reviews.append(filtered_review)\n",
    "        else:\n",
    "            without_stop_words_reviews.append(text_review)\n",
    "        \n",
    "\n",
    "    updated_df['without_stop_words'] = without_stop_words_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_stop_words_df = filter_stop_words(no_extra_space_df, 'without_extra_space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_stop_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_stop_words:\")\n",
    "# generate_sample_reviews(no_stop_words_df, 'without_stop_words', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Perform lemmatization  \n",
    "\n",
    "- \"A sentence with many words\"\n",
    "    - \"words\" -> word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmentize_review(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Lemmentize all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the extra spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    lemmed_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "\n",
    "    lem = WordNetLemmatizer()\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]   \n",
    "        if isinstance(text_review, str):     \n",
    "            words_in_review = word_tokenize(text_review) \n",
    "\n",
    "            # print(\"Before lem update\", text_reviews_idx, \" -- \", text_review)\n",
    "            # print(\"Lemmed words\", words_in_review)\n",
    "            \n",
    "\n",
    "            lemmed_sentence = []\n",
    "\n",
    "            # Split review into words\n",
    "            for lemmed_words_idx in range(len(words_in_review)):\n",
    "                word = words_in_review[lemmed_words_idx]\n",
    "                \n",
    "                apply_lemmatization = lem.lemmatize(word)\n",
    "                # print(apply_lemmatization)\n",
    "                \n",
    "                lemmed_sentence.append(apply_lemmatization)\n",
    "                filtered_review = \" \".join(lemmed_sentence)\n",
    "        \n",
    "            # print(\"After lem update -- \", filtered_review)\n",
    "            # print()\n",
    "\n",
    "            lemmed_reviews.append(filtered_review)\n",
    "        else:\n",
    "            lemmed_reviews.append(text_review)\n",
    "\n",
    "    updated_df['lemmed_reviews'] = lemmed_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmed_df = lemmentize_review(no_stop_words_df, 'without_stop_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_unlemmed_words:\")\n",
    "# generate_sample_reviews(lemmed_df, 'lemmed_reviews', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Clean data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, col_name):\n",
    "    \"\"\"Perform lower case, remove HTML and URLs, remove contractions, remove non-alphabetical characters, remove extra spaces, remove stop words, and lemmatize\"\"\"\n",
    "\n",
    "    print(\"original reviews:\")\n",
    "    # generate_sample_reviews(df, col_name, 3)\n",
    "\n",
    "    reviews_lower_cased = convert_reviews_to_lower_case(df, col_name)\n",
    "    print(\"reviews_lower_cased:\")\n",
    "    # generate_sample_reviews(reviews_lower_cased, 'lower_cased', 3)\n",
    "\n",
    "    no_html_urls_df = remove_html_and_urls(reviews_lower_cased, 'lower_cased')\n",
    "    print(\"without_html_urls:\")\n",
    "    # generate_sample_reviews(no_html_urls_df, 'without_html_urls', 3)\n",
    "\n",
    "    no_contractions_df = remove_contractions(no_html_urls_df, 'without_html_urls')\n",
    "    print(\"without_contractions:\")\n",
    "    # generate_sample_reviews(no_contractions_df, 'without_contractions', 3)\n",
    "\n",
    "    only_alpha_chars_df = remove_non_alphabetical_characters(no_contractions_df, 'without_contractions')\n",
    "    print(\"with_alpha_chars_only:\")\n",
    "    # generate_sample_reviews(only_alpha_chars_df, 'with_alpha_chars_only', 3)\n",
    "\n",
    "    no_extra_space_df = remove_extra_spaces(only_alpha_chars_df, 'with_alpha_chars_only')\n",
    "    print(\"without_extra_space:\")\n",
    "    # generate_sample_reviews(no_extra_space_df, 'without_extra_space', 3)\n",
    "\n",
    "    no_stop_words_df = filter_stop_words(no_extra_space_df, 'without_extra_space')\n",
    "    print(\"without_stop_words:\")\n",
    "    # generate_sample_reviews(no_stop_words_df, 'without_stop_words', 3)\n",
    "    \n",
    "    lemmed_df = lemmentize_review(no_stop_words_df, 'without_stop_words')\n",
    "    print(\"without_unlemmed_words:\")\n",
    "    # generate_sample_reviews(lemmed_df, 'lemmed_reviews', 3)\n",
    "\n",
    "    return lemmed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original reviews:\n",
      "reviews_lower_cased:\n",
      "without_html_urls:\n",
      "without_contractions:\n",
      "with_alpha_chars_only:\n",
      "without_extra_space:\n",
      "without_stop_words:\n",
      "without_unlemmed_words:\n"
     ]
    }
   ],
   "source": [
    "cleaned_reviews_df = preprocess_data(sampled_reviews_ratings_df, 'review_body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>negative_review_class</th>\n",
       "      <th>neutral_review_class</th>\n",
       "      <th>positive_review_class</th>\n",
       "      <th>binary_review_class</th>\n",
       "      <th>ternary_review_class</th>\n",
       "      <th>lower_cased</th>\n",
       "      <th>without_html_urls</th>\n",
       "      <th>without_contractions</th>\n",
       "      <th>with_alpha_chars_only</th>\n",
       "      <th>without_extra_space</th>\n",
       "      <th>without_stop_words</th>\n",
       "      <th>lemmed_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2563244</th>\n",
       "      <td>1</td>\n",
       "      <td>The unit is a base and two handsets. One hands...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>the unit is a base and two handsets. one hands...</td>\n",
       "      <td>the unit is a base and two handsets. one hands...</td>\n",
       "      <td>the unit is a base and two handsets. one hands...</td>\n",
       "      <td>the unit is a base and two handsets  one hands...</td>\n",
       "      <td>the unit is a base and two handsets one handse...</td>\n",
       "      <td>unit base two handsets one handset annoying bu...</td>\n",
       "      <td>unit base two handset one handset annoying buz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611092</th>\n",
       "      <td>1</td>\n",
       "      <td>I received this and did not open them until re...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>i received this and did not open them until re...</td>\n",
       "      <td>i received this and did not open them until re...</td>\n",
       "      <td>i received this and did not open them until re...</td>\n",
       "      <td>i received this and did not open them until re...</td>\n",
       "      <td>i received this and did not open them until re...</td>\n",
       "      <td>received open recently find bars included wort...</td>\n",
       "      <td>received open recently find bar included worth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723857</th>\n",
       "      <td>1</td>\n",
       "      <td>I just find that the black ink of this package...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>i just find that the black ink of this package...</td>\n",
       "      <td>i just find that the black ink of this package...</td>\n",
       "      <td>i just find that the black ink of this package...</td>\n",
       "      <td>i just find that the black ink of this package...</td>\n",
       "      <td>i just find that the black ink of this package...</td>\n",
       "      <td>find black ink package work color color ink we...</td>\n",
       "      <td>find black ink package work color color ink we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761019</th>\n",
       "      <td>1</td>\n",
       "      <td>rip off ran out in one week !!!!!!!!</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>rip off ran out in one week !!!!!!!!</td>\n",
       "      <td>rip off ran out in one week !!!!!!!!</td>\n",
       "      <td>rip off ran out in one week !!!!!!!!</td>\n",
       "      <td>rip off ran out in one week</td>\n",
       "      <td>rip off ran out in one week</td>\n",
       "      <td>rip ran one week</td>\n",
       "      <td>rip ran one week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256629</th>\n",
       "      <td>1</td>\n",
       "      <td>Not pleased.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>not pleased.</td>\n",
       "      <td>not pleased.</td>\n",
       "      <td>not pleased.</td>\n",
       "      <td>not pleased</td>\n",
       "      <td>not pleased</td>\n",
       "      <td>pleased</td>\n",
       "      <td>pleased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812315</th>\n",
       "      <td>5</td>\n",
       "      <td>Great</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>great</td>\n",
       "      <td>great</td>\n",
       "      <td>great</td>\n",
       "      <td>great</td>\n",
       "      <td>great</td>\n",
       "      <td>great</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202491</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product but it's a shame that its discon...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>great product but it's a shame that its discon...</td>\n",
       "      <td>great product but it's a shame that its discon...</td>\n",
       "      <td>great product but it is a shame that its disco...</td>\n",
       "      <td>great product but it is a shame that its disco...</td>\n",
       "      <td>great product but it is a shame that its disco...</td>\n",
       "      <td>great product shame discontinued</td>\n",
       "      <td>great product shame discontinued</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573678</th>\n",
       "      <td>5</td>\n",
       "      <td>This box should last a long time and costs no ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>this box should last a long time and costs no ...</td>\n",
       "      <td>this box should last a long time and costs no ...</td>\n",
       "      <td>this box should last a long time and costs no ...</td>\n",
       "      <td>this box should last a long time and costs no ...</td>\n",
       "      <td>this box should last a long time and costs no ...</td>\n",
       "      <td>box last long time costs would cost run coins ...</td>\n",
       "      <td>box last long time cost would cost run coin co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466004</th>\n",
       "      <td>5</td>\n",
       "      <td>The Schmidt EF9000, is a great alternative to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>the schmidt ef9000, is a great alternative to ...</td>\n",
       "      <td>the schmidt ef9000, is a great alternative to ...</td>\n",
       "      <td>the schmidt ef9000, is a great alternative to ...</td>\n",
       "      <td>the schmidt ef      is a great alternative to ...</td>\n",
       "      <td>the schmidt ef is a great alternative to the r...</td>\n",
       "      <td>schmidt ef great alternative regular gel parke...</td>\n",
       "      <td>schmidt ef great alternative regular gel parke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343931</th>\n",
       "      <td>5</td>\n",
       "      <td>Item delivered on time, was as described...Exc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>item delivered on time, was as described...exc...</td>\n",
       "      <td>item delivered on time, was as described...exc...</td>\n",
       "      <td>item delivered on time, was as described...exc...</td>\n",
       "      <td>item delivered on time  was as described   exc...</td>\n",
       "      <td>item delivered on time was as described excell...</td>\n",
       "      <td>item delivered time described excellent</td>\n",
       "      <td>item delivered time described excellent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body  \\\n",
       "2563244            1  The unit is a base and two handsets. One hands...   \n",
       "611092             1  I received this and did not open them until re...   \n",
       "1723857            1  I just find that the black ink of this package...   \n",
       "761019             1               rip off ran out in one week !!!!!!!!   \n",
       "256629             1                                       Not pleased.   \n",
       "...              ...                                                ...   \n",
       "812315             5                                              Great   \n",
       "1202491            5  Great product but it's a shame that its discon...   \n",
       "1573678            5  This box should last a long time and costs no ...   \n",
       "1466004            5  The Schmidt EF9000, is a great alternative to ...   \n",
       "343931             5  Item delivered on time, was as described...Exc...   \n",
       "\n",
       "         negative_review_class  neutral_review_class  positive_review_class  \\\n",
       "2563244                    2.0                   NaN                    NaN   \n",
       "611092                     2.0                   NaN                    NaN   \n",
       "1723857                    2.0                   NaN                    NaN   \n",
       "761019                     2.0                   NaN                    NaN   \n",
       "256629                     2.0                   NaN                    NaN   \n",
       "...                        ...                   ...                    ...   \n",
       "812315                     NaN                   NaN                    1.0   \n",
       "1202491                    NaN                   NaN                    1.0   \n",
       "1573678                    NaN                   NaN                    1.0   \n",
       "1466004                    NaN                   NaN                    1.0   \n",
       "343931                     NaN                   NaN                    1.0   \n",
       "\n",
       "         binary_review_class  ternary_review_class  \\\n",
       "2563244                  2.0                   2.0   \n",
       "611092                   2.0                   2.0   \n",
       "1723857                  2.0                   2.0   \n",
       "761019                   2.0                   2.0   \n",
       "256629                   2.0                   2.0   \n",
       "...                      ...                   ...   \n",
       "812315                   1.0                   1.0   \n",
       "1202491                  1.0                   1.0   \n",
       "1573678                  1.0                   1.0   \n",
       "1466004                  1.0                   1.0   \n",
       "343931                   1.0                   1.0   \n",
       "\n",
       "                                               lower_cased  \\\n",
       "2563244  the unit is a base and two handsets. one hands...   \n",
       "611092   i received this and did not open them until re...   \n",
       "1723857  i just find that the black ink of this package...   \n",
       "761019                rip off ran out in one week !!!!!!!!   \n",
       "256629                                        not pleased.   \n",
       "...                                                    ...   \n",
       "812315                                               great   \n",
       "1202491  great product but it's a shame that its discon...   \n",
       "1573678  this box should last a long time and costs no ...   \n",
       "1466004  the schmidt ef9000, is a great alternative to ...   \n",
       "343931   item delivered on time, was as described...exc...   \n",
       "\n",
       "                                         without_html_urls  \\\n",
       "2563244  the unit is a base and two handsets. one hands...   \n",
       "611092   i received this and did not open them until re...   \n",
       "1723857  i just find that the black ink of this package...   \n",
       "761019                rip off ran out in one week !!!!!!!!   \n",
       "256629                                        not pleased.   \n",
       "...                                                    ...   \n",
       "812315                                               great   \n",
       "1202491  great product but it's a shame that its discon...   \n",
       "1573678  this box should last a long time and costs no ...   \n",
       "1466004  the schmidt ef9000, is a great alternative to ...   \n",
       "343931   item delivered on time, was as described...exc...   \n",
       "\n",
       "                                      without_contractions  \\\n",
       "2563244  the unit is a base and two handsets. one hands...   \n",
       "611092   i received this and did not open them until re...   \n",
       "1723857  i just find that the black ink of this package...   \n",
       "761019                rip off ran out in one week !!!!!!!!   \n",
       "256629                                        not pleased.   \n",
       "...                                                    ...   \n",
       "812315                                               great   \n",
       "1202491  great product but it is a shame that its disco...   \n",
       "1573678  this box should last a long time and costs no ...   \n",
       "1466004  the schmidt ef9000, is a great alternative to ...   \n",
       "343931   item delivered on time, was as described...exc...   \n",
       "\n",
       "                                     with_alpha_chars_only  \\\n",
       "2563244  the unit is a base and two handsets  one hands...   \n",
       "611092   i received this and did not open them until re...   \n",
       "1723857  i just find that the black ink of this package...   \n",
       "761019                rip off ran out in one week            \n",
       "256629                                        not pleased    \n",
       "...                                                    ...   \n",
       "812315                                               great   \n",
       "1202491  great product but it is a shame that its disco...   \n",
       "1573678  this box should last a long time and costs no ...   \n",
       "1466004  the schmidt ef      is a great alternative to ...   \n",
       "343931   item delivered on time  was as described   exc...   \n",
       "\n",
       "                                       without_extra_space  \\\n",
       "2563244  the unit is a base and two handsets one handse...   \n",
       "611092   i received this and did not open them until re...   \n",
       "1723857  i just find that the black ink of this package...   \n",
       "761019                        rip off ran out in one week    \n",
       "256629                                        not pleased    \n",
       "...                                                    ...   \n",
       "812315                                               great   \n",
       "1202491  great product but it is a shame that its disco...   \n",
       "1573678  this box should last a long time and costs no ...   \n",
       "1466004  the schmidt ef is a great alternative to the r...   \n",
       "343931   item delivered on time was as described excell...   \n",
       "\n",
       "                                        without_stop_words  \\\n",
       "2563244  unit base two handsets one handset annoying bu...   \n",
       "611092   received open recently find bars included wort...   \n",
       "1723857  find black ink package work color color ink we...   \n",
       "761019                                    rip ran one week   \n",
       "256629                                             pleased   \n",
       "...                                                    ...   \n",
       "812315                                               great   \n",
       "1202491                   great product shame discontinued   \n",
       "1573678  box last long time costs would cost run coins ...   \n",
       "1466004  schmidt ef great alternative regular gel parke...   \n",
       "343931             item delivered time described excellent   \n",
       "\n",
       "                                            lemmed_reviews  \n",
       "2563244  unit base two handset one handset annoying buz...  \n",
       "611092   received open recently find bar included worth...  \n",
       "1723857  find black ink package work color color ink we...  \n",
       "761019                                    rip ran one week  \n",
       "256629                                             pleased  \n",
       "...                                                    ...  \n",
       "812315                                               great  \n",
       "1202491                   great product shame discontinued  \n",
       "1573678  box last long time cost would cost run coin co...  \n",
       "1466004  schmidt ef great alternative regular gel parke...  \n",
       "343931             item delivered time described excellent  \n",
       "\n",
       "[250000 rows x 14 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_reviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, review_class):\n",
    "    embeddings_df = df.dropna(subset=[review_class])\n",
    "    print(len(embeddings_df), embeddings_df['star_rating'].unique())\n",
    "\n",
    "    specific_review_class = embeddings_df[review_class]\n",
    "    print(specific_review_class.unique())\n",
    "\n",
    "    text = embeddings_df.loc[:, ['lemmed_reviews']]\n",
    "    print(text)\n",
    "\n",
    "    ### Train test split so I can have the same train\n",
    "    X_train, X_test, y_train, y_test = train_test_split(text, specific_review_class, test_size=0.2, random_state=42)\n",
    "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary\n",
      "\n",
      "200000 [1 2 4 5]\n",
      "[2. 1.]\n",
      "                                            lemmed_reviews\n",
      "2563244  unit base two handset one handset annoying buz...\n",
      "611092   received open recently find bar included worth...\n",
      "1723857  find black ink package work color color ink we...\n",
      "761019                                    rip ran one week\n",
      "256629                                             pleased\n",
      "...                                                    ...\n",
      "812315                                               great\n",
      "1202491                   great product shame discontinued\n",
      "1573678  box last long time cost would cost run coin co...\n",
      "1466004  schmidt ef great alternative regular gel parke...\n",
      "343931             item delivered time described excellent\n",
      "\n",
      "[200000 rows x 1 columns]\n",
      "(160000, 1) (40000, 1) (160000,) (40000,)\n",
      "\n",
      "Ternary\n",
      "250000 [1 2 3 4 5]\n",
      "[2. 3. 1.]\n",
      "                                            lemmed_reviews\n",
      "2563244  unit base two handset one handset annoying buz...\n",
      "611092   received open recently find bar included worth...\n",
      "1723857  find black ink package work color color ink we...\n",
      "761019                                    rip ran one week\n",
      "256629                                             pleased\n",
      "...                                                    ...\n",
      "812315                                               great\n",
      "1202491                   great product shame discontinued\n",
      "1573678  box last long time cost would cost run coin co...\n",
      "1466004  schmidt ef great alternative regular gel parke...\n",
      "343931             item delivered time described excellent\n",
      "\n",
      "[250000 rows x 1 columns]\n",
      "(200000, 1) (50000, 1) (200000,) (50000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Binary\\n\")\n",
    "binary_X_train, binary_X_test, binary_y_train, binary_y_test = split_data(cleaned_reviews_df, 'binary_review_class')\n",
    "print(\"\\nTernary\")\n",
    "ternary_X_train, ternary_X_test, ternary_y_train, ternary_y_test = split_data(cleaned_reviews_df, 'ternary_review_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model and train my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 00:03:52,077 : INFO : loading projection weights from /Users/brinkley97/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
      "2024-02-08 00:04:25,513 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from /Users/brinkley97/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-02-08T00:04:25.513899', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import gensim.downloader as api\n",
    "pretrained_word_two_vec_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 00:04:25,558 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2024-02-08 00:04:25,559 : INFO : built Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...> from 9 documents (total 29 corpus positions)\n",
      "2024-02-08 00:04:25,560 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...> from 9 documents (total 29 corpus positions)\", 'datetime': '2024-02-08T00:04:25.560113', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, col_name: str):\n",
    "        self.df = df\n",
    "        self.col_name = col_name\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: `pd.DataFrame`\n",
    "            The data\n",
    "        \n",
    "        col_name: `str`\n",
    "            Column with reviews\n",
    "\n",
    "        words_in_model: `list`\n",
    "            Words in Word2Vec model\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        text_reviews = self.df[self.col_name].values\n",
    "\n",
    "        for text_reviews_idx in range(len(text_reviews)):\n",
    "            text_review = text_reviews[text_reviews_idx]\n",
    "            # print(text_reviews_idx, \"--\", text_review)\n",
    "\n",
    "            yield utils.simple_preprocess(text_review)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 00:04:25,646 : INFO : collecting all words and their counts\n",
      "2024-02-08 00:04:25,647 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Case\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 00:04:26,093 : INFO : PROGRESS: at sentence #10000, processed 293727 words, keeping 13823 word types\n",
      "2024-02-08 00:04:26,436 : INFO : PROGRESS: at sentence #20000, processed 587257 words, keeping 19024 word types\n",
      "2024-02-08 00:04:26,799 : INFO : PROGRESS: at sentence #30000, processed 894215 words, keeping 23164 word types\n",
      "2024-02-08 00:04:27,164 : INFO : PROGRESS: at sentence #40000, processed 1201440 words, keeping 26686 word types\n",
      "2024-02-08 00:04:27,525 : INFO : PROGRESS: at sentence #50000, processed 1502177 words, keeping 29520 word types\n",
      "2024-02-08 00:04:27,862 : INFO : PROGRESS: at sentence #60000, processed 1797393 words, keeping 32333 word types\n",
      "2024-02-08 00:04:28,205 : INFO : PROGRESS: at sentence #70000, processed 2092279 words, keeping 34748 word types\n",
      "2024-02-08 00:04:28,564 : INFO : PROGRESS: at sentence #80000, processed 2400258 words, keeping 36963 word types\n",
      "2024-02-08 00:04:28,914 : INFO : PROGRESS: at sentence #90000, processed 2705194 words, keeping 39182 word types\n",
      "2024-02-08 00:04:29,290 : INFO : PROGRESS: at sentence #100000, processed 2994752 words, keeping 41113 word types\n",
      "2024-02-08 00:04:29,705 : INFO : PROGRESS: at sentence #110000, processed 3288772 words, keeping 42948 word types\n",
      "2024-02-08 00:04:30,100 : INFO : PROGRESS: at sentence #120000, processed 3595617 words, keeping 44838 word types\n",
      "2024-02-08 00:04:30,502 : INFO : PROGRESS: at sentence #130000, processed 3897860 words, keeping 46746 word types\n",
      "2024-02-08 00:04:30,943 : INFO : PROGRESS: at sentence #140000, processed 4203706 words, keeping 48505 word types\n",
      "2024-02-08 00:04:31,316 : INFO : PROGRESS: at sentence #150000, processed 4497700 words, keeping 50098 word types\n",
      "2024-02-08 00:04:31,730 : INFO : collected 51664 word types from a corpus of 4791520 raw words and 160000 sentences\n",
      "2024-02-08 00:04:31,731 : INFO : Creating a fresh vocabulary\n",
      "2024-02-08 00:04:31,766 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 11638 unique words (22.53% of original 51664, drops 40026)', 'datetime': '2024-02-08T00:04:31.766900', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-08 00:04:31,767 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 4706408 word corpus (98.22% of original 4791520, drops 85112)', 'datetime': '2024-02-08T00:04:31.767621', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-08 00:04:31,818 : INFO : deleting the raw counts dictionary of 51664 items\n",
      "2024-02-08 00:04:31,819 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2024-02-08 00:04:31,820 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4328502.021835381 word corpus (92.0%% of prior 4706408)', 'datetime': '2024-02-08T00:04:31.820752', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-08 00:04:31,896 : INFO : estimated required memory for 11638 words and 300 dimensions: 33750200 bytes\n",
      "2024-02-08 00:04:31,897 : INFO : resetting layer weights\n",
      "2024-02-08 00:04:31,911 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-08T00:04:31.911659', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2024-02-08 00:04:31,912 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 11638 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=11 shrink_windows=True', 'datetime': '2024-02-08T00:04:31.912280', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-08 00:04:32,920 : INFO : EPOCH 0 - PROGRESS: at 10.41% examples, 439470 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:04:33,928 : INFO : EPOCH 0 - PROGRESS: at 21.22% examples, 455887 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:04:34,937 : INFO : EPOCH 0 - PROGRESS: at 31.91% examples, 458499 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:04:35,943 : INFO : EPOCH 0 - PROGRESS: at 42.63% examples, 457759 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:04:36,978 : INFO : EPOCH 0 - PROGRESS: at 54.19% examples, 465372 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:04:37,985 : INFO : EPOCH 0 - PROGRESS: at 65.70% examples, 468108 words/s, in_qsize 4, out_qsize 1\n",
      "2024-02-08 00:04:38,997 : INFO : EPOCH 0 - PROGRESS: at 76.98% examples, 471154 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:04:40,002 : INFO : EPOCH 0 - PROGRESS: at 87.60% examples, 470346 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:04:40,988 : INFO : EPOCH 0: training on 4791520 raw words (4327845 effective words) took 9.1s, 477232 effective words/s\n",
      "2024-02-08 00:04:42,015 : INFO : EPOCH 1 - PROGRESS: at 10.36% examples, 431658 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:04:43,029 : INFO : EPOCH 1 - PROGRESS: at 21.39% examples, 454882 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:04:44,046 : INFO : EPOCH 1 - PROGRESS: at 32.09% examples, 456711 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:04:45,057 : INFO : EPOCH 1 - PROGRESS: at 43.04% examples, 458091 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:04:46,057 : INFO : EPOCH 1 - PROGRESS: at 53.60% examples, 459890 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:04:47,075 : INFO : EPOCH 1 - PROGRESS: at 65.27% examples, 464199 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:04:48,084 : INFO : EPOCH 1 - PROGRESS: at 75.75% examples, 462884 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:04:49,101 : INFO : EPOCH 1 - PROGRESS: at 86.99% examples, 465797 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:04:50,108 : INFO : EPOCH 1 - PROGRESS: at 98.22% examples, 466552 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:04:50,166 : INFO : EPOCH 1: training on 4791520 raw words (4328748 effective words) took 9.2s, 472060 effective words/s\n",
      "2024-02-08 00:04:51,186 : INFO : EPOCH 2 - PROGRESS: at 10.12% examples, 425958 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:04:52,201 : INFO : EPOCH 2 - PROGRESS: at 21.16% examples, 451817 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:04:53,225 : INFO : EPOCH 2 - PROGRESS: at 32.49% examples, 462305 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:04:54,255 : INFO : EPOCH 2 - PROGRESS: at 43.90% examples, 464559 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:04:55,267 : INFO : EPOCH 2 - PROGRESS: at 54.81% examples, 467597 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:04:56,274 : INFO : EPOCH 2 - PROGRESS: at 66.27% examples, 469979 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:04:57,281 : INFO : EPOCH 2 - PROGRESS: at 77.58% examples, 473074 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:04:58,288 : INFO : EPOCH 2 - PROGRESS: at 88.92% examples, 475193 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:04:59,191 : INFO : EPOCH 2: training on 4791520 raw words (4328451 effective words) took 9.0s, 480022 effective words/s\n",
      "2024-02-08 00:05:00,205 : INFO : EPOCH 3 - PROGRESS: at 10.36% examples, 439033 words/s, in_qsize 6, out_qsize 1\n",
      "2024-02-08 00:05:01,230 : INFO : EPOCH 3 - PROGRESS: at 22.02% examples, 469610 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:02,239 : INFO : EPOCH 3 - PROGRESS: at 33.10% examples, 473534 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:05:03,248 : INFO : EPOCH 3 - PROGRESS: at 44.76% examples, 477719 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:04,270 : INFO : EPOCH 3 - PROGRESS: at 54.60% examples, 468294 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:05,283 : INFO : EPOCH 3 - PROGRESS: at 66.27% examples, 471615 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:05:06,285 : INFO : EPOCH 3 - PROGRESS: at 76.97% examples, 470998 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:07,288 : INFO : EPOCH 3 - PROGRESS: at 88.48% examples, 474820 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:05:08,176 : INFO : EPOCH 3: training on 4791520 raw words (4328760 effective words) took 9.0s, 482455 effective words/s\n",
      "2024-02-08 00:05:09,193 : INFO : EPOCH 4 - PROGRESS: at 10.41% examples, 438391 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:05:10,205 : INFO : EPOCH 4 - PROGRESS: at 21.16% examples, 454349 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:05:11,240 : INFO : EPOCH 4 - PROGRESS: at 32.88% examples, 468169 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:05:12,254 : INFO : EPOCH 4 - PROGRESS: at 44.55% examples, 472982 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:05:13,263 : INFO : EPOCH 4 - PROGRESS: at 55.43% examples, 474664 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:05:14,281 : INFO : EPOCH 4 - PROGRESS: at 66.90% examples, 475065 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:15,293 : INFO : EPOCH 4 - PROGRESS: at 78.61% examples, 479639 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:05:16,306 : INFO : EPOCH 4 - PROGRESS: at 89.57% examples, 478409 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:05:17,155 : INFO : EPOCH 4: training on 4791520 raw words (4328243 effective words) took 9.0s, 482760 effective words/s\n",
      "2024-02-08 00:05:17,156 : INFO : Word2Vec lifecycle event {'msg': 'training on 23957600 raw words (21642047 effective words) took 45.2s, 478347 effective words/s', 'datetime': '2024-02-08T00:05:17.156415', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-08 00:05:17,156 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=11638, vector_size=300, alpha=0.025>', 'datetime': '2024-02-08T00:05:17.156852', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'created'}\n",
      "2024-02-08 00:05:17,157 : INFO : collecting all words and their counts\n",
      "2024-02-08 00:05:17,158 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ternary Case\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 00:05:17,537 : INFO : PROGRESS: at sentence #10000, processed 302667 words, keeping 14376 word types\n",
      "2024-02-08 00:05:17,896 : INFO : PROGRESS: at sentence #20000, processed 604443 words, keeping 19554 word types\n",
      "2024-02-08 00:05:18,330 : INFO : PROGRESS: at sentence #30000, processed 907113 words, keeping 23520 word types\n",
      "2024-02-08 00:05:18,752 : INFO : PROGRESS: at sentence #40000, processed 1204786 words, keeping 26735 word types\n",
      "2024-02-08 00:05:19,150 : INFO : PROGRESS: at sentence #50000, processed 1509381 words, keeping 29638 word types\n",
      "2024-02-08 00:05:19,517 : INFO : PROGRESS: at sentence #60000, processed 1815507 words, keeping 32383 word types\n",
      "2024-02-08 00:05:19,880 : INFO : PROGRESS: at sentence #70000, processed 2123811 words, keeping 34907 word types\n",
      "2024-02-08 00:05:20,233 : INFO : PROGRESS: at sentence #80000, processed 2424881 words, keeping 37098 word types\n",
      "2024-02-08 00:05:20,597 : INFO : PROGRESS: at sentence #90000, processed 2727957 words, keeping 39316 word types\n",
      "2024-02-08 00:05:20,959 : INFO : PROGRESS: at sentence #100000, processed 3034355 words, keeping 41207 word types\n",
      "2024-02-08 00:05:21,326 : INFO : PROGRESS: at sentence #110000, processed 3340609 words, keeping 43176 word types\n",
      "2024-02-08 00:05:21,683 : INFO : PROGRESS: at sentence #120000, processed 3639932 words, keeping 45060 word types\n",
      "2024-02-08 00:05:22,071 : INFO : PROGRESS: at sentence #130000, processed 3944713 words, keeping 46854 word types\n",
      "2024-02-08 00:05:22,440 : INFO : PROGRESS: at sentence #140000, processed 4247785 words, keeping 48522 word types\n",
      "2024-02-08 00:05:22,803 : INFO : PROGRESS: at sentence #150000, processed 4555792 words, keeping 50218 word types\n",
      "2024-02-08 00:05:23,176 : INFO : PROGRESS: at sentence #160000, processed 4866603 words, keeping 51961 word types\n",
      "2024-02-08 00:05:23,554 : INFO : PROGRESS: at sentence #170000, processed 5178642 words, keeping 53613 word types\n",
      "2024-02-08 00:05:23,927 : INFO : PROGRESS: at sentence #180000, processed 5489697 words, keeping 55154 word types\n",
      "2024-02-08 00:05:24,303 : INFO : PROGRESS: at sentence #190000, processed 5789539 words, keeping 56626 word types\n",
      "2024-02-08 00:05:24,674 : INFO : collected 58046 word types from a corpus of 6099933 raw words and 200000 sentences\n",
      "2024-02-08 00:05:24,675 : INFO : Creating a fresh vocabulary\n",
      "2024-02-08 00:05:24,720 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 12985 unique words (22.37% of original 58046, drops 45061)', 'datetime': '2024-02-08T00:05:24.720122', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-08 00:05:24,721 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 6005385 word corpus (98.45% of original 6099933, drops 94548)', 'datetime': '2024-02-08T00:05:24.721372', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-08 00:05:24,778 : INFO : deleting the raw counts dictionary of 58046 items\n",
      "2024-02-08 00:05:24,779 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2024-02-08 00:05:24,780 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 5539930.1907732785 word corpus (92.2%% of prior 6005385)', 'datetime': '2024-02-08T00:05:24.780231', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-08 00:05:24,864 : INFO : estimated required memory for 12985 words and 300 dimensions: 37656500 bytes\n",
      "2024-02-08 00:05:24,865 : INFO : resetting layer weights\n",
      "2024-02-08 00:05:24,878 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-08T00:05:24.878691', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2024-02-08 00:05:24,879 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 12985 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=11 shrink_windows=True', 'datetime': '2024-02-08T00:05:24.879269', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-08 00:05:25,922 : INFO : EPOCH 0 - PROGRESS: at 8.23% examples, 435038 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:26,954 : INFO : EPOCH 0 - PROGRESS: at 15.64% examples, 414723 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:05:27,963 : INFO : EPOCH 0 - PROGRESS: at 24.54% examples, 437687 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:05:28,969 : INFO : EPOCH 0 - PROGRESS: at 33.46% examples, 451403 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:29,975 : INFO : EPOCH 0 - PROGRESS: at 42.03% examples, 454335 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:05:30,997 : INFO : EPOCH 0 - PROGRESS: at 50.67% examples, 456718 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:05:31,998 : INFO : EPOCH 0 - PROGRESS: at 59.04% examples, 457337 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:05:33,029 : INFO : EPOCH 0 - PROGRESS: at 67.95% examples, 459414 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:34,031 : INFO : EPOCH 0 - PROGRESS: at 76.50% examples, 461485 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:05:35,038 : INFO : EPOCH 0 - PROGRESS: at 85.25% examples, 464595 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:05:36,048 : INFO : EPOCH 0 - PROGRESS: at 93.34% examples, 463070 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:05:36,717 : INFO : EPOCH 0: training on 6099933 raw words (5539799 effective words) took 11.8s, 468214 effective words/s\n",
      "2024-02-08 00:05:37,736 : INFO : EPOCH 1 - PROGRESS: at 8.07% examples, 437241 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:05:38,739 : INFO : EPOCH 1 - PROGRESS: at 16.65% examples, 453136 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:05:39,744 : INFO : EPOCH 1 - PROGRESS: at 25.51% examples, 464265 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:40,753 : INFO : EPOCH 1 - PROGRESS: at 34.44% examples, 471231 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:41,769 : INFO : EPOCH 1 - PROGRESS: at 43.83% examples, 478191 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:42,789 : INFO : EPOCH 1 - PROGRESS: at 52.13% examples, 473899 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:43,821 : INFO : EPOCH 1 - PROGRESS: at 61.17% examples, 475165 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:44,851 : INFO : EPOCH 1 - PROGRESS: at 70.20% examples, 476170 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:45,874 : INFO : EPOCH 1 - PROGRESS: at 79.05% examples, 477224 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:05:46,884 : INFO : EPOCH 1 - PROGRESS: at 87.42% examples, 476912 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:47,895 : INFO : EPOCH 1 - PROGRESS: at 96.67% examples, 479029 words/s, in_qsize 4, out_qsize 1\n",
      "2024-02-08 00:05:48,197 : INFO : EPOCH 1: training on 6099933 raw words (5540489 effective words) took 11.5s, 482989 effective words/s\n",
      "2024-02-08 00:05:49,228 : INFO : EPOCH 2 - PROGRESS: at 8.21% examples, 440377 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:05:50,237 : INFO : EPOCH 2 - PROGRESS: at 16.97% examples, 457645 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:51,253 : INFO : EPOCH 2 - PROGRESS: at 26.01% examples, 468444 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:52,265 : INFO : EPOCH 2 - PROGRESS: at 34.76% examples, 471787 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:53,273 : INFO : EPOCH 2 - PROGRESS: at 43.69% examples, 474077 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:05:54,308 : INFO : EPOCH 2 - PROGRESS: at 52.40% examples, 473718 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:55,330 : INFO : EPOCH 2 - PROGRESS: at 61.67% examples, 476883 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:56,338 : INFO : EPOCH 2 - PROGRESS: at 70.87% examples, 480023 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:57,339 : INFO : EPOCH 2 - PROGRESS: at 79.23% examples, 478874 words/s, in_qsize 4, out_qsize 1\n",
      "2024-02-08 00:05:58,339 : INFO : EPOCH 2 - PROGRESS: at 88.05% examples, 481547 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:05:59,342 : INFO : EPOCH 2 - PROGRESS: at 96.67% examples, 480369 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:05:59,642 : INFO : EPOCH 2: training on 6099933 raw words (5540151 effective words) took 11.4s, 484386 effective words/s\n",
      "2024-02-08 00:06:00,683 : INFO : EPOCH 3 - PROGRESS: at 8.38% examples, 451031 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:06:01,689 : INFO : EPOCH 3 - PROGRESS: at 17.75% examples, 481534 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:06:02,705 : INFO : EPOCH 3 - PROGRESS: at 26.83% examples, 484255 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:06:03,720 : INFO : EPOCH 3 - PROGRESS: at 35.08% examples, 476610 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:06:04,751 : INFO : EPOCH 3 - PROGRESS: at 44.14% examples, 477566 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:06:05,753 : INFO : EPOCH 3 - PROGRESS: at 52.77% examples, 477748 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:06:06,766 : INFO : EPOCH 3 - PROGRESS: at 61.34% examples, 475836 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:06:07,777 : INFO : EPOCH 3 - PROGRESS: at 70.35% examples, 477822 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:06:08,801 : INFO : EPOCH 3 - PROGRESS: at 79.19% examples, 478651 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:06:09,805 : INFO : EPOCH 3 - PROGRESS: at 87.72% examples, 479407 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:06:10,807 : INFO : EPOCH 3 - PROGRESS: at 96.50% examples, 479219 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:06:11,119 : INFO : EPOCH 3: training on 6099933 raw words (5539209 effective words) took 11.5s, 483504 effective words/s\n",
      "2024-02-08 00:06:12,142 : INFO : EPOCH 4 - PROGRESS: at 8.54% examples, 461654 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:06:13,158 : INFO : EPOCH 4 - PROGRESS: at 17.43% examples, 471211 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:06:14,158 : INFO : EPOCH 4 - PROGRESS: at 26.18% examples, 473920 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:06:15,175 : INFO : EPOCH 4 - PROGRESS: at 34.90% examples, 475430 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:06:16,182 : INFO : EPOCH 4 - PROGRESS: at 43.48% examples, 473525 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:06:17,199 : INFO : EPOCH 4 - PROGRESS: at 52.59% examples, 477694 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:06:18,203 : INFO : EPOCH 4 - PROGRESS: at 61.50% examples, 478973 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:06:19,209 : INFO : EPOCH 4 - PROGRESS: at 70.36% examples, 479758 words/s, in_qsize 4, out_qsize 1\n",
      "2024-02-08 00:06:20,215 : INFO : EPOCH 4 - PROGRESS: at 79.23% examples, 481307 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:06:21,222 : INFO : EPOCH 4 - PROGRESS: at 87.09% examples, 478072 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:06:22,228 : INFO : EPOCH 4 - PROGRESS: at 95.82% examples, 477873 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-08 00:06:22,591 : INFO : EPOCH 4: training on 6099933 raw words (5540003 effective words) took 11.5s, 483231 effective words/s\n",
      "2024-02-08 00:06:22,592 : INFO : Word2Vec lifecycle event {'msg': 'training on 30499665 raw words (27699651 effective words) took 57.7s, 479961 effective words/s', 'datetime': '2024-02-08T00:06:22.592254', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-08 00:06:22,592 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=12985, vector_size=300, alpha=0.025>', 'datetime': '2024-02-08T00:06:22.592650', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Binary Case\")\n",
    "binary_X_train_sentences = MyCorpus(binary_X_train, 'lemmed_reviews')\n",
    "my_binary_X_train_model = gensim.models.Word2Vec(sentences=binary_X_train_sentences, vector_size=300, window=11, min_count=10)\n",
    "\n",
    "# X test - get embeddings from my_binary_X_train_model -- vec_king = my_binary_X_train_model.wv['king']\n",
    "# binary_X_test_sentences = MyCorpus(binary_X_test, 'lemmed_reviews')\n",
    "# sentences = MyCorpus(sampled_reviews_ratings_df, 'review_body')\n",
    "# print(\"\\nSentences\", binary_X_test_sentences)\n",
    "\n",
    "print(\"\\nTernary Case\")\n",
    "ternary_X_train_sentences = MyCorpus(ternary_X_train, 'lemmed_reviews')\n",
    "my_ternary_X_train_model = gensim.models.Word2Vec(sentences=ternary_X_train_sentences, vector_size=300, window=11, min_count=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar scores\n",
    "\n",
    "[ ] Write summary of differences between their model and my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118193507194519)]\n"
     ]
    }
   ],
   "source": [
    "result = pretrained_word_two_vec_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x21569e290>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_trained_binary_X_train_model = my_binary_X_train_model.wv\n",
    "my_trained_binary_X_train_model\n",
    "\n",
    "my_trained_ternary_X_train_model = my_ternary_X_train_model.wv\n",
    "my_trained_ternary_X_train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('began', 0.46759265661239624), ('kept', 0.46228381991386414), ('became', 0.3991070091724396), ('suddenly', 0.396391898393631), ('progressively', 0.3889046907424927), ('acting', 0.38407227396965027), ('eventually', 0.3838014006614685), ('afterwards', 0.3727993369102478), ('sudden', 0.3654063940048218), ('starting', 0.3636440932750702)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Fix with proper exs\n",
    "my_result = my_trained_binary_X_train_model.most_similar(positive=['started', 'even'], negative=['plus', 'toner'], topn=10)\n",
    "print(my_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embeddings(df: pd.DataFrame, col_name: str, model_to_use):\n",
    "    \"\"\"Extract word embeddings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    model_to_use:\n",
    "        Either the pretrained model or my pretrained model\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    sentence_vectorized = []\n",
    "    mean_sentences_vectorized = []\n",
    "    concatenated_features = []\n",
    "    sentences = df[col_name].values\n",
    "\n",
    "    for sentences_idx in range(len(sentences)):\n",
    "        vectorized_words = []\n",
    "        concatenated_feature = np.random.rand(10)\n",
    "        sentence = sentences[sentences_idx]\n",
    "        # print(\"Sentence\", sentences_idx)\n",
    "        # print(\"Sentence\", sentences_idx, \"Pre-vectorized -- \", sentence)\n",
    "        for word_idx, word in enumerate(sentence.split(\" \")):\n",
    "            if word in model_to_use.key_to_index:\n",
    "                vector_of_word = model_to_use[word]\n",
    "                vectorized_words.append(vector_of_word)\n",
    "                if word_idx < 10:\n",
    "                    concatenated_feature += vector_of_word[:10]  # Concatenate first 10 features\n",
    "            else:\n",
    "                vector_of_word = np.random.rand(model_to_use.vector_size)\n",
    "                vectorized_words.append(vector_of_word)\n",
    "                if word_idx < 10:\n",
    "                    concatenated_feature += vector_of_word[:10]\n",
    "                \n",
    "        sentence_vectorized.append(vectorized_words)\n",
    "        # print(\"Sentence\", sentences_idx, \"Post-vectorized \\n\")\n",
    "        mean_of_sentence = np.mean(sentence_vectorized[sentences_idx], axis=0)\n",
    "        mean_sentences_vectorized.append(mean_of_sentence)\n",
    "        concatenated_features.append(concatenated_feature)\n",
    "\n",
    "\n",
    "    return mean_sentences_vectorized, concatenated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract embeddings for pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Pretrained Train\n",
      "Binary Pretrained Test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((160000, 300), (40000, 300), (40000, 10), (40000, 10))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Binary Pretrained Train\")\n",
    "pretrained_binary_train_embeddings, pretrained_concat_binary_train_embeddings = word_embeddings(binary_X_train, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_binary_train_embeddings = np.array(pretrained_binary_train_embeddings)\n",
    "pretrained_concat_binary_train_embeddings = np.array(pretrained_concat_binary_train_embeddings)\n",
    "\n",
    "\n",
    "print(\"Binary Pretrained Test\")\n",
    "pretrained_binary_test_embeddings, pretrained_concat_binary_test_embeddings = word_embeddings(binary_X_test, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_binary_test_embeddings = np.array(pretrained_binary_test_embeddings)\n",
    "pretrained_concat_binary_test_embeddings = np.array(pretrained_concat_binary_test_embeddings)\n",
    "\n",
    "\n",
    "pretrained_binary_train_embeddings.shape, pretrained_binary_test_embeddings.shape, pretrained_concat_binary_test_embeddings.shape, pretrained_concat_binary_test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ternary Pretrained Train\n",
      "Ternary Pretrained Test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((200000, 300), (50000, 300), (200000, 10), (50000, 10))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Ternary Pretrained Train\")\n",
    "pretrained_ternary_train_embeddings, pretrained_concat_ternary_train_embeddings = word_embeddings(ternary_X_train, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_ternary_train_embeddings = np.array(pretrained_ternary_train_embeddings)\n",
    "pretrained_concat_ternary_train_embeddings = np.array(pretrained_concat_ternary_train_embeddings)\n",
    "\n",
    "print(\"Ternary Pretrained Test\")\n",
    "pretrained_ternary_test_embeddings, pretrained_concat_ternary_test_embeddings = word_embeddings(ternary_X_test, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_ternary_test_embeddings = np.array(pretrained_ternary_test_embeddings)\n",
    "pretrained_concat_ternary_test_embeddings = np.array(pretrained_concat_ternary_test_embeddings)\n",
    "\n",
    "pretrained_ternary_train_embeddings.shape, pretrained_ternary_test_embeddings.shape, pretrained_concat_ternary_train_embeddings.shape, pretrained_concat_ternary_test_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract embeddings for my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary My Model Train\n",
      "Binary My Model Test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((160000, 300), (40000, 300))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Binary My Model Train\")\n",
    "my_binary_train_embeddings, concats = word_embeddings(binary_X_train, 'lemmed_reviews', my_trained_binary_X_train_model)\n",
    "my_binary_train_embeddings = np.array(my_binary_train_embeddings)\n",
    "\n",
    "print(\"Binary My Model Test\")\n",
    "my_binary_test_embeddings, concats_2 = word_embeddings(binary_X_test, 'lemmed_reviews', my_trained_binary_X_train_model)\n",
    "my_binary_test_embeddings = np.array(my_binary_test_embeddings)\n",
    "\n",
    "my_binary_train_embeddings.shape, my_binary_test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ternary My Model Train\n",
      "Ternary My Model Test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((200000, 300), (50000, 300))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Ternary My Model Train\")\n",
    "my_ternary_train_embeddings, concat3 = word_embeddings(ternary_X_train, 'lemmed_reviews', my_trained_ternary_X_train_model)\n",
    "my_ternary_train_embeddings = np.array(my_ternary_train_embeddings)\n",
    "\n",
    "print(\"Ternary My Model Test\")\n",
    "my_ternary_test_embeddings, concat4 = word_embeddings(ternary_X_test, 'lemmed_reviews', my_trained_ternary_X_train_model)\n",
    "my_ternary_test_embeddings = np.array(my_ternary_test_embeddings)\n",
    "\n",
    "my_ternary_train_embeddings.shape, my_ternary_test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_feature_extraction(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Extract the TF-IDF features from the reviews.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    tf_idf_features:\n",
    "        A matrix containing the TF-IDF features extracted\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tf_idf_features = vectorizer.fit_transform(df[col_name])\n",
    "\n",
    "    return tf_idf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews_df = cleaned_reviews_df.dropna(subset=['binary_review_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_features = tf_idf_feature_extraction(cleaned_reviews_df, 'lemmed_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x58064 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 26 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binnary_reviews = cleaned_reviews_df['binary_review_class']\n",
    "binnary_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test = train_test_split(tf_idf_features, binnary_reviews, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(y_true, y_prediction):\n",
    "    return sklearn.metrics.accuracy_score(y_true, y_prediction)\n",
    "\n",
    "# def eval_precision(y_true, y_prediction):\n",
    "#     return sklearn.metrics.precision_score(y_true, y_prediction)\n",
    "\n",
    "# def eval_recall(y_true, y_prediction):\n",
    "#     return sklearn.metrics.recall_score(y_true, y_prediction)\n",
    "\n",
    "# def eval_f1_score(y_true, y_prediction):\n",
    "#     return sklearn.metrics.f1_score(y_true, y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_metric(y_train_true, y_train_predictions):\n",
    "    accuracy = eval_accuracy(y_train_true, y_train_predictions)\n",
    "    # precision = eval_precision(y_train_true, y_train_predictions)\n",
    "    # recall = eval_recall(y_train_true, y_train_predictions)\n",
    "    # f1 = eval_f1_score(y_train_true, y_train_predictions)\n",
    "\n",
    "    metrics_dict = {\n",
    "        'Accuracy': accuracy,\n",
    "        # 'Precision': precision,\n",
    "        # 'Recall': recall,\n",
    "        # 'F1 Score': f1\n",
    "    }\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "def test_eval_metric(y_test_true, y_test_predictions):\n",
    "    accuracy = eval_accuracy(y_test_true, y_test_predictions)\n",
    "    # precision = eval_precision(y_test_true, y_test_predictions)\n",
    "    # recall = eval_recall(y_test_true, y_test_predictions)\n",
    "    # f1 = eval_f1_score(y_test_true, y_test_predictions)\n",
    "\n",
    "    metrics_dict = {\n",
    "        'Accuracy': accuracy,\n",
    "        # 'Precision': precision,\n",
    "        # 'Recall': recall,\n",
    "        # 'F1 Score': f1\n",
    "    }\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for perceptron on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_model(X_train, X_test, y_train, y_test): \n",
    "\n",
    "    technique = Perceptron(tol=1e-3, random_state=0)\n",
    "    technique.fit(X_train, y_train)\n",
    "    y_train_predictions = technique.predict(X_train)\n",
    "    y_test_predictions = technique.predict(X_test)\n",
    "\n",
    "\n",
    "    train_metrics = train_eval_metric(y_train, y_train_predictions)\n",
    "    test_metrics = test_eval_metric(y_test, y_test_predictions)\n",
    "\n",
    "    return train_metrics, test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_perceptron_train_metrics, tfidf_perceptron_test_metrics = perceptron_model(tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.87955625}, {'Accuracy': 0.824575})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_perceptron_train_metrics, tfidf_perceptron_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained model\n",
    "perceptron_train_metrics, perceptron_test_metrics = perceptron_model(pretrained_binary_train_embeddings, pretrained_binary_test_embeddings, binary_y_train, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.76995625}, {'Accuracy': 0.76765})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_train_metrics, perceptron_test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for perceptron on my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160000, 300), (40000, 300), (160000,), (40000,))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_binary_train_embeddings.shape, my_binary_test_embeddings.shape, binary_y_train.shape, binary_y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My model\n",
    "my_perceptron_train_metrics, my_perceptron_test_metrics = perceptron_model(my_binary_train_embeddings, my_binary_test_embeddings, binary_y_train, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.7528375}, {'Accuracy': 0.752475})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_perceptron_train_metrics, my_perceptron_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_model(X_train, X_test, y_train, y_test): \n",
    "\n",
    "    technique = LinearSVC(tol=1e-3, random_state=0)\n",
    "    technique.fit(X_train, y_train)\n",
    "    y_train_predictions = technique.predict(X_train)\n",
    "    y_test_predictions = technique.predict(X_test)\n",
    "\n",
    "\n",
    "    train_metrics = train_eval_metric(y_train, y_train_predictions)\n",
    "    test_metrics = test_eval_metric(y_test, y_test_predictions)\n",
    "\n",
    "    return train_metrics, test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tfidf_perceptron_train_metrics, tfidf_perceptron_test_metrics = svm_model(tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.91126875}, {'Accuracy': 0.8668})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_perceptron_train_metrics, tfidf_perceptron_test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for svm on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svm_train_metrics, svm_test_metrics = svm_model(pretrained_binary_train_embeddings, pretrained_binary_test_embeddings, binary_y_train, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.816125}, {'Accuracy': 0.812425})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_train_metrics, svm_test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for svm on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "my_svm_train_metrics, my_svm_test_metrics = svm_model(my_binary_train_embeddings, my_binary_test_embeddings, binary_y_train, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.850875}, {'Accuracy': 0.8465})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_svm_train_metrics, my_svm_test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do I conclude from comparing performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network\n",
    "- MLP for sentiment analysis classification\n",
    "- 2 hidden layers each with 50 and 10 nodes, respectively\n",
    "- Cross entropy loss\n",
    "- I decide other hyperparameters (ie: nonlinearity, #epochs, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_review_class = binary_embeddings_df['binary_review_class']\n",
    "# binary_review_class.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"Define the NN architecture\"\"\"\n",
    "\n",
    "    def __init__(self, num_h1_nodes: int, num_h2_nodes: int, d: int, num_output_classes: int, dropout_rate: float):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # self.height = heightght = height\n",
    "        # self.width = width\n",
    "        # linear layer (1200 x 300 dot 300 x 50 -> 1200 x 50)\n",
    "        self.fc1 = nn.Linear(d, num_h1_nodes)\n",
    "        # linear layer (1200 x 50 dot 50 x 10 -> 1200 x 10)\n",
    "        self.fc2 = nn.Linear(num_h1_nodes, num_h2_nodes)\n",
    "        # linear layer (1200 x 50 dot 50 x 10 -> 1200 x 10)\n",
    "        self.fc3 = nn.Linear(num_h2_nodes, num_output_classes) # change to 3 for ternery\n",
    "        # dropout to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # add hidden layer, with relu activation function, dropout, relu, dropout, output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def train_network(self, number_of_epochs: int, optimizer, criterion_function, train_loader):\n",
    "        # set initial \"min\" to infinity\n",
    "        valid_loss_min = np.Inf\n",
    "\n",
    "        for epoch in range(number_of_epochs):\n",
    "            train_loss = 0.0\n",
    "            \n",
    "\n",
    "            ###################\n",
    "            # train the model #\n",
    "            ###################\n",
    "            self.train() # prep model for training\n",
    "            for data, target in train_loader:\n",
    "                # clear the gradients of all optimized variables\n",
    "                optimizer.zero_grad()\n",
    "                # forward pass to compute predictions, loss, backward pass to compute gradient wrt model params\n",
    "                output = self(data)\n",
    "                loss = criterion_function(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # update running training loss\n",
    "                train_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            # print training statistics \n",
    "            # calculate average loss over an epoch\n",
    "            train_loss = train_loss / len(train_loader.dataset)\n",
    "            \n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "                epoch+1, \n",
    "                train_loss,\n",
    "                ))\n",
    "            \n",
    "            # # save model if validation loss has decreased\n",
    "            # if train_loss <= valid_loss_min:\n",
    "            #     print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            #     valid_loss_min,\n",
    "            #     train_loss))\n",
    "            #     torch.save(self.state_dict(), 'nn_model.pt')\n",
    "            #     valid_loss_min = train_loss\n",
    "\n",
    "    def predict(self, data_loader):\n",
    "        predictions = []\n",
    "        ground_truth = []\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(data_loader):\n",
    "            outputs = self(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            ground_truth.extend(targets.cpu().numpy())\n",
    "\n",
    "        # Convert predictions and ground truth lists to numpy arrays\n",
    "        predictions = np.array(predictions)\n",
    "        ground_truth = np.array(ground_truth)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = (predictions == ground_truth).mean()\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_binary_embeddings, d_binary_embeddings = my_binary_train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_h1_nodes = 50\n",
    "num_h2_nodes = 10\n",
    "num_output_classes = 2\n",
    "dropout_rate = 0.2\n",
    "\n",
    "net_model = Net(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)\n",
    "print(net_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binary_model(binary_train_embeddings, binary_train_y):\n",
    "    binary_train_y = binary_train_y.replace(2, 0)\n",
    "    data_tensor = torch.tensor(binary_train_embeddings, dtype=torch.float32)\n",
    "    target_tensor = torch.tensor(binary_train_y.values, dtype=torch.long)\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # how many samples per batch to load\n",
    "    batch_size = 64\n",
    "\n",
    "    # as a positive integer will turn on multi-process data loading with the specified number of loader worker processes; otherwise, single-process data loading\n",
    "    num_workers = 0\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    number_of_epochs = 20\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net_model.parameters(), lr=0.01)\n",
    "    output_of_model = net_model.train_network(number_of_epochs, optimizer, criterion, train_loader)\n",
    "    \n",
    "    return output_of_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1131731    1.0\n",
       "2239015    2.0\n",
       "2428027    1.0\n",
       "314988     1.0\n",
       "2333557    1.0\n",
       "          ... \n",
       "2435512    1.0\n",
       "985250     1.0\n",
       "476834     1.0\n",
       "1274011    1.0\n",
       "1475157    1.0\n",
       "Name: binary_review_class, Length: 160000, dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160000, 300), (160000,))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_binary_train_embeddings.shape, binary_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.741038\n",
      "Epoch: 2 \tTraining Loss: 0.674876\n",
      "Epoch: 3 \tTraining Loss: 0.565111\n",
      "Epoch: 4 \tTraining Loss: 0.497324\n",
      "Epoch: 5 \tTraining Loss: 0.470440\n",
      "Epoch: 6 \tTraining Loss: 0.455037\n",
      "Epoch: 7 \tTraining Loss: 0.444712\n",
      "Epoch: 8 \tTraining Loss: 0.438826\n",
      "Epoch: 9 \tTraining Loss: 0.433828\n",
      "Epoch: 10 \tTraining Loss: 0.429491\n",
      "Epoch: 11 \tTraining Loss: 0.428359\n",
      "Epoch: 12 \tTraining Loss: 0.425171\n",
      "Epoch: 13 \tTraining Loss: 0.422998\n",
      "Epoch: 14 \tTraining Loss: 0.421549\n",
      "Epoch: 15 \tTraining Loss: 0.419528\n",
      "Epoch: 16 \tTraining Loss: 0.418254\n",
      "Epoch: 17 \tTraining Loss: 0.416680\n",
      "Epoch: 18 \tTraining Loss: 0.415169\n",
      "Epoch: 19 \tTraining Loss: 0.413809\n",
      "Epoch: 20 \tTraining Loss: 0.413008\n"
     ]
    }
   ],
   "source": [
    "train_binary_model(pretrained_binary_train_embeddings, binary_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.445839\n",
      "Epoch: 2 \tTraining Loss: 0.372675\n",
      "Epoch: 3 \tTraining Loss: 0.363305\n",
      "Epoch: 4 \tTraining Loss: 0.358006\n",
      "Epoch: 5 \tTraining Loss: 0.354022\n",
      "Epoch: 6 \tTraining Loss: 0.350458\n",
      "Epoch: 7 \tTraining Loss: 0.347911\n",
      "Epoch: 8 \tTraining Loss: 0.346071\n",
      "Epoch: 9 \tTraining Loss: 0.343415\n",
      "Epoch: 10 \tTraining Loss: 0.341536\n",
      "Epoch: 11 \tTraining Loss: 0.339060\n",
      "Epoch: 12 \tTraining Loss: 0.337620\n",
      "Epoch: 13 \tTraining Loss: 0.336963\n",
      "Epoch: 14 \tTraining Loss: 0.334528\n",
      "Epoch: 15 \tTraining Loss: 0.333283\n",
      "Epoch: 16 \tTraining Loss: 0.332521\n",
      "Epoch: 17 \tTraining Loss: 0.329565\n",
      "Epoch: 18 \tTraining Loss: 0.329659\n",
      "Epoch: 19 \tTraining Loss: 0.328392\n",
      "Epoch: 20 \tTraining Loss: 0.328160\n"
     ]
    }
   ],
   "source": [
    "train_binary_model(my_binary_train_embeddings, binary_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 10), (160000,))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_concat_binary_test_embeddings.shape, binary_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_binary_model(pretrained_concat_binary_test_embeddings, binary_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_binary_model(binary_test_embeddings, binary_test_y):\n",
    "    binary_test_y = binary_test_y.replace(2, 0)\n",
    "    target_array = binary_test_y.values\n",
    "    data_tensor = torch.tensor(binary_test_embeddings, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    # Create a PyTorch tensor from the NumPy array\n",
    "    target_tensor = torch.tensor(target_array, dtype=torch.long)  # Assuming target is of type long/int\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # Define batch size\n",
    "    batch_size = 1\n",
    "\n",
    "    # Define number of DataLoader workers\n",
    "    num_workers = 0  # Set this to a positive integer to enable multi-process data loading\n",
    "\n",
    "    # Create DataLoader\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    # Assuming net_model is an instance of your model\n",
    "    predictions = net_model.predict(test_loader)\n",
    "    # predictions = np.array(predictions)\n",
    "    # predictions_flat = predictions.flatten()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.538925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.538925"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_binary_model(pretrained_binary_test_embeddings, binary_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ternary_embeddings, d_ternary_embeddings = my_ternary_train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_h1_nodes = 50\n",
    "num_h2_nodes = 10\n",
    "num_output_classes = 3\n",
    "dropout_rate = 0.2\n",
    "\n",
    "net_model = Net(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)\n",
    "print(net_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ternary_model(ternary_train_embeddings, ternary_train_y):\n",
    "    ternary_train_y = ternary_train_y.replace(2, 0)\n",
    "    ternary_train_y = ternary_train_y.replace(3, 2)\n",
    "    data_tensor = torch.tensor(ternary_train_embeddings, dtype=torch.float32)\n",
    "    target_tensor = torch.tensor(ternary_train_y.values, dtype=torch.long)\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # how many samples per batch to load\n",
    "    batch_size = 64\n",
    "\n",
    "    # as a positive integer will turn on multi-process data loading with the specified number of loader worker processes; otherwise, single-process data loading\n",
    "    num_workers = 0\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    number_of_epochs = 3\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net_model.parameters(), lr=0.01)\n",
    "    output_of_model = net_model.train_network(number_of_epochs, optimizer, criterion, train_loader)\n",
    "    \n",
    "    return output_of_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.053907\n",
      "Epoch: 2 \tTraining Loss: 1.014723\n",
      "Epoch: 3 \tTraining Loss: 0.913997\n"
     ]
    }
   ],
   "source": [
    "train_ternary_model(pretrained_ternary_train_embeddings, ternary_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ternary_model(ternary_test_embeddings, ternary_test_y):\n",
    "    ternary_test_y = ternary_test_y.replace(2, 0)\n",
    "    ternary_test_y = ternary_test_y.replace(3, 2)\n",
    "    target_array = ternary_test_y.values\n",
    "    data_tensor = torch.tensor(ternary_test_embeddings, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    # Create a PyTorch tensor from the NumPy array\n",
    "    target_tensor = torch.tensor(target_array, dtype=torch.long)  # Assuming target is of type long/int\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # Define batch size\n",
    "    batch_size = 1\n",
    "\n",
    "    # Define number of DataLoader workers\n",
    "    num_workers = 0  # Set this to a positive integer to enable multi-process data loading\n",
    "\n",
    "    # Create DataLoader\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    # Assuming net_model is an instance of your model\n",
    "    predictions = net_model.predict(test_loader)\n",
    "    # predictions = np.array(predictions)\n",
    "    # predictions_flat = predictions.flatten()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.61486"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ternary_model(pretrained_ternary_test_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CNN_Net(nn.Module):\n",
    "    def __init__(self, num_h1_nodes: int, num_h2_nodes: int, d: int, num_output_classes: int, dropout_rate: float):\n",
    "        super().__init__()\n",
    "        # self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(d, num_h1_nodes)\n",
    "        self.fc2 = nn.Linear(num_h1_nodes, num_h2_nodes)\n",
    "        self.fc3 = nn.Linear(num_h2_nodes, num_output_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.pool(F.relu(self.conv1(x)))\n",
    "        # x = self.pool(F.relu(self.conv2(x)))\n",
    "        # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def train_network(self, number_of_epochs: int, optimizer, criterion_function, train_loader):\n",
    "        for epoch in range(number_of_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, labels = data\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion_function(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                # print(running_loss)\n",
    "                if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.7f}')\n",
    "                    running_loss = 0.0\n",
    "\n",
    "        print('Finished Training')\n",
    "    \n",
    "    def cnn_predict(self, data_loader):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "        with torch.no_grad():\n",
    "            for data in data_loader:\n",
    "                images, labels = data\n",
    "                # calculate outputs by running images through the network\n",
    "                outputs = self(images)\n",
    "                # the class with the highest energy is what we choose as prediction\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_Net(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_h1_nodes = 50\n",
    "num_h2_nodes = 10\n",
    "num_output_classes = 2\n",
    "dropout_rate = 0.2\n",
    "\n",
    "cnn_net_model = CNN_Net(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)\n",
    "print(cnn_net_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn_net_model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binary_cnn_model(binary_train_embeddings, binary_train_y):\n",
    "    binary_train_y = binary_train_y.replace(2, 1)\n",
    "    data_tensor = torch.tensor(binary_train_embeddings, dtype=torch.float32)\n",
    "    target_tensor = torch.tensor(binary_train_y.values, dtype=torch.long)\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # how many samples per batch to load\n",
    "    batch_size = 64\n",
    "\n",
    "    # as a positive integer will turn on multi-process data loading with the specified number of loader worker processes; otherwise, single-process data loading\n",
    "    num_workers = 0\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    number_of_epochs = 20\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net_model.parameters(), lr=0.01)\n",
    "    output_of_model = cnn_net_model.train_network(number_of_epochs, optimizer, criterion, train_loader)\n",
    "    \n",
    "    return output_of_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.964\n",
      "[2,  2000] loss: 0.964\n",
      "[3,  2000] loss: 0.964\n",
      "[4,  2000] loss: 0.964\n",
      "[5,  2000] loss: 0.964\n",
      "[6,  2000] loss: 0.964\n",
      "[7,  2000] loss: 0.964\n",
      "[8,  2000] loss: 0.964\n",
      "[9,  2000] loss: 0.964\n",
      "[10,  2000] loss: 0.964\n",
      "[11,  2000] loss: 0.964\n",
      "[12,  2000] loss: 0.964\n",
      "[13,  2000] loss: 0.964\n",
      "[14,  2000] loss: 0.964\n",
      "[15,  2000] loss: 0.964\n",
      "[16,  2000] loss: 0.964\n",
      "[17,  2000] loss: 0.964\n",
      "[18,  2000] loss: 0.964\n",
      "[19,  2000] loss: 0.964\n",
      "[20,  2000] loss: 0.964\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_binary_cnn_model(pretrained_binary_train_embeddings, binary_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_binary_cnn_model(binary_test_embeddings, binary_test_y):\n",
    "    binary_test_y = binary_test_y.replace(2, 1)\n",
    "    target_array = binary_test_y.values\n",
    "    data_tensor = torch.tensor(binary_test_embeddings, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    # Create a PyTorch tensor from the NumPy array\n",
    "    target_tensor = torch.tensor(target_array, dtype=torch.long)  # Assuming target is of type long/int\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # Define batch size\n",
    "    batch_size = 1\n",
    "\n",
    "    # Define number of DataLoader workers\n",
    "    num_workers = 0  # Set this to a positive integer to enable multi-process data loading\n",
    "\n",
    "    # Create DataLoader\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    # Assuming net_model is an instance of your model\n",
    "    predictions = net_model.predict(test_loader)\n",
    "    # predictions = np.array(predictions)\n",
    "    # predictions_flat = predictions.flatten()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4989"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_binary_cnn_model(pretrained_binary_test_embeddings, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.949\n",
      "[2,  2000] loss: 0.949\n",
      "[3,  2000] loss: 0.949\n",
      "[4,  2000] loss: 0.949\n",
      "[5,  2000] loss: 0.949\n",
      "[6,  2000] loss: 0.949\n",
      "[7,  2000] loss: 0.949\n",
      "[8,  2000] loss: 0.949\n",
      "[9,  2000] loss: 0.949\n",
      "[10,  2000] loss: 0.949\n",
      "[11,  2000] loss: 0.949\n",
      "[12,  2000] loss: 0.949\n",
      "[13,  2000] loss: 0.949\n",
      "[14,  2000] loss: 0.949\n",
      "[15,  2000] loss: 0.949\n",
      "[16,  2000] loss: 0.949\n",
      "[17,  2000] loss: 0.949\n",
      "[18,  2000] loss: 0.949\n",
      "[19,  2000] loss: 0.949\n",
      "[20,  2000] loss: 0.949\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_binary_cnn_model(my_binary_train_embeddings, binary_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4195"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_binary_cnn_model(my_binary_train_embeddings, binary_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
