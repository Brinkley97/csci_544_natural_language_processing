{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2- Binary Classification \n",
    "- Detravious Jamari Brinkley\n",
    "- CSCI-544: Applied Natural Language Processing\n",
    "- python version: 3.11.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/brinkley97/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/brinkley97/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "2024-02-09 00:09:44,453 : INFO : loading projection weights from /Users/brinkley97/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
      "2024-02-09 00:10:15,341 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from /Users/brinkley97/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-02-09T00:10:15.341065', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'load_word2vec_format'}\n",
      "2024-02-09 00:10:15,343 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2024-02-09 00:10:15,344 : INFO : built Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...> from 9 documents (total 29 corpus positions)\n",
      "2024-02-09 00:10:15,344 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...> from 9 documents (total 29 corpus positions)\", 'datetime': '2024-02-09T00:10:15.344866', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import gensim.models\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import gensim.downloader as api\n",
    "pretrained_word_two_vec_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "1. Read data\n",
    "2. Keep reviews and ratings\n",
    "3. Create binary and ternary classes\n",
    "4. Clean data steps\n",
    "4. Clean data function\n",
    "5. Split data\n",
    "6. Load pretrained model and train my model\n",
    "    - Get similarity for the pretrained model\n",
    "    - Get similarity for my trained model\n",
    "9. Extract word embeddings\n",
    "    - Get word embeddings for pretrained model\n",
    "    - Get word embeddings for my model\n",
    "10. Simple models\n",
    "    - Get accuracy for perceptron on pretrained model\n",
    "    - Get accuracy for svm on pretrained model\n",
    "    - Get accuracy for perceptron on my model\n",
    "    - Get accuracy for svm on my model\n",
    "11. What do I conclude from comparing performances\n",
    "12. Feedforward Neural Network\n",
    "13. CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"../datasets/amazon_reviews_us_Office_Products_v1_00.tsv\"\n",
    "amazon_reviews_copy_df = pd.read_csv(dataset, sep='\\t', on_bad_lines='skip', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>4</td>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>4</td>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>4</td>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>5</td>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>5</td>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640254 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating                                        review_body\n",
       "0                 5                                     Great product.\n",
       "1                 5  What's to say about this commodity item except...\n",
       "2                 5    Haven't used yet, but I am sure I will like it.\n",
       "3                 1  Although this was labeled as &#34;new&#34; the...\n",
       "4                 4                    Gorgeous colors and easy to use\n",
       "...             ...                                                ...\n",
       "2640249           4  I can't live anymore whithout my Palm III. But...\n",
       "2640250           4  Although the Palm Pilot is thin and compact it...\n",
       "2640251           4  This book had a lot of great content without b...\n",
       "2640252           5  I am teaching a course in Excel and am using t...\n",
       "2640253           5  A very comprehensive layout of exactly how Vis...\n",
       "\n",
       "[2640254 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_ratings_df = amazon_reviews_copy_df.loc[0:, ['star_rating', 'review_body']]\n",
    "reviews_ratings_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_reviews(df: pd.DataFrame, review_col_name: str, number_of_reviews: int = 3):\n",
    "    \"\"\"Include reviews and ratings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    review_col_name: `str`\n",
    "        The specific_column to get the reviews and ratings of\n",
    "    \n",
    "    number_of_reviews: `int`\n",
    "        Number of samples to include\n",
    "\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    Nothing; instead, print the reviews with ratings\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    columns_to_include = [review_col_name, 'star_rating']\n",
    "\n",
    "    # Initialize an empty list to store dictionaries\n",
    "    list_of_dicts = []\n",
    "\n",
    "    # Iterate over the specified columns and retrieve the first three rows\n",
    "    for row in df[columns_to_include].head(3).to_dict(orient='records'):\n",
    "        list_of_dicts.append({'star_rating': row['star_rating'], review_col_name: row[review_col_name]})\n",
    "\n",
    "    for dictionary in list_of_dicts:\n",
    "        print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    " ## Create binary and ternary classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data_type(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Update the data type of the star ratings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with rating values\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the new sentiment appened\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    valid_ratings = ['1','2','3','4','5']\n",
    "    star_rating_series = df[col_name].copy()\n",
    "\n",
    "    # Convert type to strings\n",
    "    star_rating_series.astype('str')\n",
    "\n",
    "    # Check valid list and see which of our stars match\n",
    "    rows = star_rating_series.index\n",
    "    is_rating_in_valid_ratings = rows[star_rating_series.isin(valid_ratings)]\n",
    "\n",
    "    # Convert to list\n",
    "    is_rating_in_valid_ratings = is_rating_in_valid_ratings.to_list()\n",
    "\n",
    "    updated_df = df.iloc[is_rating_in_valid_ratings]\n",
    "    updated_df[col_name] = updated_df[col_name].astype(int)\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_1108/2907883124.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  updated_df[col_name] = updated_df[col_name].astype(int)\n"
     ]
    }
   ],
   "source": [
    "updated_reviews_ratings_df = update_data_type(reviews_ratings_df, 'star_rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>4</td>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>4</td>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>4</td>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>5</td>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>5</td>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640237 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "0                  5                                     Great product.\n",
       "1                  5  What's to say about this commodity item except...\n",
       "2                  5    Haven't used yet, but I am sure I will like it.\n",
       "3                  1  Although this was labeled as &#34;new&#34; the...\n",
       "4                  4                    Gorgeous colors and easy to use\n",
       "...              ...                                                ...\n",
       "2640249            4  I can't live anymore whithout my Palm III. But...\n",
       "2640250            4  Although the Palm Pilot is thin and compact it...\n",
       "2640251            4  This book had a lot of great content without b...\n",
       "2640252            5  I am teaching a course in Excel and am using t...\n",
       "2640253            5  A very comprehensive layout of exactly how Vis...\n",
       "\n",
       "[2640237 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_reviews_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>4</td>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>4</td>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>4</td>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>5</td>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>5</td>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640080 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "0                  5                                     Great product.\n",
       "1                  5  What's to say about this commodity item except...\n",
       "2                  5    Haven't used yet, but I am sure I will like it.\n",
       "3                  1  Although this was labeled as &#34;new&#34; the...\n",
       "4                  4                    Gorgeous colors and easy to use\n",
       "...              ...                                                ...\n",
       "2640249            4  I can't live anymore whithout my Palm III. But...\n",
       "2640250            4  Although the Palm Pilot is thin and compact it...\n",
       "2640251            4  This book had a lot of great content without b...\n",
       "2640252            5  I am teaching a course in Excel and am using t...\n",
       "2640253            5  A very comprehensive layout of exactly how Vis...\n",
       "\n",
       "[2640080 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_reviews_ratings_df = updated_reviews_ratings_df.dropna()\n",
    "updated_reviews_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         star_rating  review_body\n",
      "0              False        False\n",
      "1              False        False\n",
      "2              False        False\n",
      "3              False        False\n",
      "4              False        False\n",
      "...              ...          ...\n",
      "2640249        False        False\n",
      "2640250        False        False\n",
      "2640251        False        False\n",
      "2640252        False        False\n",
      "2640253        False        False\n",
      "\n",
      "[2640080 rows x 2 columns]\n",
      "There are no NaN values in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values\n",
    "nan_check = updated_reviews_ratings_df.isna()\n",
    "\n",
    "# Display the DataFrame with True where NaN values exist\n",
    "print(nan_check)\n",
    "\n",
    "# Check if any NaN value exists in the DataFrame\n",
    "if nan_check.any().any():\n",
    "    print(\"There are NaN values in the DataFrame.\")\n",
    "else:\n",
    "    print(\"There are no NaN values in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# reviews per rating star_rating\n",
      "5    1582704\n",
      "4     418348\n",
      "1     306967\n",
      "3     193680\n",
      "2     138381\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"# reviews per rating\", updated_reviews_ratings_df['star_rating'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_star_ratings(df: pd.DataFrame, col_name: str, star_value: int, number_of_reviews: int):\n",
    "    \"\"\"Build a subset balanced dataset with reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The dataframe to use\n",
    "    col_name: `str`\n",
    "        The name of the column to get reviews from\n",
    "    star_value: `int`\n",
    "        The star rating of the review\n",
    "    number_of_reviews: `int`\n",
    "        The number of sub reviews to include in sample\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    rating_df, sampled_rating_df: `tuple`\n",
    "        All reviews with that rating and the subset reviews with that rating\n",
    "    \"\"\"\n",
    "    \n",
    "    rating_df = df[df[col_name] == star_value]\n",
    "    sampled_rating_df = rating_df.sample(n=number_of_reviews)\n",
    "    return rating_df, sampled_rating_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_reviews = 50000\n",
    "subset_reviews = 200\n",
    "\n",
    "one_star = 1\n",
    "rating_one, rating_one_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', one_star, subset_reviews)\n",
    "two_stars = 2\n",
    "rating_two, rating_two_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', two_stars, subset_reviews)\n",
    "three_stars = 3\n",
    "rating_three, rating_three_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', three_stars, subset_reviews)\n",
    "four_stars = 4\n",
    "rating_four, rating_four_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', four_stars, subset_reviews)\n",
    "five_stars = 5\n",
    "rating_five, rating_five_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', five_stars, subset_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews_df = pd.concat([rating_one_sampled, rating_two_sampled, rating_three_sampled, rating_four_sampled, rating_five_sampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5833</th>\n",
       "      <td>1</td>\n",
       "      <td>I've had this for one semester in nursing scho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485162</th>\n",
       "      <td>1</td>\n",
       "      <td>Ordered this \\\\\"new printer\\\\\" on line with de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268037</th>\n",
       "      <td>1</td>\n",
       "      <td>After owning this printer for about 2 years no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199267</th>\n",
       "      <td>1</td>\n",
       "      <td>I have bought a total of 5 of these. Two of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014065</th>\n",
       "      <td>1</td>\n",
       "      <td>Much larger and bulkier than I expected. I see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152092</th>\n",
       "      <td>5</td>\n",
       "      <td>Very satisfied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171193</th>\n",
       "      <td>5</td>\n",
       "      <td>it writes well, but there is one thing i don't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101385</th>\n",
       "      <td>5</td>\n",
       "      <td>Cute flag!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547010</th>\n",
       "      <td>5</td>\n",
       "      <td>easy to install. good price. I don't know yet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214593</th>\n",
       "      <td>5</td>\n",
       "      <td>This works great for all the remotes we need f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "5833               1  I've had this for one semester in nursing scho...\n",
       "2485162            1  Ordered this \\\\\"new printer\\\\\" on line with de...\n",
       "2268037            1  After owning this printer for about 2 years no...\n",
       "2199267            1  I have bought a total of 5 of these. Two of th...\n",
       "1014065            1  Much larger and bulkier than I expected. I see...\n",
       "...              ...                                                ...\n",
       "1152092            5                                     Very satisfied\n",
       "1171193            5  it writes well, but there is one thing i don't...\n",
       "1101385            5                                         Cute flag!\n",
       "547010             5  easy to install. good price. I don't know yet ...\n",
       "1214593            5  This works great for all the remotes we need f...\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_reviews_by_rating(df: pd.DataFrame, rating_col: str, threshold: int, sentiment_type: str):\n",
    "    \"\"\"Categorizes reviews by adding a rating\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    rating_col: `str`\n",
    "        Column with rating values\n",
    "    \n",
    "    threshold: `int`\n",
    "        Where to split the ratings such that categories can be formed\n",
    "\n",
    "    sentiment_type: `str`\n",
    "        One of three types of sentiment: positive, negative, or neural\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the new sentiment appened\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if sentiment_type == 'positive_review_class':\n",
    "        positive_review_threshold = df[rating_col].astype('int32') > threshold\n",
    "        df = df[positive_review_threshold]\n",
    "        df[sentiment_type] = 1\n",
    "\n",
    "    elif sentiment_type == 'negative_review_class':\n",
    "        negative_review_threshold = df[rating_col].astype('int32') < threshold\n",
    "        df = df[negative_review_threshold]\n",
    "        df[sentiment_type] = 2\n",
    "\n",
    "    elif sentiment_type == 'neutral_review_class':\n",
    "        neutral_review_threshold = df[rating_col].astype('int32') == threshold\n",
    "        df = df[neutral_review_threshold]\n",
    "        df[sentiment_type] = 3\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_1108/1400069123.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[sentiment_type] = 2\n",
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_1108/1400069123.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[sentiment_type] = 3\n",
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_1108/1400069123.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[sentiment_type] = 1\n"
     ]
    }
   ],
   "source": [
    "negative_review_class_df = separate_reviews_by_rating(sampled_reviews_df, 'star_rating', 3, 'negative_review_class')\n",
    "neutral_review_class_df = separate_reviews_by_rating(sampled_reviews_df, 'star_rating', 3, 'neutral_review_class')\n",
    "positive_review_class_df = separate_reviews_by_rating(sampled_reviews_df, 'star_rating', 3, 'positive_review_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>negative_review_class</th>\n",
       "      <th>neutral_review_class</th>\n",
       "      <th>positive_review_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5833</th>\n",
       "      <td>1</td>\n",
       "      <td>I've had this for one semester in nursing scho...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485162</th>\n",
       "      <td>1</td>\n",
       "      <td>Ordered this \\\\\"new printer\\\\\" on line with de...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268037</th>\n",
       "      <td>1</td>\n",
       "      <td>After owning this printer for about 2 years no...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199267</th>\n",
       "      <td>1</td>\n",
       "      <td>I have bought a total of 5 of these. Two of th...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014065</th>\n",
       "      <td>1</td>\n",
       "      <td>Much larger and bulkier than I expected. I see...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152092</th>\n",
       "      <td>5</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171193</th>\n",
       "      <td>5</td>\n",
       "      <td>it writes well, but there is one thing i don't...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101385</th>\n",
       "      <td>5</td>\n",
       "      <td>Cute flag!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547010</th>\n",
       "      <td>5</td>\n",
       "      <td>easy to install. good price. I don't know yet ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214593</th>\n",
       "      <td>5</td>\n",
       "      <td>This works great for all the remotes we need f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body  \\\n",
       "5833               1  I've had this for one semester in nursing scho...   \n",
       "2485162            1  Ordered this \\\\\"new printer\\\\\" on line with de...   \n",
       "2268037            1  After owning this printer for about 2 years no...   \n",
       "2199267            1  I have bought a total of 5 of these. Two of th...   \n",
       "1014065            1  Much larger and bulkier than I expected. I see...   \n",
       "...              ...                                                ...   \n",
       "1152092            5                                     Very satisfied   \n",
       "1171193            5  it writes well, but there is one thing i don't...   \n",
       "1101385            5                                         Cute flag!   \n",
       "547010             5  easy to install. good price. I don't know yet ...   \n",
       "1214593            5  This works great for all the remotes we need f...   \n",
       "\n",
       "         negative_review_class  neutral_review_class  positive_review_class  \n",
       "5833                       2.0                   NaN                    NaN  \n",
       "2485162                    2.0                   NaN                    NaN  \n",
       "2268037                    2.0                   NaN                    NaN  \n",
       "2199267                    2.0                   NaN                    NaN  \n",
       "1014065                    2.0                   NaN                    NaN  \n",
       "...                        ...                   ...                    ...  \n",
       "1152092                    NaN                   NaN                    1.0  \n",
       "1171193                    NaN                   NaN                    1.0  \n",
       "1101385                    NaN                   NaN                    1.0  \n",
       "547010                     NaN                   NaN                    1.0  \n",
       "1214593                    NaN                   NaN                    1.0  \n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_reviews_ratings_df = pd.concat([negative_review_class_df, neutral_review_class_df, positive_review_class_df])\n",
    "sampled_reviews_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews_df = sampled_reviews_ratings_df['negative_review_class'].dropna()\n",
    "neutral_reviews_df = sampled_reviews_ratings_df['neutral_review_class'].dropna()\n",
    "positive_reviews_df = sampled_reviews_ratings_df['positive_review_class'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews_ratings_df['binary_review_class'] = pd.concat([negative_reviews_df, positive_reviews_df])\n",
    "sampled_reviews_ratings_df['ternary_review_class'] = pd.concat([negative_reviews_df, neutral_reviews_df, positive_reviews_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2., nan,  1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_reviews_ratings_df['binary_review_class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Clean data steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_reviews_to_lower_case(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Convert all reviews to lower case\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the lower cased reviews\n",
    "    \"\"\"\n",
    "    \n",
    "    lower_case_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "    \n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "        # print(text_reviews_idx, type(text_review), text_review)\n",
    "\n",
    "        # NOT all reviews are strings, thus all can't be converted to lower cased\n",
    "        if type(text_review) != str:\n",
    "            print(True, text_review)\n",
    "            converted_str = str(text_review)\n",
    "            lower_case_reviews.append(text_review)\n",
    "         \n",
    "        else:\n",
    "            update_text_review = text_review.lower()\n",
    "            lower_case_reviews.append(update_text_review)\n",
    "\n",
    "    updated_df['lower_cased'] = lower_case_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_lower_cased = convert_reviews_to_lower_case(sampled_reviews_ratings_df, 'review_body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_lower_cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"reviews_lower_cased:\")\n",
    "# generate_sample_reviews(reviews_lower_cased, 'lower_cased', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove HTML and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_and_urls(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Remove HTML and URLs from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the html_and_urls removed\n",
    "    \"\"\"\n",
    "    \n",
    "    # url_pattern = re.compile(r'https?://\\S+|www\\. \\S+')\n",
    "\n",
    "    cleaned_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "\n",
    "        if isinstance(text_review, str):\n",
    "            # Check and remove HTML tags\n",
    "            has_html = bool(re.search('<.*?>', text_review))\n",
    "            if has_html == True:\n",
    "                # print(\"Review\", text_reviews_idx, \"has HTML -- \", text_review)\n",
    "                pass\n",
    "\n",
    "            no_html_review = re.sub('<.*?>', ' ', text_review)\n",
    "            # print(\"Review\", text_reviews_idx, \"without HTML -- \", no_html_review)\n",
    "        \n",
    "            # Check and remove URLs\n",
    "            has_url = bool(re.search(r'http\\S+', no_html_review))\n",
    "            if has_url == True:\n",
    "                # print(\"Review\", text_reviews_idx, \"has URL --\", no_html_review)\n",
    "                pass\n",
    "\n",
    "            no_html_url_review = re.sub(r'http\\S+', '', no_html_review)\n",
    "            # print(\"Review\", text_reviews_idx, \"without HTML, URL -- \", no_html_url_review)\n",
    "            # print()\n",
    "            cleaned_reviews.append(no_html_url_review)\n",
    "        else:\n",
    "            # print(text_reviews_idx, text_review)\n",
    "            cleaned_reviews.append(text_review)\n",
    "            \n",
    "\n",
    "    updated_df['without_html_urls'] = cleaned_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_html_urls_df = remove_html_and_urls(reviews_lower_cased, 'lower_cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_html_urls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_html_urls:\")\n",
    "# generate_sample_reviews(no_html_urls_df, 'without_html_urls', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_contractions = {\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"i'm\": \"I am\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"that'll\": \"that will\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'd\": \"what would\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when'll\": \"when will\",\n",
    "    \"when'd\": \"when would\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where'll\": \"where will\",\n",
    "    \"where'd\": \"where would\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why'll\": \"why will\",\n",
    "    \"why'd\": \"why would\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how'd\": \"how would\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_and_replace_contractions(review):\n",
    "    \"\"\"Find the contractions to replace from a specific review\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    review: `str`\n",
    "        A specific review\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    non_contraction_review: `str`\n",
    "        The updated specific review with contractions expanded\n",
    "    \n",
    "    \"\"\"\n",
    "    if isinstance(review, str):\n",
    "        get_words = review.split()\n",
    "\n",
    "        store_non_contraction_words = []\n",
    "\n",
    "        for word in get_words:\n",
    "            if word in store_contractions:\n",
    "                non_contraction_form = store_contractions[word]\n",
    "                # print(word, \"-->\", non_contraction_form)\n",
    "\n",
    "                store_non_contraction_words.append(non_contraction_form)\n",
    "\n",
    "            else:\n",
    "                # print(word)\n",
    "                store_non_contraction_words.append(word)\n",
    "\n",
    "        non_contraction_review = ' '.join(store_non_contraction_words)\n",
    "        return non_contraction_review\n",
    "    else:\n",
    "        return review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contractions(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Remove contractions from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the extra spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    without_contractions_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "\n",
    "        # print(\"Review\", text_reviews_idx, \"with possible contraction(s) -- \", text_review)\n",
    "\n",
    "        without_contraction = locate_and_replace_contractions(text_review)\n",
    "\n",
    "        # print(\"Review\", text_reviews_idx, \"without contraction -- \", without_contraction)\n",
    "        # print()\n",
    "\n",
    "        without_contractions_reviews.append(without_contraction)\n",
    "\n",
    "    updated_df['without_contractions'] = without_contractions_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_contractions_df = remove_contractions(no_html_urls_df, 'without_html_urls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_contractions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_contractions:\")\n",
    "# generate_sample_reviews(no_contractions_df, 'without_contractions', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove Non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphabetical_characters(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Remove Non-alphabetical characters from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the non-alphabetical characters removed\n",
    "    \"\"\"\n",
    "\n",
    "    alphabetical_char_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "    # print(text_reviews)\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "        \n",
    "        if isinstance(text_review, str):\n",
    "\n",
    "            # Check for non-alphabetical characters\n",
    "            has_non_alphabetical_char = bool(re.search(r'[^a-zA-Z]', text_review))\n",
    "            if has_non_alphabetical_char == True:\n",
    "                # print(\"Review\", text_reviews_idx, \"has HTML -- \", text_review)\n",
    "                pass\n",
    "            \n",
    "            # Remove non-alphabetical characters\n",
    "            with_alphabetical_char = re.sub(r'[^a-zA-Z\\s]', ' ', text_review)\n",
    "            # print(\"Review\", text_reviews_idx, \"has HTML -- \", with_alphabetical_char)\n",
    "            alphabetical_char_reviews.append(with_alphabetical_char)\n",
    "        else:\n",
    "            alphabetical_char_reviews.append(text_review)\n",
    "\n",
    "    updated_df['with_alpha_chars_only'] = alphabetical_char_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only_alpha_chars_df = remove_non_alphabetical_characters(no_contractions_df, 'without_contractions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only_alpha_chars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"with_alpha_chars_only:\")\n",
    "# generate_sample_reviews(only_alpha_chars_df, 'with_alpha_chars_only', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Remove extra spaces from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the extra spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    single_spaced_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "    # print(text_reviews)\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "\n",
    "        if isinstance(text_review, str):\n",
    "        # Check if there are any extra spaces\n",
    "            has_extra_space = bool(re.search(r' +', text_review))\n",
    "            if has_extra_space == True:\n",
    "                # print(\"Review\", text_reviews_idx, \"has extra space -- \", text_review)\n",
    "                pass\n",
    "            \n",
    "            # Remove extra spaces\n",
    "            single_spaced_review = re.sub(r' +', ' ', text_review)\n",
    "            # print(\"Review\", text_reviews_idx, \"without extra space -- \", single_spaced_review)\n",
    "            # print()\n",
    "            \n",
    "            single_spaced_reviews.append(single_spaced_review)\n",
    "        else:\n",
    "            single_spaced_reviews.append(text_review)\n",
    "\n",
    "    updated_df['without_extra_space'] = single_spaced_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_extra_space_df = remove_extra_spaces(only_alpha_chars_df, 'with_alpha_chars_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_extra_space_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_extra_space:\")\n",
    "# generate_sample_reviews(no_extra_space_df, 'without_extra_space', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stop_words(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Filter stop words out from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the extra spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    without_stop_words_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "\n",
    "        if isinstance(text_review, str):\n",
    "            text_review_words = word_tokenize(text_review) \n",
    "\n",
    "        \n",
    "\n",
    "            # print(\"Before stop word removal\", text_reviews_idx, \" -- \", text_review)\n",
    "\n",
    "            filtered_review = []\n",
    "\n",
    "            for text_review_words_idx in range(len(text_review_words)):\n",
    "                text_review_word = text_review_words[text_review_words_idx]\n",
    "                \n",
    "                # Check if review word is a stop word\n",
    "                if text_review_word in stop_words:\n",
    "                    # print(\"  Stop word -- \", text_review_word)\n",
    "                    pass\n",
    "                else:\n",
    "                    # print(text_review_word, \" -- is NOT a stop word in review\")\n",
    "                    filtered_review.append(text_review_word)\n",
    "\n",
    "            \n",
    "            filtered_review = \" \".join(filtered_review)\n",
    "            # print(\"After stop word removal\", text_reviews_idx, \" -- \", filtered_review)\n",
    "            # print()\n",
    "            \n",
    "            without_stop_words_reviews.append(filtered_review)\n",
    "        else:\n",
    "            without_stop_words_reviews.append(text_review)\n",
    "        \n",
    "\n",
    "    updated_df['without_stop_words'] = without_stop_words_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_stop_words_df = filter_stop_words(no_extra_space_df, 'without_extra_space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_stop_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_stop_words:\")\n",
    "# generate_sample_reviews(no_stop_words_df, 'without_stop_words', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Perform lemmatization  \n",
    "\n",
    "- \"A sentence with many words\"\n",
    "    - \"words\" -> word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmentize_review(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Lemmentize all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the extra spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    lemmed_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "\n",
    "    lem = WordNetLemmatizer()\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]   \n",
    "        if isinstance(text_review, str):     \n",
    "            words_in_review = word_tokenize(text_review) \n",
    "\n",
    "            # print(\"Before lem update\", text_reviews_idx, \" -- \", text_review)\n",
    "            # print(\"Lemmed words\", words_in_review)\n",
    "            \n",
    "\n",
    "            lemmed_sentence = []\n",
    "\n",
    "            # Split review into words\n",
    "            for lemmed_words_idx in range(len(words_in_review)):\n",
    "                word = words_in_review[lemmed_words_idx]\n",
    "                \n",
    "                apply_lemmatization = lem.lemmatize(word)\n",
    "                # print(apply_lemmatization)\n",
    "                \n",
    "                lemmed_sentence.append(apply_lemmatization)\n",
    "                filtered_review = \" \".join(lemmed_sentence)\n",
    "        \n",
    "            # print(\"After lem update -- \", filtered_review)\n",
    "            # print()\n",
    "\n",
    "            lemmed_reviews.append(filtered_review)\n",
    "        else:\n",
    "            lemmed_reviews.append(text_review)\n",
    "\n",
    "    updated_df['lemmed_reviews'] = lemmed_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmed_df = lemmentize_review(no_stop_words_df, 'without_stop_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_unlemmed_words:\")\n",
    "# generate_sample_reviews(lemmed_df, 'lemmed_reviews', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Clean data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, col_name):\n",
    "    \"\"\"Perform lower case, remove HTML and URLs, remove contractions, remove non-alphabetical characters, remove extra spaces, remove stop words, and lemmatize\"\"\"\n",
    "\n",
    "    print(\"original reviews:\")\n",
    "    # generate_sample_reviews(df, col_name, 3)\n",
    "\n",
    "    reviews_lower_cased = convert_reviews_to_lower_case(df, col_name)\n",
    "    print(\"reviews_lower_cased:\")\n",
    "    # generate_sample_reviews(reviews_lower_cased, 'lower_cased', 3)\n",
    "\n",
    "    no_html_urls_df = remove_html_and_urls(reviews_lower_cased, 'lower_cased')\n",
    "    print(\"without_html_urls:\")\n",
    "    # generate_sample_reviews(no_html_urls_df, 'without_html_urls', 3)\n",
    "\n",
    "    no_contractions_df = remove_contractions(no_html_urls_df, 'without_html_urls')\n",
    "    print(\"without_contractions:\")\n",
    "    # generate_sample_reviews(no_contractions_df, 'without_contractions', 3)\n",
    "\n",
    "    only_alpha_chars_df = remove_non_alphabetical_characters(no_contractions_df, 'without_contractions')\n",
    "    print(\"with_alpha_chars_only:\")\n",
    "    # generate_sample_reviews(only_alpha_chars_df, 'with_alpha_chars_only', 3)\n",
    "\n",
    "    no_extra_space_df = remove_extra_spaces(only_alpha_chars_df, 'with_alpha_chars_only')\n",
    "    print(\"without_extra_space:\")\n",
    "    # generate_sample_reviews(no_extra_space_df, 'without_extra_space', 3)\n",
    "\n",
    "    no_stop_words_df = filter_stop_words(no_extra_space_df, 'without_extra_space')\n",
    "    print(\"without_stop_words:\")\n",
    "    # generate_sample_reviews(no_stop_words_df, 'without_stop_words', 3)\n",
    "    \n",
    "    lemmed_df = lemmentize_review(no_stop_words_df, 'without_stop_words')\n",
    "    print(\"without_unlemmed_words:\")\n",
    "    # generate_sample_reviews(lemmed_df, 'lemmed_reviews', 3)\n",
    "\n",
    "    return lemmed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original reviews:\n",
      "reviews_lower_cased:\n",
      "without_html_urls:\n",
      "without_contractions:\n",
      "with_alpha_chars_only:\n",
      "without_extra_space:\n",
      "without_stop_words:\n",
      "without_unlemmed_words:\n"
     ]
    }
   ],
   "source": [
    "cleaned_reviews_df = preprocess_data(sampled_reviews_ratings_df, 'review_body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>negative_review_class</th>\n",
       "      <th>neutral_review_class</th>\n",
       "      <th>positive_review_class</th>\n",
       "      <th>binary_review_class</th>\n",
       "      <th>ternary_review_class</th>\n",
       "      <th>lower_cased</th>\n",
       "      <th>without_html_urls</th>\n",
       "      <th>without_contractions</th>\n",
       "      <th>with_alpha_chars_only</th>\n",
       "      <th>without_extra_space</th>\n",
       "      <th>without_stop_words</th>\n",
       "      <th>lemmed_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5833</th>\n",
       "      <td>1</td>\n",
       "      <td>I've had this for one semester in nursing scho...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>i've had this for one semester in nursing scho...</td>\n",
       "      <td>i've had this for one semester in nursing scho...</td>\n",
       "      <td>I have had this for one semester in nursing sc...</td>\n",
       "      <td>I have had this for one semester in nursing sc...</td>\n",
       "      <td>I have had this for one semester in nursing sc...</td>\n",
       "      <td>I one semester nursing school paint around edg...</td>\n",
       "      <td>I one semester nursing school paint around edg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485162</th>\n",
       "      <td>1</td>\n",
       "      <td>Ordered this \\\\\"new printer\\\\\" on line with de...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ordered this \\\\\"new printer\\\\\" on line with de...</td>\n",
       "      <td>ordered this \\\\\"new printer\\\\\" on line with de...</td>\n",
       "      <td>ordered this \\\\\"new printer\\\\\" on line with de...</td>\n",
       "      <td>ordered this    new printer    on line with de...</td>\n",
       "      <td>ordered this new printer on line with descript...</td>\n",
       "      <td>ordered new printer line descriptin box hp col...</td>\n",
       "      <td>ordered new printer line descriptin box hp col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268037</th>\n",
       "      <td>1</td>\n",
       "      <td>After owning this printer for about 2 years no...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>after owning this printer for about 2 years no...</td>\n",
       "      <td>after owning this printer for about 2 years no...</td>\n",
       "      <td>after owning this printer for about 2 years no...</td>\n",
       "      <td>after owning this printer for about   years no...</td>\n",
       "      <td>after owning this printer for about years now ...</td>\n",
       "      <td>owning printer years looking replacing constan...</td>\n",
       "      <td>owning printer year looking replacing constant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199267</th>\n",
       "      <td>1</td>\n",
       "      <td>I have bought a total of 5 of these. Two of th...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>i have bought a total of 5 of these. two of th...</td>\n",
       "      <td>i have bought a total of 5 of these. two of th...</td>\n",
       "      <td>i have bought a total of 5 of these. two of th...</td>\n",
       "      <td>i have bought a total of   of these  two of th...</td>\n",
       "      <td>i have bought a total of of these two of these...</td>\n",
       "      <td>bought total two two different seller one yet ...</td>\n",
       "      <td>bought total two two different seller one yet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014065</th>\n",
       "      <td>1</td>\n",
       "      <td>Much larger and bulkier than I expected. I see...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>much larger and bulkier than i expected. i see...</td>\n",
       "      <td>much larger and bulkier than i expected. i see...</td>\n",
       "      <td>much larger and bulkier than i expected. i see...</td>\n",
       "      <td>much larger and bulkier than i expected  i see...</td>\n",
       "      <td>much larger and bulkier than i expected i seem...</td>\n",
       "      <td>much larger bulkier expected seem love buy ner...</td>\n",
       "      <td>much larger bulkier expected seem love buy ner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152092</th>\n",
       "      <td>5</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>very satisfied</td>\n",
       "      <td>very satisfied</td>\n",
       "      <td>very satisfied</td>\n",
       "      <td>very satisfied</td>\n",
       "      <td>very satisfied</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>satisfied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171193</th>\n",
       "      <td>5</td>\n",
       "      <td>it writes well, but there is one thing i don't...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>it writes well, but there is one thing i don't...</td>\n",
       "      <td>it writes well, but there is one thing i don't...</td>\n",
       "      <td>it writes well, but there is one thing i do no...</td>\n",
       "      <td>it writes well  but there is one thing i do no...</td>\n",
       "      <td>it writes well but there is one thing i do not...</td>\n",
       "      <td>writes well one thing like contain much ink ev...</td>\n",
       "      <td>writes well one thing like contain much ink ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101385</th>\n",
       "      <td>5</td>\n",
       "      <td>Cute flag!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cute flag!</td>\n",
       "      <td>cute flag!</td>\n",
       "      <td>cute flag!</td>\n",
       "      <td>cute flag</td>\n",
       "      <td>cute flag</td>\n",
       "      <td>cute flag</td>\n",
       "      <td>cute flag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547010</th>\n",
       "      <td>5</td>\n",
       "      <td>easy to install. good price. I don't know yet ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>easy to install. good price. i don't know yet ...</td>\n",
       "      <td>easy to install. good price. i don't know yet ...</td>\n",
       "      <td>easy to install. good price. i do not know yet...</td>\n",
       "      <td>easy to install  good price  i do not know yet...</td>\n",
       "      <td>easy to install good price i do not know yet a...</td>\n",
       "      <td>easy install good price know yet ink quality soon</td>\n",
       "      <td>easy install good price know yet ink quality soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214593</th>\n",
       "      <td>5</td>\n",
       "      <td>This works great for all the remotes we need f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>this works great for all the remotes we need f...</td>\n",
       "      <td>this works great for all the remotes we need f...</td>\n",
       "      <td>this works great for all the remotes we need f...</td>\n",
       "      <td>this works great for all the remotes we need f...</td>\n",
       "      <td>this works great for all the remotes we need f...</td>\n",
       "      <td>works great remotes need theater system bedroo...</td>\n",
       "      <td>work great remote need theater system bedroom ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body  \\\n",
       "5833               1  I've had this for one semester in nursing scho...   \n",
       "2485162            1  Ordered this \\\\\"new printer\\\\\" on line with de...   \n",
       "2268037            1  After owning this printer for about 2 years no...   \n",
       "2199267            1  I have bought a total of 5 of these. Two of th...   \n",
       "1014065            1  Much larger and bulkier than I expected. I see...   \n",
       "...              ...                                                ...   \n",
       "1152092            5                                     Very satisfied   \n",
       "1171193            5  it writes well, but there is one thing i don't...   \n",
       "1101385            5                                         Cute flag!   \n",
       "547010             5  easy to install. good price. I don't know yet ...   \n",
       "1214593            5  This works great for all the remotes we need f...   \n",
       "\n",
       "         negative_review_class  neutral_review_class  positive_review_class  \\\n",
       "5833                       2.0                   NaN                    NaN   \n",
       "2485162                    2.0                   NaN                    NaN   \n",
       "2268037                    2.0                   NaN                    NaN   \n",
       "2199267                    2.0                   NaN                    NaN   \n",
       "1014065                    2.0                   NaN                    NaN   \n",
       "...                        ...                   ...                    ...   \n",
       "1152092                    NaN                   NaN                    1.0   \n",
       "1171193                    NaN                   NaN                    1.0   \n",
       "1101385                    NaN                   NaN                    1.0   \n",
       "547010                     NaN                   NaN                    1.0   \n",
       "1214593                    NaN                   NaN                    1.0   \n",
       "\n",
       "         binary_review_class  ternary_review_class  \\\n",
       "5833                     2.0                   2.0   \n",
       "2485162                  2.0                   2.0   \n",
       "2268037                  2.0                   2.0   \n",
       "2199267                  2.0                   2.0   \n",
       "1014065                  2.0                   2.0   \n",
       "...                      ...                   ...   \n",
       "1152092                  1.0                   1.0   \n",
       "1171193                  1.0                   1.0   \n",
       "1101385                  1.0                   1.0   \n",
       "547010                   1.0                   1.0   \n",
       "1214593                  1.0                   1.0   \n",
       "\n",
       "                                               lower_cased  \\\n",
       "5833     i've had this for one semester in nursing scho...   \n",
       "2485162  ordered this \\\\\"new printer\\\\\" on line with de...   \n",
       "2268037  after owning this printer for about 2 years no...   \n",
       "2199267  i have bought a total of 5 of these. two of th...   \n",
       "1014065  much larger and bulkier than i expected. i see...   \n",
       "...                                                    ...   \n",
       "1152092                                     very satisfied   \n",
       "1171193  it writes well, but there is one thing i don't...   \n",
       "1101385                                         cute flag!   \n",
       "547010   easy to install. good price. i don't know yet ...   \n",
       "1214593  this works great for all the remotes we need f...   \n",
       "\n",
       "                                         without_html_urls  \\\n",
       "5833     i've had this for one semester in nursing scho...   \n",
       "2485162  ordered this \\\\\"new printer\\\\\" on line with de...   \n",
       "2268037  after owning this printer for about 2 years no...   \n",
       "2199267  i have bought a total of 5 of these. two of th...   \n",
       "1014065  much larger and bulkier than i expected. i see...   \n",
       "...                                                    ...   \n",
       "1152092                                     very satisfied   \n",
       "1171193  it writes well, but there is one thing i don't...   \n",
       "1101385                                         cute flag!   \n",
       "547010   easy to install. good price. i don't know yet ...   \n",
       "1214593  this works great for all the remotes we need f...   \n",
       "\n",
       "                                      without_contractions  \\\n",
       "5833     I have had this for one semester in nursing sc...   \n",
       "2485162  ordered this \\\\\"new printer\\\\\" on line with de...   \n",
       "2268037  after owning this printer for about 2 years no...   \n",
       "2199267  i have bought a total of 5 of these. two of th...   \n",
       "1014065  much larger and bulkier than i expected. i see...   \n",
       "...                                                    ...   \n",
       "1152092                                     very satisfied   \n",
       "1171193  it writes well, but there is one thing i do no...   \n",
       "1101385                                         cute flag!   \n",
       "547010   easy to install. good price. i do not know yet...   \n",
       "1214593  this works great for all the remotes we need f...   \n",
       "\n",
       "                                     with_alpha_chars_only  \\\n",
       "5833     I have had this for one semester in nursing sc...   \n",
       "2485162  ordered this    new printer    on line with de...   \n",
       "2268037  after owning this printer for about   years no...   \n",
       "2199267  i have bought a total of   of these  two of th...   \n",
       "1014065  much larger and bulkier than i expected  i see...   \n",
       "...                                                    ...   \n",
       "1152092                                     very satisfied   \n",
       "1171193  it writes well  but there is one thing i do no...   \n",
       "1101385                                         cute flag    \n",
       "547010   easy to install  good price  i do not know yet...   \n",
       "1214593  this works great for all the remotes we need f...   \n",
       "\n",
       "                                       without_extra_space  \\\n",
       "5833     I have had this for one semester in nursing sc...   \n",
       "2485162  ordered this new printer on line with descript...   \n",
       "2268037  after owning this printer for about years now ...   \n",
       "2199267  i have bought a total of of these two of these...   \n",
       "1014065  much larger and bulkier than i expected i seem...   \n",
       "...                                                    ...   \n",
       "1152092                                     very satisfied   \n",
       "1171193  it writes well but there is one thing i do not...   \n",
       "1101385                                         cute flag    \n",
       "547010   easy to install good price i do not know yet a...   \n",
       "1214593  this works great for all the remotes we need f...   \n",
       "\n",
       "                                        without_stop_words  \\\n",
       "5833     I one semester nursing school paint around edg...   \n",
       "2485162  ordered new printer line descriptin box hp col...   \n",
       "2268037  owning printer years looking replacing constan...   \n",
       "2199267  bought total two two different seller one yet ...   \n",
       "1014065  much larger bulkier expected seem love buy ner...   \n",
       "...                                                    ...   \n",
       "1152092                                          satisfied   \n",
       "1171193  writes well one thing like contain much ink ev...   \n",
       "1101385                                          cute flag   \n",
       "547010   easy install good price know yet ink quality soon   \n",
       "1214593  works great remotes need theater system bedroo...   \n",
       "\n",
       "                                            lemmed_reviews  \n",
       "5833     I one semester nursing school paint around edg...  \n",
       "2485162  ordered new printer line descriptin box hp col...  \n",
       "2268037  owning printer year looking replacing constant...  \n",
       "2199267  bought total two two different seller one yet ...  \n",
       "1014065  much larger bulkier expected seem love buy ner...  \n",
       "...                                                    ...  \n",
       "1152092                                          satisfied  \n",
       "1171193  writes well one thing like contain much ink ev...  \n",
       "1101385                                          cute flag  \n",
       "547010   easy install good price know yet ink quality soon  \n",
       "1214593  work great remote need theater system bedroom ...  \n",
       "\n",
       "[1000 rows x 14 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_reviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, review_class):\n",
    "    embeddings_df = df.dropna(subset=[review_class])\n",
    "    # print(len(embeddings_df), embeddings_df['star_rating'].unique())\n",
    "\n",
    "    specific_review_class = embeddings_df[review_class]\n",
    "    # print(specific_review_class.unique())\n",
    "\n",
    "    text = embeddings_df.loc[:, ['lemmed_reviews']]\n",
    "    # print(text)\n",
    "\n",
    "    ### Train test split so I can have the same train\n",
    "    X_train, X_test, y_train, y_test = train_test_split(text, specific_review_class, test_size=0.2, random_state=42)\n",
    "    # print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Binary\\n\")\n",
    "binary_X_train, binary_X_test, binary_y_train, binary_y_test = split_data(cleaned_reviews_df, 'binary_review_class')\n",
    "# print(\"\\nTernary\")\n",
    "ternary_X_train, ternary_X_test, ternary_y_train, ternary_y_test = split_data(cleaned_reviews_df, 'ternary_review_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model and train my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, col_name: str):\n",
    "        self.df = df\n",
    "        self.col_name = col_name\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: `pd.DataFrame`\n",
    "            The data\n",
    "        \n",
    "        col_name: `str`\n",
    "            Column with reviews\n",
    "\n",
    "        words_in_model: `list`\n",
    "            Words in Word2Vec model\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        text_reviews = self.df[self.col_name].values\n",
    "\n",
    "        for text_reviews_idx in range(len(text_reviews)):\n",
    "            text_review = text_reviews[text_reviews_idx]\n",
    "            # print(text_reviews_idx, \"--\", text_review)\n",
    "\n",
    "            yield utils.simple_preprocess(text_review)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 00:10:39,914 : INFO : collecting all words and their counts\n",
      "2024-02-09 00:10:39,916 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-02-09 00:10:39,947 : INFO : collected 3864 word types from a corpus of 22291 raw words and 640 sentences\n",
      "2024-02-09 00:10:39,948 : INFO : Creating a fresh vocabulary\n",
      "2024-02-09 00:10:39,950 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 475 unique words (12.29% of original 3864, drops 3389)', 'datetime': '2024-02-09T00:10:39.950718', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-09 00:10:39,951 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 14530 word corpus (65.18% of original 22291, drops 7761)', 'datetime': '2024-02-09T00:10:39.951374', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-09 00:10:39,953 : INFO : deleting the raw counts dictionary of 3864 items\n",
      "2024-02-09 00:10:39,954 : INFO : sample=0.001 downsamples 94 most-common words\n",
      "2024-02-09 00:10:39,954 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 11593.627069146458 word corpus (79.8%% of prior 14530)', 'datetime': '2024-02-09T00:10:39.954755', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-09 00:10:39,959 : INFO : estimated required memory for 475 words and 300 dimensions: 1377500 bytes\n",
      "2024-02-09 00:10:39,959 : INFO : resetting layer weights\n",
      "2024-02-09 00:10:39,961 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-09T00:10:39.961666', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2024-02-09 00:10:39,962 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 475 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=11 shrink_windows=True', 'datetime': '2024-02-09T00:10:39.962149', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-09 00:10:40,007 : INFO : EPOCH 0: training on 22291 raw words (11577 effective words) took 0.0s, 309318 effective words/s\n",
      "2024-02-09 00:10:40,047 : INFO : EPOCH 1: training on 22291 raw words (11651 effective words) took 0.0s, 349298 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Case\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 00:10:40,086 : INFO : EPOCH 2: training on 22291 raw words (11607 effective words) took 0.0s, 358547 effective words/s\n",
      "2024-02-09 00:10:40,128 : INFO : EPOCH 3: training on 22291 raw words (11596 effective words) took 0.0s, 348073 effective words/s\n",
      "2024-02-09 00:10:40,170 : INFO : EPOCH 4: training on 22291 raw words (11605 effective words) took 0.0s, 324641 effective words/s\n",
      "2024-02-09 00:10:40,171 : INFO : Word2Vec lifecycle event {'msg': 'training on 111455 raw words (58036 effective words) took 0.2s, 277928 effective words/s', 'datetime': '2024-02-09T00:10:40.171371', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-09 00:10:40,171 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=475, vector_size=300, alpha=0.025>', 'datetime': '2024-02-09T00:10:40.171752', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'created'}\n",
      "2024-02-09 00:10:40,172 : INFO : collecting all words and their counts\n",
      "2024-02-09 00:10:40,173 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-02-09 00:10:40,213 : INFO : collected 4369 word types from a corpus of 27127 raw words and 800 sentences\n",
      "2024-02-09 00:10:40,213 : INFO : Creating a fresh vocabulary\n",
      "2024-02-09 00:10:40,216 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 569 unique words (13.02% of original 4369, drops 3800)', 'datetime': '2024-02-09T00:10:40.216217', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-09 00:10:40,216 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 18375 word corpus (67.74% of original 27127, drops 8752)', 'datetime': '2024-02-09T00:10:40.216789', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-09 00:10:40,219 : INFO : deleting the raw counts dictionary of 4369 items\n",
      "2024-02-09 00:10:40,220 : INFO : sample=0.001 downsamples 85 most-common words\n",
      "2024-02-09 00:10:40,220 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 15089.253299687809 word corpus (82.1%% of prior 18375)', 'datetime': '2024-02-09T00:10:40.220422', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-09 00:10:40,224 : INFO : estimated required memory for 569 words and 300 dimensions: 1650100 bytes\n",
      "2024-02-09 00:10:40,225 : INFO : resetting layer weights\n",
      "2024-02-09 00:10:40,226 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-09T00:10:40.226915', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2024-02-09 00:10:40,227 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 569 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=11 shrink_windows=True', 'datetime': '2024-02-09T00:10:40.227369', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-09 00:10:40,319 : INFO : EPOCH 0: training on 27127 raw words (15075 effective words) took 0.1s, 222877 effective words/s\n",
      "2024-02-09 00:10:40,364 : INFO : EPOCH 1: training on 27127 raw words (15155 effective words) took 0.0s, 498136 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ternary Case\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 00:10:40,404 : INFO : EPOCH 2: training on 27127 raw words (15136 effective words) took 0.0s, 459738 effective words/s\n",
      "2024-02-09 00:10:40,449 : INFO : EPOCH 3: training on 27127 raw words (15076 effective words) took 0.0s, 393944 effective words/s\n",
      "2024-02-09 00:10:40,493 : INFO : EPOCH 4: training on 27127 raw words (15114 effective words) took 0.0s, 547279 effective words/s\n",
      "2024-02-09 00:10:40,493 : INFO : Word2Vec lifecycle event {'msg': 'training on 135635 raw words (75556 effective words) took 0.3s, 284298 effective words/s', 'datetime': '2024-02-09T00:10:40.493531', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-09 00:10:40,493 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=569, vector_size=300, alpha=0.025>', 'datetime': '2024-02-09T00:10:40.493946', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Binary Case\")\n",
    "binary_X_train_sentences = MyCorpus(binary_X_train, 'lemmed_reviews')\n",
    "my_binary_X_train_model = gensim.models.Word2Vec(sentences=binary_X_train_sentences, vector_size=300, window=11, min_count=10)\n",
    "\n",
    "# X test - get embeddings from my_binary_X_train_model -- vec_king = my_binary_X_train_model.wv['king']\n",
    "# binary_X_test_sentences = MyCorpus(binary_X_test, 'lemmed_reviews')\n",
    "# sentences = MyCorpus(sampled_reviews_ratings_df, 'review_body')\n",
    "# print(\"\\nSentences\", binary_X_test_sentences)\n",
    "\n",
    "print(\"\\nTernary Case\")\n",
    "ternary_X_train_sentences = MyCorpus(ternary_X_train, 'lemmed_reviews')\n",
    "my_ternary_X_train_model = gensim.models.Word2Vec(sentences=ternary_X_train_sentences, vector_size=300, window=11, min_count=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar scores\n",
    "\n",
    "- [x] Write summary of differences between their model and my model\n",
    "    - My model doesn't perform as well as the pretrained because the pretrained has trained on more data compared to my model. Thus, my model don't have sufficient training compared to the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118193507194519)]\n"
     ]
    }
   ],
   "source": [
    "result = pretrained_word_two_vec_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x10c709b10>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_trained_binary_X_train_model = my_binary_X_train_model.wv\n",
    "my_trained_binary_X_train_model\n",
    "\n",
    "my_trained_ternary_X_train_model = my_ternary_X_train_model.wv\n",
    "my_trained_ternary_X_train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix with proper exs\n",
    "# my_result = my_trained_binary_X_train_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)\n",
    "# print(my_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embeddings(df: pd.DataFrame, col_name: str, model_to_use):\n",
    "    \"\"\"Extract word embeddings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    model_to_use:\n",
    "        Either the pretrained model or my pretrained model\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    mean_sentences_vectorized\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_vectorized = []\n",
    "    mean_sentences_vectorized = []\n",
    "    sentences = df[col_name].values\n",
    "\n",
    "    for sentences_idx in range(len(sentences)):\n",
    "        vectorized_words = []\n",
    "        sentence = sentences[sentences_idx]\n",
    "        # print(\"Sentence\", sentences_idx)\n",
    "        # print(\"Sentence\", sentences_idx, \"Pre-vectorized -- \", sentence)\n",
    "        for word_idx, word in enumerate(sentence.split(\" \")):\n",
    "            if word in model_to_use.key_to_index:\n",
    "                vector_of_word = model_to_use[word]\n",
    "                vectorized_words.append(vector_of_word)\n",
    "            else:\n",
    "                vector_of_word = np.random.rand(model_to_use.vector_size)\n",
    "                vectorized_words.append(vector_of_word)\n",
    "\n",
    "        sentence_vectorized.append(vectorized_words)\n",
    "        # print(\"Sentence\", sentences_idx, \"Post-vectorized \\n\")\n",
    "        mean_of_sentence = np.mean(sentence_vectorized[sentences_idx], axis=0)\n",
    "        mean_sentences_vectorized.append(mean_of_sentence)\n",
    "    print(len(mean_sentences_vectorized))\n",
    "    return mean_sentences_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract embeddings for pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Pretrained Train\n",
      "640\n",
      "Binary Pretrained Test\n",
      "160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((640, 300), (160, 300))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Binary Pretrained Train\")\n",
    "pretrained_binary_train_embeddings = word_embeddings(binary_X_train, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_binary_train_embeddings = np.array(pretrained_binary_train_embeddings)\n",
    "\n",
    "print(\"Binary Pretrained Test\")\n",
    "pretrained_binary_test_embeddings = word_embeddings(binary_X_test, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_binary_test_embeddings = np.array(pretrained_binary_test_embeddings)\n",
    "\n",
    "pretrained_binary_train_embeddings.shape, pretrained_binary_test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ternary Pretrained Train\n",
      "800\n",
      "Ternary Pretrained Test\n",
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((800, 300), (200, 300))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Ternary Pretrained Train\")\n",
    "pretrained_ternary_train_embeddings = word_embeddings(ternary_X_train, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_ternary_train_embeddings = np.array(pretrained_ternary_train_embeddings)\n",
    "\n",
    "print(\"Ternary Pretrained Test\")\n",
    "pretrained_ternary_test_embeddings = word_embeddings(ternary_X_test, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_ternary_test_embeddings = np.array(pretrained_ternary_test_embeddings)\n",
    "\n",
    "pretrained_ternary_train_embeddings.shape, pretrained_ternary_test_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract embeddings for my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary My Model Train\n",
      "640\n",
      "Binary My Model Test\n",
      "160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((640, 300), (160, 300))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Binary My Model Train\")\n",
    "my_binary_train_embeddings = word_embeddings(binary_X_train, 'lemmed_reviews', my_trained_binary_X_train_model)\n",
    "my_binary_train_embeddings = np.array(my_binary_train_embeddings)\n",
    "\n",
    "print(\"Binary My Model Test\")\n",
    "my_binary_test_embeddings = word_embeddings(binary_X_test, 'lemmed_reviews', my_trained_binary_X_train_model)\n",
    "my_binary_test_embeddings = np.array(my_binary_test_embeddings)\n",
    "\n",
    "my_binary_train_embeddings.shape, my_binary_test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ternary My Model Train\n",
      "800\n",
      "Ternary My Model Test\n",
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((800, 300), (200, 300))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Ternary My Model Train\")\n",
    "my_ternary_train_embeddings = word_embeddings(ternary_X_train, 'lemmed_reviews', my_trained_ternary_X_train_model)\n",
    "my_ternary_train_embeddings = np.array(my_ternary_train_embeddings)\n",
    "\n",
    "print(\"Ternary My Model Test\")\n",
    "my_ternary_test_embeddings = word_embeddings(ternary_X_test, 'lemmed_reviews', my_trained_ternary_X_train_model)\n",
    "my_ternary_test_embeddings = np.array(my_ternary_test_embeddings)\n",
    "\n",
    "my_ternary_train_embeddings.shape, my_ternary_test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_word_embeddings(df: pd.DataFrame, col_name: str, model_to_use):\n",
    "    \"\"\"Extract word embeddings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    model_to_use:\n",
    "        Either the pretrained model or my pretrained model\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    concatenated_vectors: list\n",
    "        List of concatenated vectors for each review (first 10 Word2Vec vectors)\n",
    "    \"\"\"\n",
    "\n",
    "    mean_concatenated_vectors = []\n",
    "    sentences = df[col_name].values\n",
    "\n",
    "    for sentence in sentences:\n",
    "        vectorized_words = []\n",
    "        words = sentence.split(\" \")[:10]  # Select the first 10 words\n",
    "        for word in words:\n",
    "            if word in model_to_use.key_to_index:\n",
    "                vector_of_word = model_to_use[word]\n",
    "            else:\n",
    "                vector_of_word = np.random.rand(model_to_use.vector_size)\n",
    "            vectorized_words.append(vector_of_word)\n",
    "        \n",
    "        concatenated_features = np.concatenate(vectorized_words, axis=0)\n",
    "        # Ensure dimensionality of 300 for each sentence\n",
    "        if concatenated_features.shape[0] < 300:\n",
    "            # If concatenated_features has less than 300 dimensions, pad it with zeros\n",
    "            concatenated_features = np.pad(concatenated_features, ((0, 300 - concatenated_features.shape[0]), (0, 0)), mode='constant')\n",
    "        elif concatenated_features.shape[0] > 300:\n",
    "            # If concatenated_features has more than 300 dimensions, truncate it\n",
    "            concatenated_features = concatenated_features[:300 :]\n",
    "            \n",
    "        mean_concatenated_vectors.append(concatenated_features)\n",
    "\n",
    "    print(len(mean_concatenated_vectors))\n",
    "    return mean_concatenated_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Pretrained Train\n",
      "640\n",
      "Binary Pretrained Test\n",
      "160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((640, 300), (160, 300))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Binary Pretrained Train\")\n",
    "pretrained_binary_train_concat_embeddings = concat_word_embeddings(binary_X_train, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_binary_train_concat_embeddings = np.array(pretrained_binary_train_concat_embeddings)\n",
    "# pretrained_binary_train_concat_embeddings\n",
    "\n",
    "print(\"Binary Pretrained Test\")\n",
    "pretrained_binary_test_concat_embeddings = concat_word_embeddings(binary_X_test, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_binary_test_concat_embeddings = np.array(pretrained_binary_test_concat_embeddings)\n",
    "\n",
    "\n",
    "pretrained_binary_train_concat_embeddings.shape, pretrained_binary_test_concat_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary My Model Train\n",
      "640\n",
      "Binary My Model Test\n",
      "160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((640, 300), (160, 300))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Binary My Model Train\")\n",
    "my_binary_train_concat_embeddings = concat_word_embeddings(binary_X_train, 'lemmed_reviews', my_trained_ternary_X_train_model)\n",
    "my_binary_train_concat_embeddings = np.array(my_binary_train_concat_embeddings)\n",
    "\n",
    "print(\"Binary My Model Test\")\n",
    "my_binary_test_concat_embeddings = concat_word_embeddings(binary_X_test, 'lemmed_reviews', my_trained_ternary_X_train_model)\n",
    "my_binary_test_concat_embeddings = np.array(my_binary_test_concat_embeddings)\n",
    "\n",
    "my_binary_train_concat_embeddings.shape, my_binary_test_concat_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ternary Pretrained Train\n",
      "800\n",
      "Binary Pretrained Test\n",
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((800, 300), (200, 300))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Ternary Pretrained Train\")\n",
    "pretrained_ternary_train_concat_embeddings = concat_word_embeddings(ternary_X_train, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_ternary_train_concat_embeddings = np.array(pretrained_ternary_train_concat_embeddings)\n",
    "# pretrained_ternary_train_concat_embeddings\n",
    "\n",
    "print(\"Binary Pretrained Test\")\n",
    "pretrained_ternary_test_concat_embeddings = concat_word_embeddings(ternary_X_test, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_ternary_test_concat_embeddings = np.array(pretrained_ternary_test_concat_embeddings)\n",
    "\n",
    "\n",
    "pretrained_ternary_train_concat_embeddings.shape, pretrained_ternary_test_concat_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ternary My Model Train\n",
      "800\n",
      "Binary My Model Test\n",
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((800, 300), (200, 300))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Ternary My Model Train\")\n",
    "my_ternary_train_concat_embeddings = concat_word_embeddings(ternary_X_train, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "my_ternary_train_concat_embeddings = np.array(my_ternary_train_concat_embeddings)\n",
    "# my_ternary_train_concat_embeddings\n",
    "\n",
    "print(\"Binary My Model Test\")\n",
    "my_ternary_test_concat_embeddings = concat_word_embeddings(ternary_X_test, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "my_ternary_test_concat_embeddings = np.array(my_ternary_test_concat_embeddings)\n",
    "\n",
    "\n",
    "my_ternary_train_concat_embeddings.shape, my_ternary_test_concat_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_feature_extraction(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Extract the TF-IDF features from the reviews.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    tf_idf_features:\n",
    "        A matrix containing the TF-IDF features extracted\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tf_idf_features = vectorizer.fit_transform(df[col_name])\n",
    "\n",
    "    return tf_idf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews_df = cleaned_reviews_df.dropna(subset=['binary_review_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_features = tf_idf_feature_extraction(cleaned_reviews_df, 'lemmed_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x4215 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binnary_reviews = cleaned_reviews_df['binary_review_class']\n",
    "binnary_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test = train_test_split(tf_idf_features, binnary_reviews, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results(results_dict, w2v_type, method, classification, accuracy):\n",
    "    \"\"\"\n",
    "    Store results in a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    results_dict (dict): Dictionary to store the results.\n",
    "    w2v_type (str): Type of word2vec.\n",
    "    method (str): Method used.\n",
    "    classification (str): Type of classification.\n",
    "    accuracy (float): Accuracy of the classification.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing the stored results.\n",
    "    \"\"\"\n",
    "    results_dict['W2V Type'].append(w2v_type)\n",
    "    results_dict['Method'].append(method)\n",
    "    results_dict['Classification'].append(classification)\n",
    "    results_dict['Accuracy'].append(accuracy)\n",
    "\n",
    "    return pd.DataFrame(results_dict)  # Return DataFrame with a single row\n",
    "\n",
    "results_dict = {'W2V Type': [], 'Method': [], 'Classification': [], 'Accuracy': []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(y_true, y_prediction):\n",
    "    return sklearn.metrics.accuracy_score(y_true, y_prediction)\n",
    "\n",
    "# def eval_precision(y_true, y_prediction):\n",
    "#     return sklearn.metrics.precision_score(y_true, y_prediction)\n",
    "\n",
    "# def eval_recall(y_true, y_prediction):\n",
    "#     return sklearn.metrics.recall_score(y_true, y_prediction)\n",
    "\n",
    "# def eval_f1_score(y_true, y_prediction):\n",
    "#     return sklearn.metrics.f1_score(y_true, y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_metric(y_train_true, y_train_predictions):\n",
    "    accuracy = eval_accuracy(y_train_true, y_train_predictions)\n",
    "    # precision = eval_precision(y_train_true, y_train_predictions)\n",
    "    # recall = eval_recall(y_train_true, y_train_predictions)\n",
    "    # f1 = eval_f1_score(y_train_true, y_train_predictions)\n",
    "\n",
    "    metrics_dict = {\n",
    "        'Accuracy': accuracy,\n",
    "        # 'Precision': precision,\n",
    "        # 'Recall': recall,\n",
    "        # 'F1 Score': f1\n",
    "    }\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "def test_eval_metric(y_test_true, y_test_predictions):\n",
    "    accuracy = eval_accuracy(y_test_true, y_test_predictions)\n",
    "    # precision = eval_precision(y_test_true, y_test_predictions)\n",
    "    # recall = eval_recall(y_test_true, y_test_predictions)\n",
    "    # f1 = eval_f1_score(y_test_true, y_test_predictions)\n",
    "\n",
    "    metrics_dict = {\n",
    "        'Accuracy': accuracy,\n",
    "        # 'Precision': precision,\n",
    "        # 'Recall': recall,\n",
    "        # 'F1 Score': f1\n",
    "    }\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for perceptron on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_model(X_train, X_test, y_train, y_test): \n",
    "\n",
    "    technique = Perceptron(tol=1e-3, random_state=0)\n",
    "    technique.fit(X_train, y_train)\n",
    "    y_train_predictions = technique.predict(X_train)\n",
    "    y_test_predictions = technique.predict(X_test)\n",
    "\n",
    "\n",
    "    train_metrics = train_eval_metric(y_train, y_train_predictions)\n",
    "    test_metrics = test_eval_metric(y_test, y_test_predictions)\n",
    "\n",
    "    return train_metrics, test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_perceptron_train_metrics, tfidf_perceptron_test_metrics = perceptron_model(tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 1.0}, {'Accuracy': 0.75625})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_perceptron_train_metrics, tfidf_perceptron_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75625"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tfidf_perceptron_test_metrics.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       W2V Type          Method Classification  Accuracy\n",
      "0  TF-IDF-train  Perceptron-avg         Binary   1.00000\n",
      "1   TF-IDF-test  Perceptron-avg         Binary   0.75625\n"
     ]
    }
   ],
   "source": [
    "# Update the dictionary with new results\n",
    "\n",
    "results_df = store_results(results_dict, 'TF-IDF-train', 'Perceptron-avg', 'Binary', list(tfidf_perceptron_train_metrics.values())[0])\n",
    "results_df = store_results(results_dict, 'TF-IDF-test', 'Perceptron-avg', 'Binary', list(tfidf_perceptron_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained model\n",
    "perceptron_train_metrics, perceptron_test_metrics = perceptron_model(pretrained_binary_train_embeddings, pretrained_binary_test_embeddings, binary_y_train, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.8796875}, {'Accuracy': 0.75})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_train_metrics, perceptron_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1       TF-IDF-test  Perceptron-avg         Binary  0.756250\n",
      "2  Pretrained-train  Perceptron-avg         Binary  0.879687\n",
      "3   Pretrained-test  Perceptron-avg         Binary  0.750000\n"
     ]
    }
   ],
   "source": [
    "# Update the dictionary with new results\n",
    "results_df = store_results(results_dict, 'Pretrained-train', 'Perceptron-avg', 'Binary', list(perceptron_train_metrics.values())[0])\n",
    "results_df = store_results(results_dict, 'Pretrained-test', 'Perceptron-avg', 'Binary', list(perceptron_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for perceptron on my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((640, 300), (160, 300), (640,), (160,))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_binary_train_embeddings.shape, my_binary_test_embeddings.shape, binary_y_train.shape, binary_y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My model\n",
    "my_perceptron_train_metrics, my_perceptron_test_metrics = perceptron_model(my_binary_train_embeddings, my_binary_test_embeddings, binary_y_train, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.5484375}, {'Accuracy': 0.48125})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_perceptron_train_metrics, my_perceptron_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1       TF-IDF-test  Perceptron-avg         Binary  0.756250\n",
      "2  Pretrained-train  Perceptron-avg         Binary  0.879687\n",
      "3   Pretrained-test  Perceptron-avg         Binary  0.750000\n",
      "4    My model-train  Perceptron-avg         Binary  0.548438\n",
      "5     My model-test  Perceptron-avg         Binary  0.481250\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'My model-train', 'Perceptron-avg', 'Binary', list(my_perceptron_train_metrics.values())[0])\n",
    "results_df = store_results(results_dict, 'My model-test', 'Perceptron-avg', 'Binary', list(my_perceptron_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_model(X_train, X_test, y_train, y_test): \n",
    "\n",
    "    technique = LinearSVC(tol=1e-3, random_state=0)\n",
    "    technique.fit(X_train, y_train)\n",
    "    y_train_predictions = technique.predict(X_train)\n",
    "    y_test_predictions = technique.predict(X_test)\n",
    "\n",
    "\n",
    "    train_metrics = train_eval_metric(y_train, y_train_predictions)\n",
    "    test_metrics = test_eval_metric(y_test, y_test_predictions)\n",
    "\n",
    "    return train_metrics, test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tfidf_svm_train_metrics, tfidf_svm_test_metrics = svm_model(tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.9984375}, {'Accuracy': 0.78125})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_svm_train_metrics, tfidf_svm_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1       TF-IDF-test  Perceptron-avg         Binary  0.756250\n",
      "2  Pretrained-train  Perceptron-avg         Binary  0.879687\n",
      "3   Pretrained-test  Perceptron-avg         Binary  0.750000\n",
      "4    My model-train  Perceptron-avg         Binary  0.548438\n",
      "5     My model-test  Perceptron-avg         Binary  0.481250\n",
      "6      TF-IDF-train         SVM-avg         Binary  0.998437\n",
      "7       TF-IDF-test         SVM-avg         Binary  0.781250\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'TF-IDF-train', 'SVM-avg', 'Binary', list(tfidf_svm_train_metrics.values())[0])\n",
    "results_df = store_results(results_dict, 'TF-IDF-test', 'SVM-avg', 'Binary', list(tfidf_svm_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for svm on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svm_train_metrics, svm_test_metrics = svm_model(pretrained_binary_train_embeddings, pretrained_binary_test_embeddings, binary_y_train, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.89375}, {'Accuracy': 0.825})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_train_metrics, svm_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1       TF-IDF-test  Perceptron-avg         Binary  0.756250\n",
      "2  Pretrained-train  Perceptron-avg         Binary  0.879687\n",
      "3   Pretrained-test  Perceptron-avg         Binary  0.750000\n",
      "4    My model-train  Perceptron-avg         Binary  0.548438\n",
      "5     My model-test  Perceptron-avg         Binary  0.481250\n",
      "6      TF-IDF-train         SVM-avg         Binary  0.998437\n",
      "7       TF-IDF-test         SVM-avg         Binary  0.781250\n",
      "8  Pretrained-train         SVM-avg         Binary  0.893750\n",
      "9   Pretrained-test         SVM-avg         Binary  0.825000\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained-train', 'SVM-avg', 'Binary', list(svm_train_metrics.values())[0])\n",
    "results_df = store_results(results_dict, 'Pretrained-test', 'SVM-avg', 'Binary', list(svm_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for svm on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "my_svm_train_metrics, my_svm_test_metrics = svm_model(my_binary_train_embeddings, my_binary_test_embeddings, binary_y_train, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.8296875}, {'Accuracy': 0.41875})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_svm_train_metrics, my_svm_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1        TF-IDF-test  Perceptron-avg         Binary  0.756250\n",
      "2   Pretrained-train  Perceptron-avg         Binary  0.879687\n",
      "3    Pretrained-test  Perceptron-avg         Binary  0.750000\n",
      "4     My model-train  Perceptron-avg         Binary  0.548438\n",
      "5      My model-test  Perceptron-avg         Binary  0.481250\n",
      "6       TF-IDF-train         SVM-avg         Binary  0.998437\n",
      "7        TF-IDF-test         SVM-avg         Binary  0.781250\n",
      "8   Pretrained-train         SVM-avg         Binary  0.893750\n",
      "9    Pretrained-test         SVM-avg         Binary  0.825000\n",
      "10    My model-train         SVM-avg         Binary  0.829688\n",
      "11     My model-test         SVM-avg         Binary  0.418750\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'My model-train', 'SVM-avg', 'Binary', list(my_svm_train_metrics.values())[0])\n",
    "results_df = store_results(results_dict, 'My model-test', 'SVM-avg', 'Binary', list(my_svm_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do I conclude from comparing performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network\n",
    "- MLP for sentiment analysis classification\n",
    "- 2 hidden layers each with 50 and 10 nodes, respectively\n",
    "- Cross entropy loss\n",
    "- I decide other hyperparameters (ie: nonlinearity, #epochs, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_review_class = binary_embeddings_df['binary_review_class']\n",
    "# binary_review_class.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"Define the NN architecture\"\"\"\n",
    "\n",
    "    def __init__(self, num_h1_nodes: int, num_h2_nodes: int, d: int, num_output_classes: int, dropout_rate: float):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # self.height = heightght = height\n",
    "        # self.width = width\n",
    "        # linear layer (1200 x 300 dot 300 x 50 -> 1200 x 50)\n",
    "        self.fc1 = nn.Linear(d, num_h1_nodes)\n",
    "        # linear layer (1200 x 50 dot 50 x 10 -> 1200 x 10)\n",
    "        self.fc2 = nn.Linear(num_h1_nodes, num_h2_nodes)\n",
    "        # linear layer (1200 x 50 dot 50 x 10 -> 1200 x 10)\n",
    "        self.fc3 = nn.Linear(num_h2_nodes, num_output_classes) # change to 3 for ternery\n",
    "        # dropout to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # x = torch.cat((x, concatenated_features), dim=1)\n",
    "        \n",
    "        # add hidden layer, with relu activation function, dropout, relu, dropout, output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def train_network(self, number_of_epochs: int, optimizer, criterion_function, train_loader):\n",
    "        # set initial \"min\" to infinity\n",
    "        valid_loss_min = np.Inf\n",
    "\n",
    "        for epoch in range(number_of_epochs):\n",
    "            train_loss = 0.0\n",
    "            \n",
    "\n",
    "            ###################\n",
    "            # train the model #\n",
    "            ###################\n",
    "            self.train() # prep model for training\n",
    "            for data, target in train_loader:\n",
    "                # clear the gradients of all optimized variables\n",
    "                optimizer.zero_grad()\n",
    "                # forward pass to compute predictions, loss, backward pass to compute gradient wrt model params\n",
    "                output = self(data)\n",
    "                loss = criterion_function(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # update running training loss\n",
    "                train_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            # print training statistics \n",
    "            # calculate average loss over an epoch\n",
    "            train_loss = train_loss / len(train_loader.dataset)\n",
    "            \n",
    "            # print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            #     epoch+1, \n",
    "            #     train_loss,\n",
    "            #     ))\n",
    "            \n",
    "            # # save model if validation loss has decreased\n",
    "            # if train_loss <= valid_loss_min:\n",
    "            #     print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            #     valid_loss_min,\n",
    "            #     train_loss))\n",
    "            #     torch.save(self.state_dict(), 'nn_model.pt')\n",
    "            #     valid_loss_min = train_loss\n",
    "\n",
    "    def predict(self, data_loader):\n",
    "        predictions = []\n",
    "        ground_truth = []\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(data_loader):\n",
    "            outputs = self(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            ground_truth.extend(targets.cpu().numpy())\n",
    "\n",
    "        # Convert predictions and ground truth lists to numpy arrays\n",
    "        predictions = np.array(predictions)\n",
    "        ground_truth = np.array(ground_truth)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = (predictions == ground_truth).mean()\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640 300\n"
     ]
    }
   ],
   "source": [
    "N_binary_embeddings, d_binary_embeddings = my_binary_train_embeddings.shape\n",
    "print(N_binary_embeddings, d_binary_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_h1_nodes = 50\n",
    "num_h2_nodes = 10\n",
    "num_output_classes = 2\n",
    "dropout_rate = 0.2\n",
    "\n",
    "net_model = Net(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)\n",
    "print(net_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binary_model(binary_train_embeddings, binary_train_y):\n",
    "    binary_train_y = binary_train_y.replace(2, 0)\n",
    "    data_tensor = torch.tensor(binary_train_embeddings, dtype=torch.float32)\n",
    "    target_tensor = torch.tensor(binary_train_y.values, dtype=torch.long)\n",
    "    print(data_tensor.size(), target_tensor.size())\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "    \n",
    "    print(len(dataset))\n",
    "\n",
    "    # how many samples per batch to load\n",
    "    batch_size = 64\n",
    "\n",
    "    # as a positive integer will turn on multi-process data loading with the specified number of loader worker processes; otherwise, single-process data loading\n",
    "    num_workers = 0\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    number_of_epochs = 25\n",
    "    # number_of_epochs = 50\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net_model.parameters(), lr=0.01)\n",
    "    output_of_model = net_model.train_network(number_of_epochs, optimizer, criterion, train_loader)\n",
    "    \n",
    "    return output_of_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_binary_model(binary_test_embeddings, binary_test_y):\n",
    "    binary_test_y = binary_test_y.replace(2, 0)\n",
    "    target_array = binary_test_y.values\n",
    "    data_tensor = torch.tensor(binary_test_embeddings, dtype=torch.float32)\n",
    "    # print(len(data_tensor))\n",
    "\n",
    "    # Create a PyTorch tensor from the NumPy array\n",
    "    target_tensor = torch.tensor(target_array, dtype=torch.long)  # Assuming target is of type long/int\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # Define batch size\n",
    "    batch_size = 1\n",
    "\n",
    "    # Define number of DataLoader workers\n",
    "    num_workers = 0  # Set this to a positive integer to enable multi-process data loading\n",
    "\n",
    "    # Create DataLoader\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    # Assuming net_model is an instance of your model\n",
    "    predictions = net_model.predict(test_loader)\n",
    "    # predictions = np.array(predictions)\n",
    "    # predictions_flat = predictions.flatten()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([640, 300]) torch.Size([640])\n",
      "640\n",
      "Accuracy: 0.55625\n"
     ]
    }
   ],
   "source": [
    "train_binary_model(pretrained_binary_train_embeddings, binary_y_train)\n",
    "acc_avg_pretrained_binary = predict_binary_model(pretrained_binary_test_embeddings, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1        TF-IDF-test  Perceptron-avg         Binary  0.756250\n",
      "2   Pretrained-train  Perceptron-avg         Binary  0.879687\n",
      "3    Pretrained-test  Perceptron-avg         Binary  0.750000\n",
      "4     My model-train  Perceptron-avg         Binary  0.548438\n",
      "5      My model-test  Perceptron-avg         Binary  0.481250\n",
      "6       TF-IDF-train         SVM-avg         Binary  0.998437\n",
      "7        TF-IDF-test         SVM-avg         Binary  0.781250\n",
      "8   Pretrained-train         SVM-avg         Binary  0.893750\n",
      "9    Pretrained-test         SVM-avg         Binary  0.825000\n",
      "10    My model-train         SVM-avg         Binary  0.829688\n",
      "11     My model-test         SVM-avg         Binary  0.418750\n",
      "12        Pretrained         FFN-avg         Binary  0.556250\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained', 'FFN-avg', 'Binary', acc_avg_pretrained_binary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([640, 300]) torch.Size([640])\n",
      "640\n",
      "Accuracy: 0.44375\n",
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1        TF-IDF-test  Perceptron-avg         Binary  0.756250\n",
      "2   Pretrained-train  Perceptron-avg         Binary  0.879687\n",
      "3    Pretrained-test  Perceptron-avg         Binary  0.750000\n",
      "4     My model-train  Perceptron-avg         Binary  0.548438\n",
      "5      My model-test  Perceptron-avg         Binary  0.481250\n",
      "6       TF-IDF-train         SVM-avg         Binary  0.998437\n",
      "7        TF-IDF-test         SVM-avg         Binary  0.781250\n",
      "8   Pretrained-train         SVM-avg         Binary  0.893750\n",
      "9    Pretrained-test         SVM-avg         Binary  0.825000\n",
      "10    My model-train         SVM-avg         Binary  0.829688\n",
      "11     My model-test         SVM-avg         Binary  0.418750\n",
      "12        Pretrained         FFN-avg         Binary  0.556250\n",
      "13          My model         FFN-avg         Binary  0.443750\n"
     ]
    }
   ],
   "source": [
    "train_binary_model(my_binary_train_embeddings, binary_y_train)\n",
    "acc_avg_my_binary = predict_binary_model(my_binary_test_embeddings, binary_y_test)\n",
    "results_df = store_results(results_dict, 'My model', 'FFN-avg', 'Binary', acc_avg_my_binary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([640, 300]) torch.Size([640])\n",
      "640\n",
      "Accuracy: 0.53125\n"
     ]
    }
   ],
   "source": [
    "train_binary_model(pretrained_binary_train_concat_embeddings, binary_y_train)\n",
    "acc_concat_pretrained_binary = predict_binary_model(pretrained_binary_test_concat_embeddings, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1        TF-IDF-test  Perceptron-avg         Binary  0.756250\n",
      "2   Pretrained-train  Perceptron-avg         Binary  0.879687\n",
      "3    Pretrained-test  Perceptron-avg         Binary  0.750000\n",
      "4     My model-train  Perceptron-avg         Binary  0.548438\n",
      "5      My model-test  Perceptron-avg         Binary  0.481250\n",
      "6       TF-IDF-train         SVM-avg         Binary  0.998437\n",
      "7        TF-IDF-test         SVM-avg         Binary  0.781250\n",
      "8   Pretrained-train         SVM-avg         Binary  0.893750\n",
      "9    Pretrained-test         SVM-avg         Binary  0.825000\n",
      "10    My model-train         SVM-avg         Binary  0.829688\n",
      "11     My model-test         SVM-avg         Binary  0.418750\n",
      "12        Pretrained         FFN-avg         Binary  0.556250\n",
      "13          My model         FFN-avg         Binary  0.443750\n",
      "14        Pretrained      FFN-concat         Binary  0.531250\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained', 'FFN-concat', 'Binary', acc_concat_pretrained_binary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([640, 300]) torch.Size([640])\n",
      "640\n",
      "Accuracy: 0.51875\n"
     ]
    }
   ],
   "source": [
    "train_binary_model(my_binary_train_concat_embeddings, binary_y_train)\n",
    "acc_concat_my_binary = predict_binary_model(my_binary_test_concat_embeddings, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1        TF-IDF-test  Perceptron-avg         Binary  0.756250\n",
      "2   Pretrained-train  Perceptron-avg         Binary  0.879687\n",
      "3    Pretrained-test  Perceptron-avg         Binary  0.750000\n",
      "4     My model-train  Perceptron-avg         Binary  0.548438\n",
      "5      My model-test  Perceptron-avg         Binary  0.481250\n",
      "6       TF-IDF-train         SVM-avg         Binary  0.998437\n",
      "7        TF-IDF-test         SVM-avg         Binary  0.781250\n",
      "8   Pretrained-train         SVM-avg         Binary  0.893750\n",
      "9    Pretrained-test         SVM-avg         Binary  0.825000\n",
      "10    My model-train         SVM-avg         Binary  0.829688\n",
      "11     My model-test         SVM-avg         Binary  0.418750\n",
      "12        Pretrained         FFN-avg         Binary  0.556250\n",
      "13          My model         FFN-avg         Binary  0.443750\n",
      "14        Pretrained      FFN-concat         Binary  0.531250\n",
      "15          My model      FFN-concat         Binary  0.518750\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'My model', 'FFN-concat', 'Binary', acc_concat_my_binary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_ternary_embeddings, d_ternary_embeddings = my_ternary_train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_h1_nodes = 50\n",
    "num_h2_nodes = 10\n",
    "num_output_classes = 3\n",
    "dropout_rate = 0.2\n",
    "\n",
    "net_model = Net(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)\n",
    "print(net_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ternary_model(ternary_train_embeddings, ternary_train_y):\n",
    "    ternary_train_y = ternary_train_y.replace(2, 0)\n",
    "    ternary_train_y = ternary_train_y.replace(3, 2)\n",
    "    data_tensor = torch.tensor(ternary_train_embeddings, dtype=torch.float32)\n",
    "    target_tensor = torch.tensor(ternary_train_y.values, dtype=torch.long)\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # how many samples per batch to load\n",
    "    batch_size = 64\n",
    "\n",
    "    # as a positive integer will turn on multi-process data loading with the specified number of loader worker processes; otherwise, single-process data loading\n",
    "    num_workers = 0\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    number_of_epochs = 3\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net_model.parameters(), lr=0.01)\n",
    "    output_of_model = net_model.train_network(number_of_epochs, optimizer, criterion, train_loader)\n",
    "    \n",
    "    return output_of_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ternary_model(ternary_test_embeddings, ternary_test_y):\n",
    "    ternary_test_y = ternary_test_y.replace(2, 0)\n",
    "    ternary_test_y = ternary_test_y.replace(3, 2)\n",
    "    target_array = ternary_test_y.values\n",
    "    data_tensor = torch.tensor(ternary_test_embeddings, dtype=torch.float32)\n",
    "\n",
    "    # Create a PyTorch tensor from the NumPy array\n",
    "    target_tensor = torch.tensor(target_array, dtype=torch.long)  # Assuming target is of type long/int\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # Define batch size\n",
    "    batch_size = 1\n",
    "\n",
    "    # Define number of DataLoader workers\n",
    "    num_workers = 0  # Set this to a positive integer to enable multi-process data loading\n",
    "\n",
    "    # Create DataLoader\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    # Assuming net_model is an instance of your model\n",
    "    predictions = net_model.predict(test_loader)\n",
    "    # predictions = np.array(predictions)\n",
    "    # predictions_flat = predictions.flatten()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.265\n"
     ]
    }
   ],
   "source": [
    "train_ternary_model(pretrained_ternary_train_embeddings, ternary_y_train)\n",
    "acc_avg_pretrained_ternary = predict_ternary_model(pretrained_ternary_test_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1        TF-IDF-test  Perceptron-avg         Binary  0.756250\n",
      "2   Pretrained-train  Perceptron-avg         Binary  0.879687\n",
      "3    Pretrained-test  Perceptron-avg         Binary  0.750000\n",
      "4     My model-train  Perceptron-avg         Binary  0.548438\n",
      "5      My model-test  Perceptron-avg         Binary  0.481250\n",
      "6       TF-IDF-train         SVM-avg         Binary  0.998437\n",
      "7        TF-IDF-test         SVM-avg         Binary  0.781250\n",
      "8   Pretrained-train         SVM-avg         Binary  0.893750\n",
      "9    Pretrained-test         SVM-avg         Binary  0.825000\n",
      "10    My model-train         SVM-avg         Binary  0.829688\n",
      "11     My model-test         SVM-avg         Binary  0.418750\n",
      "12        Pretrained         FFN-avg         Binary  0.556250\n",
      "13          My model         FFN-avg         Binary  0.443750\n",
      "14        Pretrained      FFN-concat         Binary  0.531250\n",
      "15          My model      FFN-concat         Binary  0.518750\n",
      "16        Pretrained         FFN-avg        Ternary  0.265000\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained', 'FFN-avg', 'Ternary', acc_avg_pretrained_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.41\n"
     ]
    }
   ],
   "source": [
    "train_ternary_model(my_ternary_train_embeddings, ternary_y_train)\n",
    "acc_avg_my_ternary = predict_ternary_model(my_ternary_test_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1        TF-IDF-test  Perceptron-avg         Binary  0.756250\n",
      "2   Pretrained-train  Perceptron-avg         Binary  0.879687\n",
      "3    Pretrained-test  Perceptron-avg         Binary  0.750000\n",
      "4     My model-train  Perceptron-avg         Binary  0.548438\n",
      "5      My model-test  Perceptron-avg         Binary  0.481250\n",
      "6       TF-IDF-train         SVM-avg         Binary  0.998437\n",
      "7        TF-IDF-test         SVM-avg         Binary  0.781250\n",
      "8   Pretrained-train         SVM-avg         Binary  0.893750\n",
      "9    Pretrained-test         SVM-avg         Binary  0.825000\n",
      "10    My model-train         SVM-avg         Binary  0.829688\n",
      "11     My model-test         SVM-avg         Binary  0.418750\n",
      "12        Pretrained         FFN-avg         Binary  0.556250\n",
      "13          My model         FFN-avg         Binary  0.443750\n",
      "14        Pretrained      FFN-concat         Binary  0.531250\n",
      "15          My model      FFN-concat         Binary  0.518750\n",
      "16        Pretrained         FFN-avg        Ternary  0.265000\n",
      "17          My model         FFN-avg        Ternary  0.410000\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'My model', 'FFN-avg', 'Ternary', acc_avg_my_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.415\n"
     ]
    }
   ],
   "source": [
    "train_ternary_model(pretrained_ternary_train_concat_embeddings, ternary_y_train)\n",
    "acc_concat_pretrained_ternary = predict_ternary_model(pretrained_ternary_test_concat_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1        TF-IDF-test  Perceptron-avg         Binary  0.756250\n",
      "2   Pretrained-train  Perceptron-avg         Binary  0.879687\n",
      "3    Pretrained-test  Perceptron-avg         Binary  0.750000\n",
      "4     My model-train  Perceptron-avg         Binary  0.548438\n",
      "5      My model-test  Perceptron-avg         Binary  0.481250\n",
      "6       TF-IDF-train         SVM-avg         Binary  0.998437\n",
      "7        TF-IDF-test         SVM-avg         Binary  0.781250\n",
      "8   Pretrained-train         SVM-avg         Binary  0.893750\n",
      "9    Pretrained-test         SVM-avg         Binary  0.825000\n",
      "10    My model-train         SVM-avg         Binary  0.829688\n",
      "11     My model-test         SVM-avg         Binary  0.418750\n",
      "12        Pretrained         FFN-avg         Binary  0.556250\n",
      "13          My model         FFN-avg         Binary  0.443750\n",
      "14        Pretrained      FFN-concat         Binary  0.531250\n",
      "15          My model      FFN-concat         Binary  0.518750\n",
      "16        Pretrained         FFN-avg        Ternary  0.265000\n",
      "17          My model         FFN-avg        Ternary  0.410000\n",
      "18        Pretrained      FFN-concat        Ternary  0.415000\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained', 'FFN-concat', 'Ternary', acc_concat_pretrained_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.41\n"
     ]
    }
   ],
   "source": [
    "train_ternary_model(my_ternary_train_concat_embeddings, ternary_y_train)\n",
    "acc_concat_my_ternary = predict_ternary_model(my_ternary_test_concat_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1        TF-IDF-test  Perceptron-avg         Binary  0.756250\n",
      "2   Pretrained-train  Perceptron-avg         Binary  0.879687\n",
      "3    Pretrained-test  Perceptron-avg         Binary  0.750000\n",
      "4     My model-train  Perceptron-avg         Binary  0.548438\n",
      "5      My model-test  Perceptron-avg         Binary  0.481250\n",
      "6       TF-IDF-train         SVM-avg         Binary  0.998437\n",
      "7        TF-IDF-test         SVM-avg         Binary  0.781250\n",
      "8   Pretrained-train         SVM-avg         Binary  0.893750\n",
      "9    Pretrained-test         SVM-avg         Binary  0.825000\n",
      "10    My model-train         SVM-avg         Binary  0.829688\n",
      "11     My model-test         SVM-avg         Binary  0.418750\n",
      "12        Pretrained         FFN-avg         Binary  0.556250\n",
      "13          My model         FFN-avg         Binary  0.443750\n",
      "14        Pretrained      FFN-concat         Binary  0.531250\n",
      "15          My model      FFN-concat         Binary  0.518750\n",
      "16        Pretrained         FFN-avg        Ternary  0.265000\n",
      "17          My model         FFN-avg        Ternary  0.410000\n",
      "18        Pretrained      FFN-concat        Ternary  0.415000\n",
      "19          My model      FFN-concat        Ternary  0.410000\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'My model', 'FFN-concat', 'Ternary', acc_concat_my_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condition word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_word_embeddings(df: pd.DataFrame, col_name: str, model_to_use):\n",
    "    \"\"\"Extract word embeddings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    model_to_use:\n",
    "        Either the pretrained model or my pretrained model\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    mean_sentences_vectorized\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_vectorized = []\n",
    "    mean_sentences_vectorized = []\n",
    "    sentences = df[col_name].values\n",
    "\n",
    "    for sentences_idx in range(len(sentences)):\n",
    "        vectorized_words = []\n",
    "        sentence = sentences[sentences_idx]\n",
    "        # print(\"Sentence\", sentences_idx)\n",
    "        # print(\"Sentence\", sentences_idx, \"Pre-vectorized -- \", sentence)\n",
    "        for word_idx, word in enumerate(sentence.split(\" \")):\n",
    "            if word in model_to_use.key_to_index:\n",
    "                vector_of_word = model_to_use[word]\n",
    "                vectorized_words.append(vector_of_word)\n",
    "            else:\n",
    "                vector_of_word = np.random.rand(model_to_use.vector_size)\n",
    "                vectorized_words.append(vector_of_word)\n",
    "\n",
    "        sentence_vectorized.append(vectorized_words)\n",
    "        # print(\"Sentence\", sentences_idx, \"Post-vectorized \\n\")\n",
    "        mean_of_sentence = np.mean(sentence_vectorized[sentences_idx], axis=0)\n",
    "        mean_sentences_vectorized.append(mean_of_sentence)\n",
    "    print(len(mean_sentences_vectorized))\n",
    "    return mean_sentences_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# def condition_word_embeddings(df: pd.DataFrame, col_name: str, max_sentence_length: int, model_to_use):\n",
    "#     \"\"\"Extract word embeddings\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     df: `pd.DataFrame`\n",
    "#         The data\n",
    "    \n",
    "#     col_name: `str`\n",
    "#         Column with reviews\n",
    "\n",
    "#     model_to_use:\n",
    "#         Either the pretrained model or my pretrained model\n",
    "\n",
    "#     Return\n",
    "#     ------\n",
    "#     conditioned_sequences\n",
    "#     \"\"\"\n",
    "\n",
    "#     sentence_vectorized = []\n",
    "#     mean_sentences_vectorized = []\n",
    "#     concatenated_features = []\n",
    "#     sentences = df[col_name].values\n",
    "\n",
    "#     for sentences_idx in range(len(sentences)):\n",
    "#         vectorized_words = []\n",
    "#         sentence = sentences[sentences_idx]\n",
    "#         # print(\"Sentence\", sentences_idx, sentence)\n",
    "#         words = sentence.split(\" \")\n",
    "#         # print(\"Before -- \", len(words))\n",
    "#         if len(words) > max_sentence_length:\n",
    "#             words = words[:max_sentence_length]\n",
    "#         elif len(words) < max_sentence_length:\n",
    "#             words += [''] * (max_sentence_length - len(words))\n",
    "\n",
    "#         # print(\"After -- \", len(words), words)\n",
    "\n",
    "#         for word in words:\n",
    "#             if word in model_to_use.key_to_index:\n",
    "#                 vector_of_word = model_to_use[word]\n",
    "#                 vectorized_words.append(vector_of_word)\n",
    "#             else:\n",
    "#                 vector_of_word = np.random.rand(model_to_use.vector_size)\n",
    "#                 vectorized_words.append(vector_of_word)\n",
    "\n",
    "#         sentence_vectorized.append(vectorized_words)\n",
    "#     # print(sentence_vectorized)\n",
    "#         # print()\n",
    "    \n",
    "        \n",
    "#     tensors = []\n",
    "#     for words in sentence_vectorized:\n",
    "#         tensors.append(torch.tensor(words))\n",
    "#     padded_sequences = pad_sequence(tensors, batch_first=True, padding_value=0)\n",
    "#     print(padded_sequences.shape)\n",
    "\n",
    "#     return padded_sequences\n",
    "#     # return sentence_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Pretrained Train\n",
      "640\n",
      "Binary Pretrained Test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(640, 300)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Binary Pretrained Train\")\n",
    "pretrained_binary_train_50_embeddings = new_word_embeddings(binary_X_train, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_binary_train_50_embeddings = np.array(pretrained_binary_train_50_embeddings)\n",
    "\n",
    "\n",
    "print(\"Binary Pretrained Test\")\n",
    "# pretrained_binary_test_50_embeddings = condition_word_embeddings(binary_X_test, 'lemmed_reviews', 50, pretrained_word_two_vec_model)\n",
    "# pretrained_binary_test_50_embeddings = np.array(pretrained_binary_test_50_embeddings)\n",
    "\n",
    "pretrained_binary_train_50_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binary_cnn_model(binary_train_embeddings, binary_train_y): # TODO: pass in test data and y as well\n",
    "    print(\"1-train_binary_cnn_model\")\n",
    "    binary_train_y = binary_train_y.replace(2, 1)\n",
    "    data_tensor = torch.tensor(binary_train_embeddings, dtype=torch.float32)\n",
    "    target_tensor = torch.tensor(binary_train_y.values, dtype=torch.long)\n",
    "    train_dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # TODO: Manipulate test data\n",
    "    # test_dataset = TensorDataset(torch.tensor(binary_train_embeddings, dtype=torch.float32), torch.tensor(binary_train_y.values, dtype=torch.long))\n",
    "\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Define your CNN model\n",
    "    # cnn_net_model.to(device)\n",
    "    \n",
    "    # how many samples per batch to load\n",
    "    batch_size = 64\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "    number_of_epochs = 3\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(cnn_net_model.parameters(), lr=0.001, momentum=0.9)\n",
    "    print(\"2-train_binary_cnn_model\")\n",
    "    print(binary_train_embeddings.shape, binary_train_y.shape)\n",
    "    output_of_model = cnn_net_model.train_network(number_of_epochs, optimizer, criterion, train_loader)\n",
    "    # number_of_epochs: int, optimizer, criterion_function, train_loader\n",
    "    \n",
    "    # return output_of_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((640, 300), (640,))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_binary_train_50_embeddings.shape, binary_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640 300\n",
      "(640, 300)\n",
      "CNN_Net(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv1d(1, 50, kernel_size=(3,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(50, 10, kernel_size=(3,), stride=(1,))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=2960, out_features=2, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN_Net(nn.Module):\n",
    "    def __init__(self, embedding_size, num_channels, output_channels1, output_channels2, kernel_size1, kernel_size2, num_classes):\n",
    "        super(CNN_Net, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=output_channels1, kernel_size=kernel_size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=output_channels1, out_channels=output_channels2, kernel_size=kernel_size2),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        # Compute the size of the output from the convolutional layers\n",
    "        # self.final_seq_length = sequence_length - (kernel_size1 - 1) - (kernel_size2 - 1)\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            # nn.Linear(output_channels2 * self.final_seq_length * embedding_size, 138000)\n",
    "            nn.Linear(2960, num_classes),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"Before permute\", x.shape)\n",
    "        # x = x.permute(0, 2, 1)  # Reshape for Conv1d: [batch_size, num_channels, embedding_size, sequence_length]\n",
    "        x = x.reshape(x.shape[0], 1 , x.shape[1]) #### TODO: \n",
    "        x = self.conv_layers(x)\n",
    "        # print(\"1st\", x.shape)\n",
    "        # Flatten the output of the convolutional layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass through fully connected layers\n",
    "        # print(\"2nd\", x.shape)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    def train_network(self, number_of_epochs: int, optimizer, criterion_function, train_loader):\n",
    "        for epoch in range(number_of_epochs):\n",
    "            correct_train = 0 # delete\n",
    "            self.train()\n",
    "            \n",
    "            for data, target in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = self(data) # predictions\n",
    "                # print(\"Output\", output.shape)\n",
    "                target = target.squeeze()\n",
    "                loss = criterion_function(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step\n",
    "\n",
    "                #### DELETE\n",
    "                # if debug mode == 1:\n",
    "                print(\"Debug\")\n",
    "                loss += loss.item() * data.size(0)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                #\n",
    "                correct_train += (predicted == target).sum().item()\n",
    "                #### DELETE\n",
    "\n",
    "            print(\"Loss: \", loss)\n",
    "        \n",
    "             \n",
    "            # self.eval()\n",
    "        #     with torch.no grad():\n",
    "        #         for data, target in test_loader:\n",
    "        #             output = self(data)\n",
    "        #             target = target.squeeze()\n",
    "        #             loss = criterion_function(output, target)\n",
    "\n",
    "            loss = loss / len(train_loader.dataset)\n",
    "            # valid loss = valid loss / len(test loader. dataset)\n",
    "            train_accuracy = correct_train / len(train_loader.dataset)\n",
    "            print(f'Epoch: {epoch+1} \\tTraining Loss: {loss:.6f} \\tTraining Accuracy: {train_accuracy * 100:.2f}%')\n",
    "        print('Finished Training')\n",
    "\n",
    "\n",
    "\n",
    "# reshaped_tensor = pretrained_binary_train_50_embeddings.unsqueeze(1)  # Add channel dimension\n",
    "N, embedding_size = pretrained_binary_train_50_embeddings.shape\n",
    "print(N, embedding_size)\n",
    "\n",
    "print(pretrained_binary_train_50_embeddings.shape)\n",
    "num_channels = 1 # bc it's text\n",
    "\n",
    "output_channels1 = 50\n",
    "output_channels2 = 10\n",
    "kernel_size1 = 3  # Adjust kernel size as needed\n",
    "kernel_size2 = 3  # Adjust kernel size as needed\n",
    "num_classes = 2  # Number of output classes, adjust as needed\n",
    "\n",
    "cnn_net_model = CNN_Net(embedding_size, num_channels, output_channels1, output_channels2, kernel_size1, kernel_size2, num_classes)\n",
    "print(cnn_net_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-train_binary_cnn_model\n",
      "2-train_binary_cnn_model\n",
      "(640, 300) (640,)\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Loss:  tensor(44.8735, grad_fn=<AddBackward0>)\n",
      "Epoch: 1 \tTraining Loss: 0.070115 \tTraining Accuracy: 95.16%\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Loss:  tensor(44.8735, grad_fn=<AddBackward0>)\n",
      "Epoch: 2 \tTraining Loss: 0.070115 \tTraining Accuracy: 95.16%\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Debug\n",
      "Loss:  tensor(44.8735, grad_fn=<AddBackward0>)\n",
      "Epoch: 3 \tTraining Loss: 0.070115 \tTraining Accuracy: 95.16%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_binary_cnn_model(pretrained_binary_train_50_embeddings, binary_y_train)\n",
    "# acc_50_pretrained_binary = predict_binary_cnn_model(pretrained_binary_test_embeddings, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = store_results(results_dict, 'Pretrained', 'CNN-50', 'Binary', acc_50_pretrained_binary)\n",
    "# print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_binary_cnn_model(my_binary_train_embeddings, binary_y_train)\n",
    "# acc_50_my_binary = predict_binary_cnn_model(my_binary_train_embeddings, binary_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = store_results(results_dict, 'My model', 'CNN-500', 'Binary', acc_50_my_binary)\n",
    "# print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ternary_embeddings, d_ternary_embeddings = my_ternary_train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CNN_Net.__init__() missing 2 required positional arguments: 'kernel_size2' and 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m num_output_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      4\u001b[0m dropout_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m----> 6\u001b[0m net_model \u001b[38;5;241m=\u001b[39m \u001b[43mCNN_Net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_h1_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_h2_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_binary_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_output_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(net_model)\n",
      "\u001b[0;31mTypeError\u001b[0m: CNN_Net.__init__() missing 2 required positional arguments: 'kernel_size2' and 'num_classes'"
     ]
    }
   ],
   "source": [
    "num_h1_nodes = 50\n",
    "num_h2_nodes = 10\n",
    "num_output_classes = 3\n",
    "dropout_rate = 0.2\n",
    "\n",
    "net_model = CNN_Net(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)\n",
    "print(net_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ternary_cnn_model(ternary_train_embeddings, ternary_train_y):\n",
    "    ternary_train_y = ternary_train_y.replace(2, 0)\n",
    "    ternary_train_y = ternary_train_y.replace(3, 2)\n",
    "    data_tensor = torch.tensor(ternary_train_embeddings, dtype=torch.float32)\n",
    "    target_tensor = torch.tensor(ternary_train_y.values, dtype=torch.long)\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # how many samples per batch to load\n",
    "    batch_size = 64\n",
    "\n",
    "    # as a positive integer will turn on multi-process data loading with the specified number of loader worker processes; otherwise, single-process data loading\n",
    "    num_workers = 0\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    number_of_epochs = 30\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net_model.parameters(), lr=0.01)\n",
    "    output_of_model = cnn_net_model.train_network(number_of_epochs, optimizer, criterion, train_loader)\n",
    "    \n",
    "    return output_of_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ternary_cnn_model(ternary_test_embeddings, ternary_test_y):\n",
    "    ternary_test_y = ternary_test_y.replace(2, 0)\n",
    "    ternary_test_y = ternary_test_y.replace(3, 2)\n",
    "    target_array = ternary_test_y.values\n",
    "    data_tensor = torch.tensor(ternary_test_embeddings, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    # Create a PyTorch tensor from the NumPy array\n",
    "    target_tensor = torch.tensor(target_array, dtype=torch.long)  # Assuming target is of type long/int\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # Define batch size\n",
    "    batch_size = 1\n",
    "\n",
    "    # Define number of DataLoader workers\n",
    "    num_workers = 0  # Set this to a positive integer to enable multi-process data loading\n",
    "\n",
    "    # Create DataLoader\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    # Assuming net_model is an instance of your model\n",
    "    predictions = cnn_net_model.cnn_predict(test_loader)\n",
    "    # predictions = np.array(predictions)\n",
    "    # predictions_flat = predictions.flatten()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ternary_cnn_model(pretrained_ternary_train_embeddings, ternary_y_train)\n",
    "# acc_50_pretrained_ternary = predict_ternary_cnn_model(pretrained_ternary_test_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained', 'CNN-50', 'Ternary', acc_50_pretrained_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ternary_cnn_model(my_ternary_train_embeddings, ternary_y_train)\n",
    "acc_50_my_ternary = predict_ternary_cnn_model(my_ternary_test_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = store_results(results_dict, 'My model', 'CNN-50', 'Ternary', acc_50_my_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output.shape) # [batch_size = 64 , number_of_class (ternary = 3), (bin = 2) ]\n",
    "#             print(output[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
